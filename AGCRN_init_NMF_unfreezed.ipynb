{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688306fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import torch\n",
    "\n",
    "data = np.load('C:/Users/Hoda/A - Uni/thesis/AGCRN_GSL-init/data/PEMS04/pems04.npz')['data'][:, :, 0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f00457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40e361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoda\\A - Uni\\AGCRN\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "__file__ = 'C:/Users/Hoda/A - Uni/AGCRN/data/PEMS04.npz'\n",
    "file_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "print(file_dir)\n",
    "sys.path.append(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900b384b-6839-4dcb-90bd-467a5a2bd44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
      "Normalize the dataset by MinMax01 Normalization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e9ad0a126f45778dd7f01e4b7e6349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Adjacency Matrix Shape: (307, 307)\n",
      "NMF Embedding shape: torch.Size([307, 10])\n",
      "Adjacency Matrix Shape: (307, 307)\n",
      "Adjacency matrix saved to adj_matrix.npy\n",
      "Embeddings saved to embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "from lib.dataloader import *\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate the adjacency matrix and embeddings\n",
    "dataset_name = \"PEMSD4\"\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "prediction_length = 1  # Set prediction length\n",
    "lambda1 = 0.02\n",
    "embedding_dim = 10  # Set embedding dimensions\n",
    "\n",
    "adj_matrix, embeddings = get_train_adj_matrix_and_embeddings(\n",
    "    dataset_name, val_ratio, test_ratio, prediction_length, lambda1, embedding_dim)\n",
    "\n",
    "# Step 2: Print adjacency matrix shape\n",
    "print(f\"Adjacency Matrix Shape: {adj_matrix.shape}\")\n",
    "\n",
    "# Step 3: Save the adjacency matrix and embeddings\n",
    "adj_matrix_path = \"adj_matrix.npy\"\n",
    "embeddings_path = \"embeddings.pt\"\n",
    "\n",
    "np.save(adj_matrix_path, adj_matrix)\n",
    "torch.save(embeddings, embeddings_path)\n",
    "\n",
    "print(f\"Adjacency matrix saved to {adj_matrix_path}\")\n",
    "print(f\"Embeddings saved to {embeddings_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b77795-5e7f-4c2e-ab16-38fee4004bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed Embeddings Shape: torch.Size([307, 10])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate NMF-based embeddings\n",
    "precomputed_embeddings = torch.load('C:/Users/Hoda/A - Uni/thesis/AGCRN_GSL-init/embeddings.pt')\n",
    "\n",
    "# Step 4: Print or use embeddings\n",
    "print(f\"Precomputed Embeddings Shape: {precomputed_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7398e152-4ba0-4737-800c-af56639f04a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed embeddings from C:/Users/Hoda/A - Uni/thesis/est_adj_agcrn/E_init_NMF_nrmlze.npy with shape torch.Size([307, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Define the path to the adjacency matrix\n",
    "adj_file_name = \"C:/Users/Hoda/A - Uni/thesis/est_adj_agcrn/E_init_NMF_nrmlze.npy\"\n",
    "\n",
    "# Check if the file exists and load it\n",
    "if os.path.exists(adj_file_name):\n",
    "    # Load the matrix from the file\n",
    "    precomputed_embeddings = torch.tensor(np.load(adj_file_name), dtype=torch.float32)\n",
    "    print(f\"Loaded precomputed embeddings from {adj_file_name} with shape {precomputed_embeddings.shape}\")\n",
    "else:\n",
    "    precomputed_embeddings = None\n",
    "    print(f\"Embedding file not found at {adj_file_name}; using random initialization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0027bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "node_embeddings torch.Size([307, 10]) False\n",
      "encoder.dcrnn_cells.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True\n",
      "encoder.dcrnn_cells.0.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder.dcrnn_cells.0.update.weights_pool torch.Size([10, 2, 65, 64]) True\n",
      "encoder.dcrnn_cells.0.update.bias_pool torch.Size([10, 64]) True\n",
      "encoder.dcrnn_cells.1.gate.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "encoder.dcrnn_cells.1.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder.dcrnn_cells.1.update.weights_pool torch.Size([10, 2, 128, 64]) True\n",
      "encoder.dcrnn_cells.1.update.bias_pool torch.Size([10, 64]) True\n",
      "end_conv.weight torch.Size([12, 1, 1, 64]) True\n",
      "end_conv.bias torch.Size([12]) True\n",
      "Total params num: 748810\n",
      "*****************Finish Parameter****************\n",
      "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
      "Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)\n",
      "Val:    (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
      "Test:   (3375, 12, 307, 1) (3375, 12, 307, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from model.AGCRN import AGCRN as Network\n",
    "from model.BasicTrainer import Trainer\n",
    "from lib.TrainInits import init_seed\n",
    "from lib.dataloader import get_dataloader\n",
    "from lib.TrainInits import print_model_parameters\n",
    "\n",
    "\n",
    "#*************************************************************************#\n",
    "Mode = 'Train'\n",
    "DEBUG = 'True'\n",
    "DATASET = 'PEMSD4'      #PEMSD4 or PEMSD8\n",
    "DEVICE = 'cuda:0'\n",
    "MODEL = 'AGCRN'\n",
    "\n",
    "#get configuration\n",
    "config_file = 'model/PEMSD4_AGCRN.conf'  # Assuming the file is in the same directory as your script\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "from lib.metrics import MAE_torch\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss\n",
    "\n",
    "#parser\n",
    "args = argparse.ArgumentParser(description='arguments')\n",
    "args.add_argument('--dataset', default=DATASET, type=str)\n",
    "args.add_argument('--mode', default=Mode, type=str)\n",
    "args.add_argument('--device', default=DEVICE, type=str, help='indices of GPUs')\n",
    "args.add_argument('--debug', default=DEBUG, type=eval)\n",
    "args.add_argument('--model', default=MODEL, type=str)\n",
    "args.add_argument('--cuda', default=True, type=bool)\n",
    "#data\n",
    "args.add_argument('--val_ratio', default=config['data']['val_ratio'], type=float)\n",
    "args.add_argument('--test_ratio', default=config['data']['test_ratio'], type=float)\n",
    "args.add_argument('--lag', default=config['data']['lag'], type=int)\n",
    "args.add_argument('--horizon', default=config['data']['horizon'], type=int)\n",
    "args.add_argument('--num_nodes', default=config['data']['num_nodes'], type=int)\n",
    "args.add_argument('--tod', default=config['data']['tod'], type=eval)\n",
    "args.add_argument('--normalizer', default=config['data']['normalizer'], type=str)\n",
    "args.add_argument('--column_wise', default=config['data']['column_wise'], type=eval)\n",
    "args.add_argument('--default_graph', default=config['data']['default_graph'], type=eval)\n",
    "#model\n",
    "args.add_argument('--input_dim', default=config['model']['input_dim'], type=int)\n",
    "args.add_argument('--output_dim', default=config['model']['output_dim'], type=int)\n",
    "args.add_argument('--embed_dim', default=config['model']['embed_dim'], type=int)\n",
    "args.add_argument('--rnn_units', default=config['model']['rnn_units'], type=int)\n",
    "args.add_argument('--num_layers', default=config['model']['num_layers'], type=int)\n",
    "args.add_argument('--cheb_k', default=config['model']['cheb_order'], type=int)\n",
    "#train\n",
    "args.add_argument('--loss_func', default=config['train']['loss_func'], type=str)\n",
    "args.add_argument('--seed', default=config['train']['seed'], type=int)\n",
    "args.add_argument('--batch_size', default=config['train']['batch_size'], type=int)\n",
    "args.add_argument('--epochs', default=config['train']['epochs'], type=int)\n",
    "args.add_argument('--lr_init', default=config['train']['lr_init'], type=float)\n",
    "args.add_argument('--lr_decay', default=config['train']['lr_decay'], type=eval)\n",
    "args.add_argument('--lr_decay_rate', default=config['train']['lr_decay_rate'], type=float)\n",
    "args.add_argument('--lr_decay_step', default=config['train']['lr_decay_step'], type=str)\n",
    "args.add_argument('--early_stop', default=config['train']['early_stop'], type=eval)\n",
    "args.add_argument('--early_stop_patience', default=config['train']['early_stop_patience'], type=int)\n",
    "args.add_argument('--grad_norm', default=config['train']['grad_norm'], type=eval)\n",
    "args.add_argument('--max_grad_norm', default=config['train']['max_grad_norm'], type=int)\n",
    "args.add_argument('--teacher_forcing', default=False, type=bool)\n",
    "#args.add_argument('--tf_decay_steps', default=2000, type=int, help='teacher forcing decay steps')\n",
    "args.add_argument('--real_value', default=config['train']['real_value'], type=eval, help = 'use real value for loss calculation')\n",
    "#test\n",
    "args.add_argument('--mae_thresh', default=config['test']['mae_thresh'], type=eval)\n",
    "args.add_argument('--mape_thresh', default=config['test']['mape_thresh'], type=float)\n",
    "#log\n",
    "args.add_argument('--log_dir', default='./', type=str)\n",
    "args.add_argument('--log_step', default=config['log']['log_step'], type=int)\n",
    "args.add_argument('--plot', default=config['log']['plot'], type=eval)\n",
    "args = args.parse_args()\n",
    "init_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(int(args.device[5]))\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n",
    "\n",
    "#init model\n",
    "# Initialize the model with precomputed embeddings\n",
    "model = Network(args, precomputed_embeddings=precomputed_embeddings)\n",
    "model = model.to(args.device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "print_model_parameters(model, only_num=False)\n",
    "\n",
    "#load dataset\n",
    "train_loader, val_loader, test_loader, scaler = get_dataloader(args,\n",
    "                                                               normalizer=args.normalizer,\n",
    "                                                               tod=args.tod, dow=False,\n",
    "                                                               weather=False, single=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239aa7cb-0025-41d0-bab8-55b35f8717cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 09:44: Experiment log path in: C:\\Users\\Hoda\\A - Uni\\AGCRN\\data\\experiments\\PEMSD4\\20241222094435\n",
      "2024-12-22 09:44: Train Epoch 1: 0/158 Loss: 184.452835\n",
      "2024-12-22 09:46: Train Epoch 1: 20/158 Loss: 202.696243\n",
      "2024-12-22 09:47: Train Epoch 1: 40/158 Loss: 194.430710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "if args.mode.lower() == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    # Load the model on CPU\n",
    "    model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "    model.to(torch.device('cpu'))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4219bdd9-2fee-4159-9825-404b5049b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust embeddings to match expected node count\n",
    "expected_nodes = args.num_nodes  # 358\n",
    "current_nodes, embed_dim = precomputed_embeddings.shape  # 307, embed_dim\n",
    "if current_nodes < expected_nodes:\n",
    "    padding = torch.zeros(expected_nodes - current_nodes, embed_dim)  # Zero embeddings for missing nodes\n",
    "    precomputed_embeddings = torch.cat((precomputed_embeddings, padding), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877db29f-3126-4387-905c-aa4bcbfca9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538ddad-c95b-4e0f-ab59-7cfaf49c6eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4dfd0-47ad-4ccc-b913-3c2e00b7aad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ccc17-4c44-4069-bb87-35b2901b3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 07:46: Experiment log path in: C:\\Users\\Hoda\\A - Uni\\AGCRN\\data\\experiments\\PEMSD4\\20241218074645\n",
      "2024-12-18 07:46: Train Epoch 1: 0/158 Loss: 184.452835\n",
      "2024-12-18 07:48: Train Epoch 1: 20/158 Loss: 202.696243\n",
      "2024-12-18 07:49: Train Epoch 1: 40/158 Loss: 194.430710\n",
      "2024-12-18 07:51: Train Epoch 1: 60/158 Loss: 188.901917\n",
      "2024-12-18 07:52: Train Epoch 1: 80/158 Loss: 145.514496\n",
      "2024-12-18 07:54: Train Epoch 1: 100/158 Loss: 185.942703\n",
      "2024-12-18 07:55: Train Epoch 1: 120/158 Loss: 162.186142\n",
      "2024-12-18 07:56: Train Epoch 1: 140/158 Loss: 136.544693\n",
      "2024-12-18 07:58: Train Epoch 1: averaged Loss: 182.000048, MAE: 182.0000, RMSE: 237.5332, MAPE: 0.9057, tf_ratio: 1.000000, Time: 680.57 seconds\n",
      "2024-12-18 07:59: Validation Epoch 1: average Loss: 175.115212, MAE: 175.1152, RMSE: 231.0441, MAPE: 0.9443, Time: 93.24 seconds\n",
      "2024-12-18 07:59: ********** Current best model saved!\n",
      "2024-12-18 07:59: Train Epoch 2: 0/158 Loss: 167.912537\n",
      "2024-12-18 08:01: Train Epoch 2: 20/158 Loss: 134.199799\n",
      "2024-12-18 08:02: Train Epoch 2: 40/158 Loss: 155.323090\n",
      "2024-12-18 08:04: Train Epoch 2: 60/158 Loss: 151.857620\n",
      "2024-12-18 08:05: Train Epoch 2: 80/158 Loss: 169.945602\n",
      "2024-12-18 08:06: Train Epoch 2: 100/158 Loss: 158.027390\n",
      "2024-12-18 08:08: Train Epoch 2: 120/158 Loss: 146.064880\n",
      "2024-12-18 08:09: Train Epoch 2: 140/158 Loss: 145.828094\n",
      "2024-12-18 08:11: Train Epoch 2: averaged Loss: 149.989430, MAE: 149.9894, RMSE: 202.9784, MAPE: 1.1074, tf_ratio: 1.000000, Time: 680.35 seconds\n",
      "2024-12-18 08:12: Validation Epoch 2: average Loss: 153.006943, MAE: 153.0069, RMSE: 204.3158, MAPE: 1.1901, Time: 93.23 seconds\n",
      "2024-12-18 08:12: ********** Current best model saved!\n",
      "2024-12-18 08:12: Train Epoch 3: 0/158 Loss: 146.250061\n",
      "2024-12-18 08:14: Train Epoch 3: 20/158 Loss: 130.411667\n",
      "2024-12-18 08:15: Train Epoch 3: 40/158 Loss: 147.036377\n",
      "2024-12-18 08:16: Train Epoch 3: 60/158 Loss: 141.895020\n",
      "2024-12-18 08:18: Train Epoch 3: 80/158 Loss: 114.995201\n",
      "2024-12-18 08:19: Train Epoch 3: 100/158 Loss: 110.384125\n",
      "2024-12-18 08:21: Train Epoch 3: 120/158 Loss: 137.854233\n",
      "2024-12-18 08:22: Train Epoch 3: 140/158 Loss: 128.113556\n",
      "2024-12-18 08:23: Train Epoch 3: averaged Loss: 126.916785, MAE: 126.9168, RMSE: 179.5343, MAPE: 0.9002, tf_ratio: 1.000000, Time: 678.96 seconds\n",
      "2024-12-18 08:25: Validation Epoch 3: average Loss: 118.739023, MAE: 118.7390, RMSE: 176.8103, MAPE: 0.4475, Time: 93.29 seconds\n",
      "2024-12-18 08:25: ********** Current best model saved!\n",
      "2024-12-18 08:25: Train Epoch 4: 0/158 Loss: 105.833557\n",
      "2024-12-18 08:26: Train Epoch 4: 20/158 Loss: 100.342339\n",
      "2024-12-18 08:28: Train Epoch 4: 40/158 Loss: 97.787453\n",
      "2024-12-18 08:29: Train Epoch 4: 60/158 Loss: 91.116348\n",
      "2024-12-18 08:31: Train Epoch 4: 80/158 Loss: 107.563660\n",
      "2024-12-18 08:32: Train Epoch 4: 100/158 Loss: 80.230270\n",
      "2024-12-18 08:34: Train Epoch 4: 120/158 Loss: 88.921120\n",
      "2024-12-18 08:35: Train Epoch 4: 140/158 Loss: 79.274551\n",
      "2024-12-18 08:36: Train Epoch 4: averaged Loss: 94.584633, MAE: 94.5846, RMSE: 150.6441, MAPE: 0.3679, tf_ratio: 1.000000, Time: 678.20 seconds\n",
      "2024-12-18 08:38: Validation Epoch 4: average Loss: 93.807561, MAE: 93.8076, RMSE: 149.5344, MAPE: 0.3313, Time: 93.46 seconds\n",
      "2024-12-18 08:38: ********** Current best model saved!\n",
      "2024-12-18 08:38: Train Epoch 5: 0/158 Loss: 81.994164\n",
      "2024-12-18 08:39: Train Epoch 5: 20/158 Loss: 79.022842\n",
      "2024-12-18 08:41: Train Epoch 5: 40/158 Loss: 74.209404\n",
      "2024-12-18 08:42: Train Epoch 5: 60/158 Loss: 72.185547\n",
      "2024-12-18 08:44: Train Epoch 5: 80/158 Loss: 65.272636\n",
      "2024-12-18 08:45: Train Epoch 5: 100/158 Loss: 80.631294\n",
      "2024-12-18 08:47: Train Epoch 5: 120/158 Loss: 79.512390\n",
      "2024-12-18 08:48: Train Epoch 5: 140/158 Loss: 58.827888\n",
      "2024-12-18 08:49: Train Epoch 5: averaged Loss: 75.797605, MAE: 75.7976, RMSE: 127.8194, MAPE: 0.2996, tf_ratio: 1.000000, Time: 680.35 seconds\n",
      "2024-12-18 08:51: Validation Epoch 5: average Loss: 77.335023, MAE: 77.3350, RMSE: 128.6354, MAPE: 0.2807, Time: 93.32 seconds\n",
      "2024-12-18 08:51: ********** Current best model saved!\n",
      "2024-12-18 08:51: Train Epoch 6: 0/158 Loss: 68.130783\n",
      "2024-12-18 08:52: Train Epoch 6: 20/158 Loss: 64.219635\n",
      "2024-12-18 08:54: Train Epoch 6: 40/158 Loss: 64.480621\n",
      "2024-12-18 08:55: Train Epoch 6: 60/158 Loss: 59.112736\n",
      "2024-12-18 08:57: Train Epoch 6: 80/158 Loss: 70.249771\n",
      "2024-12-18 08:58: Train Epoch 6: 100/158 Loss: 51.954895\n",
      "2024-12-18 08:59: Train Epoch 6: 120/158 Loss: 54.262016\n",
      "2024-12-18 09:01: Train Epoch 6: 140/158 Loss: 57.795589\n",
      "2024-12-18 09:02: Train Epoch 6: averaged Loss: 62.895198, MAE: 62.8952, RMSE: 110.0104, MAPE: 0.2598, tf_ratio: 1.000000, Time: 679.37 seconds\n",
      "2024-12-18 09:04: Validation Epoch 6: average Loss: 64.907427, MAE: 64.9074, RMSE: 111.7050, MAPE: 0.2456, Time: 93.75 seconds\n",
      "2024-12-18 09:04: ********** Current best model saved!\n",
      "2024-12-18 09:04: Train Epoch 7: 0/158 Loss: 55.283558\n",
      "2024-12-18 09:05: Train Epoch 7: 20/158 Loss: 60.615761\n",
      "2024-12-18 09:07: Train Epoch 7: 40/158 Loss: 60.978912\n",
      "2024-12-18 09:08: Train Epoch 7: 60/158 Loss: 58.553059\n",
      "2024-12-18 09:09: Train Epoch 7: 80/158 Loss: 55.405380\n",
      "2024-12-18 09:11: Train Epoch 7: 100/158 Loss: 52.261978\n",
      "2024-12-18 09:12: Train Epoch 7: 120/158 Loss: 46.359020\n",
      "2024-12-18 09:14: Train Epoch 7: 140/158 Loss: 50.311764\n",
      "2024-12-18 09:15: Train Epoch 7: averaged Loss: 53.516548, MAE: 53.5165, RMSE: 95.7496, MAPE: 0.2357, tf_ratio: 1.000000, Time: 677.51 seconds\n",
      "2024-12-18 09:16: Validation Epoch 7: average Loss: 55.474717, MAE: 55.4747, RMSE: 97.8297, MAPE: 0.2260, Time: 92.49 seconds\n",
      "2024-12-18 09:16: ********** Current best model saved!\n",
      "2024-12-18 09:17: Train Epoch 8: 0/158 Loss: 47.494858\n",
      "2024-12-18 09:18: Train Epoch 8: 20/158 Loss: 51.176193\n",
      "2024-12-18 09:19: Train Epoch 8: 40/158 Loss: 40.409286\n",
      "2024-12-18 09:21: Train Epoch 8: 60/158 Loss: 42.310966\n",
      "2024-12-18 09:22: Train Epoch 8: 80/158 Loss: 49.683979\n",
      "2024-12-18 09:24: Train Epoch 8: 100/158 Loss: 42.211884\n",
      "2024-12-18 09:25: Train Epoch 8: 120/158 Loss: 38.858967\n",
      "2024-12-18 09:26: Train Epoch 8: 140/158 Loss: 49.378597\n",
      "2024-12-18 09:28: Train Epoch 8: averaged Loss: 46.305776, MAE: 46.3058, RMSE: 84.1048, MAPE: 0.2164, tf_ratio: 1.000000, Time: 674.00 seconds\n",
      "2024-12-18 09:29: Validation Epoch 8: average Loss: 48.407945, MAE: 48.4079, RMSE: 86.6366, MAPE: 0.2047, Time: 93.43 seconds\n",
      "2024-12-18 09:29: ********** Current best model saved!\n",
      "2024-12-18 09:29: Train Epoch 9: 0/158 Loss: 45.714954\n",
      "2024-12-18 09:31: Train Epoch 9: 20/158 Loss: 34.726013\n",
      "2024-12-18 09:32: Train Epoch 9: 40/158 Loss: 46.303246\n",
      "2024-12-18 09:34: Train Epoch 9: 60/158 Loss: 44.572773\n",
      "2024-12-18 09:35: Train Epoch 9: 80/158 Loss: 38.803616\n",
      "2024-12-18 09:36: Train Epoch 9: 100/158 Loss: 40.857731\n",
      "2024-12-18 09:38: Train Epoch 9: 120/158 Loss: 39.518646\n",
      "2024-12-18 09:39: Train Epoch 9: 140/158 Loss: 38.665451\n",
      "2024-12-18 09:41: Train Epoch 9: averaged Loss: 40.988797, MAE: 40.9888, RMSE: 74.8035, MAPE: 0.2037, tf_ratio: 1.000000, Time: 678.52 seconds\n",
      "2024-12-18 09:42: Validation Epoch 9: average Loss: 43.262259, MAE: 43.2623, RMSE: 77.7694, MAPE: 0.1913, Time: 92.61 seconds\n",
      "2024-12-18 09:42: ********** Current best model saved!\n",
      "2024-12-18 09:42: Train Epoch 10: 0/158 Loss: 39.977123\n",
      "2024-12-18 09:44: Train Epoch 10: 20/158 Loss: 37.297478\n",
      "2024-12-18 09:45: Train Epoch 10: 40/158 Loss: 37.040478\n",
      "2024-12-18 09:46: Train Epoch 10: 60/158 Loss: 36.262489\n",
      "2024-12-18 09:48: Train Epoch 10: 80/158 Loss: 38.177372\n",
      "2024-12-18 09:49: Train Epoch 10: 100/158 Loss: 36.505051\n",
      "2024-12-18 09:51: Train Epoch 10: 120/158 Loss: 36.442947\n",
      "2024-12-18 09:52: Train Epoch 10: 140/158 Loss: 35.628056\n",
      "2024-12-18 09:53: Train Epoch 10: averaged Loss: 36.872122, MAE: 36.8721, RMSE: 67.3226, MAPE: 0.1931, tf_ratio: 1.000000, Time: 674.79 seconds\n",
      "2024-12-18 09:55: Validation Epoch 10: average Loss: 39.377464, MAE: 39.3775, RMSE: 70.6183, MAPE: 0.1836, Time: 92.78 seconds\n",
      "2024-12-18 09:55: ********** Current best model saved!\n",
      "2024-12-18 09:55: Train Epoch 11: 0/158 Loss: 37.745285\n",
      "2024-12-18 09:56: Train Epoch 11: 20/158 Loss: 37.628185\n",
      "2024-12-18 09:58: Train Epoch 11: 40/158 Loss: 35.257786\n",
      "2024-12-18 09:59: Train Epoch 11: 60/158 Loss: 35.057678\n",
      "2024-12-18 10:01: Train Epoch 11: 80/158 Loss: 32.603931\n",
      "2024-12-18 10:02: Train Epoch 11: 100/158 Loss: 36.920753\n",
      "2024-12-18 10:04: Train Epoch 11: 120/158 Loss: 33.721863\n",
      "2024-12-18 10:05: Train Epoch 11: 140/158 Loss: 32.851818\n",
      "2024-12-18 10:06: Train Epoch 11: averaged Loss: 33.929573, MAE: 33.9296, RMSE: 61.4607, MAPE: 0.1876, tf_ratio: 1.000000, Time: 675.83 seconds\n",
      "2024-12-18 10:08: Validation Epoch 11: average Loss: 35.943234, MAE: 35.9432, RMSE: 64.6210, MAPE: 0.1789, Time: 92.56 seconds\n",
      "2024-12-18 10:08: ********** Current best model saved!\n",
      "2024-12-18 10:08: Train Epoch 12: 0/158 Loss: 31.927586\n",
      "2024-12-18 10:09: Train Epoch 12: 20/158 Loss: 30.731890\n",
      "2024-12-18 10:11: Train Epoch 12: 40/158 Loss: 28.517832\n",
      "2024-12-18 10:12: Train Epoch 12: 60/158 Loss: 33.090836\n",
      "2024-12-18 10:13: Train Epoch 12: 80/158 Loss: 31.614307\n",
      "2024-12-18 10:15: Train Epoch 12: 100/158 Loss: 30.676987\n",
      "2024-12-18 10:16: Train Epoch 12: 120/158 Loss: 33.116341\n",
      "2024-12-18 10:18: Train Epoch 12: 140/158 Loss: 27.363607\n",
      "2024-12-18 10:19: Train Epoch 12: averaged Loss: 31.549623, MAE: 31.5496, RMSE: 56.7041, MAPE: 0.1815, tf_ratio: 1.000000, Time: 674.05 seconds\n",
      "2024-12-18 10:20: Validation Epoch 12: average Loss: 33.809345, MAE: 33.8093, RMSE: 60.0732, MAPE: 0.1736, Time: 92.54 seconds\n",
      "2024-12-18 10:20: ********** Current best model saved!\n",
      "2024-12-18 10:21: Train Epoch 13: 0/158 Loss: 29.400909\n",
      "2024-12-18 10:22: Train Epoch 13: 20/158 Loss: 29.326483\n",
      "2024-12-18 10:23: Train Epoch 13: 40/158 Loss: 32.751999\n",
      "2024-12-18 10:25: Train Epoch 13: 60/158 Loss: 27.469315\n",
      "2024-12-18 10:26: Train Epoch 13: 80/158 Loss: 29.415594\n",
      "2024-12-18 10:28: Train Epoch 13: 100/158 Loss: 29.765209\n",
      "2024-12-18 10:29: Train Epoch 13: 120/158 Loss: 31.102858\n",
      "2024-12-18 10:30: Train Epoch 13: 140/158 Loss: 33.829720\n",
      "2024-12-18 10:32: Train Epoch 13: averaged Loss: 29.912606, MAE: 29.9126, RMSE: 53.0505, MAPE: 0.1792, tf_ratio: 1.000000, Time: 674.46 seconds\n",
      "2024-12-18 10:33: Validation Epoch 13: average Loss: 31.883443, MAE: 31.8834, RMSE: 56.2670, MAPE: 0.1700, Time: 92.60 seconds\n",
      "2024-12-18 10:33: ********** Current best model saved!\n",
      "2024-12-18 10:33: Train Epoch 14: 0/158 Loss: 28.762703\n",
      "2024-12-18 10:35: Train Epoch 14: 20/158 Loss: 26.378702\n",
      "2024-12-18 10:36: Train Epoch 14: 40/158 Loss: 31.271046\n",
      "2024-12-18 10:38: Train Epoch 14: 60/158 Loss: 27.933510\n",
      "2024-12-18 10:39: Train Epoch 14: 80/158 Loss: 28.592497\n",
      "2024-12-18 10:40: Train Epoch 14: 100/158 Loss: 29.608553\n",
      "2024-12-18 10:42: Train Epoch 14: 120/158 Loss: 27.609867\n",
      "2024-12-18 10:43: Train Epoch 14: 140/158 Loss: 29.078953\n",
      "2024-12-18 10:45: Train Epoch 14: averaged Loss: 28.469889, MAE: 28.4699, RMSE: 50.0207, MAPE: 0.1745, tf_ratio: 1.000000, Time: 678.93 seconds\n",
      "2024-12-18 10:46: Validation Epoch 14: average Loss: 30.321161, MAE: 30.3212, RMSE: 53.2071, MAPE: 0.1643, Time: 92.68 seconds\n",
      "2024-12-18 10:46: ********** Current best model saved!\n",
      "2024-12-18 10:46: Train Epoch 15: 0/158 Loss: 26.724171\n",
      "2024-12-18 10:48: Train Epoch 15: 20/158 Loss: 32.498447\n",
      "2024-12-18 10:49: Train Epoch 15: 40/158 Loss: 24.084652\n",
      "2024-12-18 10:50: Train Epoch 15: 60/158 Loss: 26.748281\n",
      "2024-12-18 10:52: Train Epoch 15: 80/158 Loss: 27.707663\n",
      "2024-12-18 10:53: Train Epoch 15: 100/158 Loss: 26.352774\n",
      "2024-12-18 10:55: Train Epoch 15: 120/158 Loss: 25.795546\n",
      "2024-12-18 10:56: Train Epoch 15: 140/158 Loss: 29.147875\n",
      "2024-12-18 10:57: Train Epoch 15: averaged Loss: 27.289932, MAE: 27.2899, RMSE: 47.5250, MAPE: 0.1709, tf_ratio: 1.000000, Time: 675.96 seconds\n",
      "2024-12-18 10:59: Validation Epoch 15: average Loss: 29.099606, MAE: 29.0996, RMSE: 50.6654, MAPE: 0.1615, Time: 92.82 seconds\n",
      "2024-12-18 10:59: ********** Current best model saved!\n",
      "2024-12-18 10:59: Train Epoch 16: 0/158 Loss: 25.864279\n",
      "2024-12-18 11:00: Train Epoch 16: 20/158 Loss: 25.811178\n",
      "2024-12-18 11:02: Train Epoch 16: 40/158 Loss: 24.466196\n",
      "2024-12-18 11:03: Train Epoch 16: 60/158 Loss: 25.378736\n",
      "2024-12-18 11:05: Train Epoch 16: 80/158 Loss: 25.732780\n",
      "2024-12-18 11:06: Train Epoch 16: 100/158 Loss: 26.872782\n",
      "2024-12-18 11:08: Train Epoch 16: 120/158 Loss: 25.848421\n",
      "2024-12-18 11:09: Train Epoch 16: 140/158 Loss: 23.881689\n",
      "2024-12-18 11:10: Train Epoch 16: averaged Loss: 26.426780, MAE: 26.4268, RMSE: 45.5852, MAPE: 0.1682, tf_ratio: 1.000000, Time: 676.73 seconds\n",
      "2024-12-18 11:12: Validation Epoch 16: average Loss: 28.247427, MAE: 28.2474, RMSE: 48.6954, MAPE: 0.1623, Time: 92.75 seconds\n",
      "2024-12-18 11:12: ********** Current best model saved!\n",
      "2024-12-18 11:12: Train Epoch 17: 0/158 Loss: 27.755537\n",
      "2024-12-18 11:13: Train Epoch 17: 20/158 Loss: 24.658722\n",
      "2024-12-18 11:15: Train Epoch 17: 40/158 Loss: 24.966311\n",
      "2024-12-18 11:16: Train Epoch 17: 60/158 Loss: 24.518324\n",
      "2024-12-18 11:18: Train Epoch 17: 80/158 Loss: 24.646866\n",
      "2024-12-18 11:19: Train Epoch 17: 100/158 Loss: 26.828312\n",
      "2024-12-18 11:21: Train Epoch 17: 120/158 Loss: 27.507847\n",
      "2024-12-18 11:22: Train Epoch 17: 140/158 Loss: 25.321720\n",
      "2024-12-18 11:23: Train Epoch 17: averaged Loss: 25.642705, MAE: 25.6427, RMSE: 43.8841, MAPE: 0.1663, tf_ratio: 1.000000, Time: 690.12 seconds\n",
      "2024-12-18 11:25: Validation Epoch 17: average Loss: 27.553984, MAE: 27.5540, RMSE: 47.0943, MAPE: 0.1558, Time: 95.29 seconds\n",
      "2024-12-18 11:25: ********** Current best model saved!\n",
      "2024-12-18 11:25: Train Epoch 18: 0/158 Loss: 25.246384\n",
      "2024-12-18 11:26: Train Epoch 18: 20/158 Loss: 25.775475\n",
      "2024-12-18 11:28: Train Epoch 18: 40/158 Loss: 25.851719\n",
      "2024-12-18 11:29: Train Epoch 18: 60/158 Loss: 24.907700\n",
      "2024-12-18 11:31: Train Epoch 18: 80/158 Loss: 27.744989\n",
      "2024-12-18 11:32: Train Epoch 18: 100/158 Loss: 24.787247\n",
      "2024-12-18 11:34: Train Epoch 18: 120/158 Loss: 25.859133\n",
      "2024-12-18 11:35: Train Epoch 18: 140/158 Loss: 26.912550\n",
      "2024-12-18 11:36: Train Epoch 18: averaged Loss: 24.912944, MAE: 24.9129, RMSE: 42.4171, MAPE: 0.1626, tf_ratio: 1.000000, Time: 690.84 seconds\n",
      "2024-12-18 11:38: Validation Epoch 18: average Loss: 26.573418, MAE: 26.5734, RMSE: 45.3115, MAPE: 0.1562, Time: 94.52 seconds\n",
      "2024-12-18 11:38: ********** Current best model saved!\n",
      "2024-12-18 11:38: Train Epoch 19: 0/158 Loss: 23.035662\n",
      "2024-12-18 11:39: Train Epoch 19: 20/158 Loss: 24.376049\n",
      "2024-12-18 11:41: Train Epoch 19: 40/158 Loss: 25.864149\n",
      "2024-12-18 11:42: Train Epoch 19: 60/158 Loss: 23.137329\n",
      "2024-12-18 11:44: Train Epoch 19: 80/158 Loss: 22.451559\n",
      "2024-12-18 11:45: Train Epoch 19: 100/158 Loss: 23.894510\n",
      "2024-12-18 11:47: Train Epoch 19: 120/158 Loss: 24.715641\n",
      "2024-12-18 11:48: Train Epoch 19: 140/158 Loss: 24.983662\n",
      "2024-12-18 11:50: Train Epoch 19: averaged Loss: 24.300965, MAE: 24.3010, RMSE: 41.1517, MAPE: 0.1602, tf_ratio: 1.000000, Time: 695.48 seconds\n",
      "2024-12-18 11:51: Validation Epoch 19: average Loss: 25.963539, MAE: 25.9635, RMSE: 44.0486, MAPE: 0.1504, Time: 102.70 seconds\n",
      "2024-12-18 11:51: ********** Current best model saved!\n",
      "2024-12-18 11:51: Train Epoch 20: 0/158 Loss: 23.959156\n",
      "2024-12-18 11:53: Train Epoch 20: 20/158 Loss: 23.424292\n",
      "2024-12-18 11:54: Train Epoch 20: 40/158 Loss: 24.028942\n",
      "2024-12-18 11:56: Train Epoch 20: 60/158 Loss: 21.998631\n",
      "2024-12-18 11:57: Train Epoch 20: 80/158 Loss: 24.048834\n",
      "2024-12-18 11:59: Train Epoch 20: 100/158 Loss: 23.993666\n",
      "2024-12-18 12:00: Train Epoch 20: 120/158 Loss: 24.330517\n",
      "2024-12-18 12:01: Train Epoch 20: 140/158 Loss: 23.372591\n",
      "2024-12-18 12:03: Train Epoch 20: averaged Loss: 23.821327, MAE: 23.8213, RMSE: 40.1070, MAPE: 0.1588, tf_ratio: 1.000000, Time: 684.51 seconds\n",
      "2024-12-18 12:04: Validation Epoch 20: average Loss: 25.475453, MAE: 25.4755, RMSE: 42.9502, MAPE: 0.1494, Time: 93.59 seconds\n",
      "2024-12-18 12:04: ********** Current best model saved!\n",
      "2024-12-18 12:04: Train Epoch 21: 0/158 Loss: 24.191988\n",
      "2024-12-18 12:06: Train Epoch 21: 20/158 Loss: 23.388147\n",
      "2024-12-18 12:07: Train Epoch 21: 40/158 Loss: 21.998825\n",
      "2024-12-18 12:09: Train Epoch 21: 60/158 Loss: 24.953358\n",
      "2024-12-18 12:10: Train Epoch 21: 80/158 Loss: 23.957388\n",
      "2024-12-18 12:11: Train Epoch 21: 100/158 Loss: 21.429026\n",
      "2024-12-18 12:13: Train Epoch 21: 120/158 Loss: 24.012558\n",
      "2024-12-18 12:14: Train Epoch 21: 140/158 Loss: 22.390980\n",
      "2024-12-18 12:15: Train Epoch 21: averaged Loss: 23.305603, MAE: 23.3056, RMSE: 39.1171, MAPE: 0.1560, tf_ratio: 1.000000, Time: 674.68 seconds\n",
      "2024-12-18 12:17: Validation Epoch 21: average Loss: 24.971373, MAE: 24.9714, RMSE: 41.9261, MAPE: 0.1477, Time: 95.89 seconds\n",
      "2024-12-18 12:17: ********** Current best model saved!\n",
      "2024-12-18 12:17: Train Epoch 22: 0/158 Loss: 23.988440\n",
      "2024-12-18 12:19: Train Epoch 22: 20/158 Loss: 22.614710\n",
      "2024-12-18 12:20: Train Epoch 22: 40/158 Loss: 22.786066\n",
      "2024-12-18 12:21: Train Epoch 22: 60/158 Loss: 22.229612\n",
      "2024-12-18 12:23: Train Epoch 22: 80/158 Loss: 23.862476\n",
      "2024-12-18 12:25: Train Epoch 22: 100/158 Loss: 24.750410\n",
      "2024-12-18 12:26: Train Epoch 22: 120/158 Loss: 24.132414\n",
      "2024-12-18 12:27: Train Epoch 22: 140/158 Loss: 22.534157\n",
      "2024-12-18 12:29: Train Epoch 22: averaged Loss: 22.858737, MAE: 22.8587, RMSE: 38.2506, MAPE: 0.1538, tf_ratio: 1.000000, Time: 694.98 seconds\n",
      "2024-12-18 12:30: Validation Epoch 22: average Loss: 24.399594, MAE: 24.3996, RMSE: 40.8874, MAPE: 0.1441, Time: 95.19 seconds\n",
      "2024-12-18 12:30: ********** Current best model saved!\n",
      "2024-12-18 12:30: Train Epoch 23: 0/158 Loss: 23.080273\n",
      "2024-12-18 12:32: Train Epoch 23: 20/158 Loss: 22.488060\n",
      "2024-12-18 12:33: Train Epoch 23: 40/158 Loss: 22.337963\n",
      "2024-12-18 12:35: Train Epoch 23: 60/158 Loss: 23.524908\n",
      "2024-12-18 12:36: Train Epoch 23: 80/158 Loss: 20.178888\n",
      "2024-12-18 12:38: Train Epoch 23: 100/158 Loss: 24.302898\n",
      "2024-12-18 12:39: Train Epoch 23: 120/158 Loss: 24.137589\n",
      "2024-12-18 12:41: Train Epoch 23: 140/158 Loss: 22.909542\n",
      "2024-12-18 12:42: Train Epoch 23: averaged Loss: 22.525353, MAE: 22.5254, RMSE: 37.5396, MAPE: 0.1530, tf_ratio: 1.000000, Time: 711.02 seconds\n",
      "2024-12-18 12:44: Validation Epoch 23: average Loss: 23.899481, MAE: 23.8995, RMSE: 39.9655, MAPE: 0.1434, Time: 94.69 seconds\n",
      "2024-12-18 12:44: ********** Current best model saved!\n",
      "2024-12-18 12:44: Train Epoch 24: 0/158 Loss: 24.108315\n",
      "2024-12-18 12:45: Train Epoch 24: 20/158 Loss: 20.316305\n",
      "2024-12-18 12:47: Train Epoch 24: 40/158 Loss: 22.877197\n",
      "2024-12-18 12:48: Train Epoch 24: 60/158 Loss: 20.607264\n",
      "2024-12-18 12:49: Train Epoch 24: 80/158 Loss: 23.160690\n",
      "2024-12-18 12:51: Train Epoch 24: 100/158 Loss: 22.104288\n",
      "2024-12-18 12:52: Train Epoch 24: 120/158 Loss: 22.595482\n",
      "2024-12-18 12:54: Train Epoch 24: 140/158 Loss: 21.263647\n",
      "2024-12-18 12:55: Train Epoch 24: averaged Loss: 22.179273, MAE: 22.1793, RMSE: 36.8588, MAPE: 0.1512, tf_ratio: 1.000000, Time: 682.52 seconds\n",
      "2024-12-18 12:57: Validation Epoch 24: average Loss: 23.863744, MAE: 23.8637, RMSE: 39.5736, MAPE: 0.1424, Time: 97.22 seconds\n",
      "2024-12-18 12:57: ********** Current best model saved!\n",
      "2024-12-18 12:57: Train Epoch 25: 0/158 Loss: 22.130718\n",
      "2024-12-18 12:58: Train Epoch 25: 20/158 Loss: 20.771254\n",
      "2024-12-18 13:00: Train Epoch 25: 40/158 Loss: 23.934246\n",
      "2024-12-18 13:01: Train Epoch 25: 60/158 Loss: 23.231766\n",
      "2024-12-18 13:03: Train Epoch 25: 80/158 Loss: 23.018568\n",
      "2024-12-18 13:04: Train Epoch 25: 100/158 Loss: 22.233709\n",
      "2024-12-18 13:06: Train Epoch 25: 120/158 Loss: 23.578123\n",
      "2024-12-18 13:07: Train Epoch 25: 140/158 Loss: 23.527832\n",
      "2024-12-18 13:09: Train Epoch 25: averaged Loss: 21.928428, MAE: 21.9284, RMSE: 36.3186, MAPE: 0.1504, tf_ratio: 1.000000, Time: 710.85 seconds\n",
      "2024-12-18 13:10: Validation Epoch 25: average Loss: 23.350159, MAE: 23.3502, RMSE: 38.7156, MAPE: 0.1410, Time: 94.19 seconds\n",
      "2024-12-18 13:10: ********** Current best model saved!\n",
      "2024-12-18 13:10: Train Epoch 26: 0/158 Loss: 20.001780\n",
      "2024-12-18 13:12: Train Epoch 26: 20/158 Loss: 23.033821\n",
      "2024-12-18 13:13: Train Epoch 26: 40/158 Loss: 21.506212\n",
      "2024-12-18 13:14: Train Epoch 26: 60/158 Loss: 20.465731\n",
      "2024-12-18 13:16: Train Epoch 26: 80/158 Loss: 23.716572\n",
      "2024-12-18 13:17: Train Epoch 26: 100/158 Loss: 22.505390\n",
      "2024-12-18 13:19: Train Epoch 26: 120/158 Loss: 22.643332\n",
      "2024-12-18 13:20: Train Epoch 26: 140/158 Loss: 20.309479\n",
      "2024-12-18 13:21: Train Epoch 26: averaged Loss: 21.636083, MAE: 21.6361, RMSE: 35.7819, MAPE: 0.1487, tf_ratio: 1.000000, Time: 680.67 seconds\n",
      "2024-12-18 13:23: Validation Epoch 26: average Loss: 23.389293, MAE: 23.3893, RMSE: 38.4256, MAPE: 0.1425, Time: 93.42 seconds\n",
      "2024-12-18 13:23: Train Epoch 27: 0/158 Loss: 19.930174\n",
      "2024-12-18 13:25: Train Epoch 27: 20/158 Loss: 22.255989\n",
      "2024-12-18 13:26: Train Epoch 27: 40/158 Loss: 22.752726\n",
      "2024-12-18 13:27: Train Epoch 27: 60/158 Loss: 21.633698\n",
      "2024-12-18 13:29: Train Epoch 27: 80/158 Loss: 23.164328\n",
      "2024-12-18 13:30: Train Epoch 27: 100/158 Loss: 23.061872\n",
      "2024-12-18 13:32: Train Epoch 27: 120/158 Loss: 20.509600\n",
      "2024-12-18 13:33: Train Epoch 27: 140/158 Loss: 20.757111\n",
      "2024-12-18 13:34: Train Epoch 27: averaged Loss: 21.444780, MAE: 21.4448, RMSE: 35.3728, MAPE: 0.1477, tf_ratio: 1.000000, Time: 680.72 seconds\n",
      "2024-12-18 13:36: Validation Epoch 27: average Loss: 22.846652, MAE: 22.8467, RMSE: 37.6570, MAPE: 0.1377, Time: 93.07 seconds\n",
      "2024-12-18 13:36: ********** Current best model saved!\n",
      "2024-12-18 13:36: Train Epoch 28: 0/158 Loss: 22.970129\n",
      "2024-12-18 13:37: Train Epoch 28: 20/158 Loss: 21.493811\n",
      "2024-12-18 13:39: Train Epoch 28: 40/158 Loss: 22.255362\n",
      "2024-12-18 13:40: Train Epoch 28: 60/158 Loss: 21.600071\n",
      "2024-12-18 13:42: Train Epoch 28: 80/158 Loss: 21.415993\n",
      "2024-12-18 13:43: Train Epoch 28: 100/158 Loss: 21.925869\n",
      "2024-12-18 13:45: Train Epoch 28: 120/158 Loss: 22.052771\n",
      "2024-12-18 13:46: Train Epoch 28: 140/158 Loss: 22.483517\n",
      "2024-12-18 13:47: Train Epoch 28: averaged Loss: 21.262499, MAE: 21.2625, RMSE: 34.9823, MAPE: 0.1472, tf_ratio: 1.000000, Time: 679.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# learning rate 0.003---> 0.005\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "if args.mode.lower() == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    # Load the model on CPU\n",
    "    model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "    model.to(torch.device('cpu'))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db474e87-8755-4f8b-ad7e-08a17b7dce63",
   "metadata": {},
   "source": [
    "### initializing embedding\n",
    "#### Freezeing the E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc8dcf-5dab-420a-b8df-cca5eab2800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 10:35: Experiment log path in: C:\\Users\\Hoda\\A - Uni\\AGCRN\\data\\experiments\\PEMSD4\\20241216103526\n",
      "2024-12-16 10:35: Train Epoch 1: 0/158 Loss: 184.452835\n",
      "2024-12-16 10:36: Train Epoch 1: 20/158 Loss: 204.920364\n",
      "2024-12-16 10:38: Train Epoch 1: 40/158 Loss: 201.505905\n",
      "2024-12-16 10:39: Train Epoch 1: 60/158 Loss: 196.307312\n",
      "2024-12-16 10:41: Train Epoch 1: 80/158 Loss: 153.899933\n",
      "2024-12-16 10:42: Train Epoch 1: 100/158 Loss: 196.483658\n",
      "2024-12-16 10:44: Train Epoch 1: 120/158 Loss: 173.205887\n",
      "2024-12-16 10:45: Train Epoch 1: 140/158 Loss: 146.412079\n",
      "2024-12-16 10:46: **********Train Epoch 1: averaged Loss: 189.967392, tf_ratio: 1.000000\n",
      "2024-12-16 10:48: **********Val Epoch 1: average Loss: 188.231514\n",
      "2024-12-16 10:48: *********************************Current best model saved!\n",
      "2024-12-16 10:48: Train Epoch 2: 0/158 Loss: 181.957031\n",
      "2024-12-16 10:49: Train Epoch 2: 20/158 Loss: 146.381958\n",
      "2024-12-16 10:51: Train Epoch 2: 40/158 Loss: 169.571121\n",
      "2024-12-16 10:52: Train Epoch 2: 60/158 Loss: 165.654343\n",
      "2024-12-16 10:54: Train Epoch 2: 80/158 Loss: 186.978455\n",
      "2024-12-16 10:55: Train Epoch 2: 100/158 Loss: 174.126999\n",
      "2024-12-16 10:57: Train Epoch 2: 120/158 Loss: 161.609070\n",
      "2024-12-16 10:58: Train Epoch 2: 140/158 Loss: 161.022720\n",
      "2024-12-16 10:59: **********Train Epoch 2: averaged Loss: 164.098657, tf_ratio: 1.000000\n",
      "2024-12-16 11:01: **********Val Epoch 2: average Loss: 169.090686\n",
      "2024-12-16 11:01: *********************************Current best model saved!\n",
      "2024-12-16 11:01: Train Epoch 3: 0/158 Loss: 161.963120\n",
      "2024-12-16 11:02: Train Epoch 3: 20/158 Loss: 142.721542\n",
      "2024-12-16 11:04: Train Epoch 3: 40/158 Loss: 164.076950\n",
      "2024-12-16 11:05: Train Epoch 3: 60/158 Loss: 158.435638\n",
      "2024-12-16 11:07: Train Epoch 3: 80/158 Loss: 132.695389\n",
      "2024-12-16 11:08: Train Epoch 3: 100/158 Loss: 136.708023\n",
      "2024-12-16 11:10: Train Epoch 3: 120/158 Loss: 170.054779\n",
      "2024-12-16 11:12: Train Epoch 3: 140/158 Loss: 163.217621\n",
      "2024-12-16 11:13: **********Train Epoch 3: averaged Loss: 149.320623, tf_ratio: 1.000000\n",
      "2024-12-16 11:14: **********Val Epoch 3: average Loss: 156.268976\n",
      "2024-12-16 11:14: *********************************Current best model saved!\n",
      "2024-12-16 11:15: Train Epoch 4: 0/158 Loss: 142.823730\n",
      "2024-12-16 11:16: Train Epoch 4: 20/158 Loss: 138.525650\n",
      "2024-12-16 11:17: Train Epoch 4: 40/158 Loss: 139.248337\n",
      "2024-12-16 11:19: Train Epoch 4: 60/158 Loss: 134.579727\n",
      "2024-12-16 11:20: Train Epoch 4: 80/158 Loss: 154.468338\n",
      "2024-12-16 11:22: Train Epoch 4: 100/158 Loss: 127.851151\n",
      "2024-12-16 11:23: Train Epoch 4: 120/158 Loss: 139.916519\n",
      "2024-12-16 11:25: Train Epoch 4: 140/158 Loss: 130.713608\n",
      "2024-12-16 11:26: **********Train Epoch 4: averaged Loss: 139.506484, tf_ratio: 1.000000\n",
      "2024-12-16 11:28: **********Val Epoch 4: average Loss: 140.559720\n",
      "2024-12-16 11:28: *********************************Current best model saved!\n",
      "2024-12-16 11:28: Train Epoch 5: 0/158 Loss: 125.242126\n",
      "2024-12-16 11:29: Train Epoch 5: 20/158 Loss: 120.254646\n",
      "2024-12-16 11:31: Train Epoch 5: 40/158 Loss: 112.062317\n",
      "2024-12-16 11:32: Train Epoch 5: 60/158 Loss: 108.617203\n",
      "2024-12-16 11:34: Train Epoch 5: 80/158 Loss: 98.798164\n",
      "2024-12-16 11:35: Train Epoch 5: 100/158 Loss: 122.524452\n",
      "2024-12-16 11:37: Train Epoch 5: 120/158 Loss: 120.132942\n",
      "2024-12-16 11:38: Train Epoch 5: 140/158 Loss: 90.170410\n",
      "2024-12-16 11:39: **********Train Epoch 5: averaged Loss: 114.522186, tf_ratio: 1.000000\n",
      "2024-12-16 11:41: **********Val Epoch 5: average Loss: 116.192516\n",
      "2024-12-16 11:41: *********************************Current best model saved!\n",
      "2024-12-16 11:41: Train Epoch 6: 0/158 Loss: 103.618393\n",
      "2024-12-16 11:42: Train Epoch 6: 20/158 Loss: 97.779907\n",
      "2024-12-16 11:44: Train Epoch 6: 40/158 Loss: 99.491837\n",
      "2024-12-16 11:45: Train Epoch 6: 60/158 Loss: 91.245346\n",
      "2024-12-16 11:47: Train Epoch 6: 80/158 Loss: 108.275993\n",
      "2024-12-16 11:48: Train Epoch 6: 100/158 Loss: 80.499390\n",
      "2024-12-16 11:50: Train Epoch 6: 120/158 Loss: 84.054535\n",
      "2024-12-16 11:51: Train Epoch 6: 140/158 Loss: 90.582283\n",
      "2024-12-16 11:52: **********Train Epoch 6: averaged Loss: 97.054341, tf_ratio: 1.000000\n",
      "2024-12-16 11:54: **********Val Epoch 6: average Loss: 100.659247\n",
      "2024-12-16 11:54: *********************************Current best model saved!\n",
      "2024-12-16 11:54: Train Epoch 7: 0/158 Loss: 86.597382\n",
      "2024-12-16 11:55: Train Epoch 7: 20/158 Loss: 95.856888\n",
      "2024-12-16 11:57: Train Epoch 7: 40/158 Loss: 95.661407\n",
      "2024-12-16 11:58: Train Epoch 7: 60/158 Loss: 93.310829\n",
      "2024-12-16 12:00: Train Epoch 7: 80/158 Loss: 88.378456\n",
      "2024-12-16 12:01: Train Epoch 7: 100/158 Loss: 83.841019\n",
      "2024-12-16 12:03: Train Epoch 7: 120/158 Loss: 72.699539\n",
      "2024-12-16 12:04: Train Epoch 7: 140/158 Loss: 78.767654\n",
      "2024-12-16 12:05: **********Train Epoch 7: averaged Loss: 84.423961, tf_ratio: 1.000000\n",
      "2024-12-16 12:07: **********Val Epoch 7: average Loss: 88.658781\n",
      "2024-12-16 12:07: *********************************Current best model saved!\n",
      "2024-12-16 12:07: Train Epoch 8: 0/158 Loss: 75.811829\n",
      "2024-12-16 12:08: Train Epoch 8: 20/158 Loss: 82.281189\n",
      "2024-12-16 12:10: Train Epoch 8: 40/158 Loss: 64.798805\n",
      "2024-12-16 12:11: Train Epoch 8: 60/158 Loss: 67.706009\n",
      "2024-12-16 12:13: Train Epoch 8: 80/158 Loss: 79.762009\n",
      "2024-12-16 12:14: Train Epoch 8: 100/158 Loss: 68.320343\n",
      "2024-12-16 12:16: Train Epoch 8: 120/158 Loss: 61.635441\n",
      "2024-12-16 12:17: Train Epoch 8: 140/158 Loss: 80.026703\n",
      "2024-12-16 12:18: **********Train Epoch 8: averaged Loss: 74.555948, tf_ratio: 1.000000\n",
      "2024-12-16 12:20: **********Val Epoch 8: average Loss: 78.950103\n",
      "2024-12-16 12:20: *********************************Current best model saved!\n",
      "2024-12-16 12:20: Train Epoch 9: 0/158 Loss: 74.643364\n",
      "2024-12-16 12:21: Train Epoch 9: 20/158 Loss: 55.087616\n",
      "2024-12-16 12:23: Train Epoch 9: 40/158 Loss: 75.335426\n",
      "2024-12-16 12:25: Train Epoch 9: 60/158 Loss: 72.646362\n",
      "2024-12-16 12:26: Train Epoch 9: 80/158 Loss: 62.710934\n",
      "2024-12-16 12:27: Train Epoch 9: 100/158 Loss: 66.657654\n",
      "2024-12-16 12:29: Train Epoch 9: 120/158 Loss: 63.444824\n",
      "2024-12-16 12:30: Train Epoch 9: 140/158 Loss: 64.060020\n",
      "2024-12-16 12:32: **********Train Epoch 9: averaged Loss: 66.569595, tf_ratio: 1.000000\n",
      "2024-12-16 12:33: **********Val Epoch 9: average Loss: 70.883067\n",
      "2024-12-16 12:33: *********************************Current best model saved!\n",
      "2024-12-16 12:33: Train Epoch 10: 0/158 Loss: 65.019928\n",
      "2024-12-16 12:35: Train Epoch 10: 20/158 Loss: 59.319107\n",
      "2024-12-16 12:36: Train Epoch 10: 40/158 Loss: 60.876171\n",
      "2024-12-16 12:38: Train Epoch 10: 60/158 Loss: 59.253387\n",
      "2024-12-16 12:39: Train Epoch 10: 80/158 Loss: 62.024719\n",
      "2024-12-16 12:41: Train Epoch 10: 100/158 Loss: 58.949989\n",
      "2024-12-16 12:42: Train Epoch 10: 120/158 Loss: 59.909973\n",
      "2024-12-16 12:44: Train Epoch 10: 140/158 Loss: 57.623482\n",
      "2024-12-16 12:45: **********Train Epoch 10: averaged Loss: 59.811681, tf_ratio: 1.000000\n",
      "2024-12-16 12:47: **********Val Epoch 10: average Loss: 64.139779\n",
      "2024-12-16 12:47: *********************************Current best model saved!\n",
      "2024-12-16 12:47: Train Epoch 11: 0/158 Loss: 61.165310\n",
      "2024-12-16 12:48: Train Epoch 11: 20/158 Loss: 62.148827\n",
      "2024-12-16 12:49: Train Epoch 11: 40/158 Loss: 55.809273\n",
      "2024-12-16 12:51: Train Epoch 11: 60/158 Loss: 57.462765\n",
      "2024-12-16 12:52: Train Epoch 11: 80/158 Loss: 50.672672\n",
      "2024-12-16 12:54: Train Epoch 11: 100/158 Loss: 60.141193\n",
      "2024-12-16 12:55: Train Epoch 11: 120/158 Loss: 54.282124\n",
      "2024-12-16 12:57: Train Epoch 11: 140/158 Loss: 52.774490\n",
      "2024-12-16 12:58: **********Train Epoch 11: averaged Loss: 54.160203, tf_ratio: 1.000000\n",
      "2024-12-16 13:00: **********Val Epoch 11: average Loss: 57.992978\n",
      "2024-12-16 13:00: *********************************Current best model saved!\n",
      "2024-12-16 13:00: Train Epoch 12: 0/158 Loss: 50.115391\n",
      "2024-12-16 13:01: Train Epoch 12: 20/158 Loss: 48.179409\n",
      "2024-12-16 13:03: Train Epoch 12: 40/158 Loss: 43.382896\n",
      "2024-12-16 13:04: Train Epoch 12: 60/158 Loss: 52.418892\n",
      "2024-12-16 13:05: Train Epoch 12: 80/158 Loss: 49.872776\n",
      "2024-12-16 13:07: Train Epoch 12: 100/158 Loss: 46.686882\n",
      "2024-12-16 13:08: Train Epoch 12: 120/158 Loss: 52.187630\n",
      "2024-12-16 13:10: Train Epoch 12: 140/158 Loss: 40.304443\n",
      "2024-12-16 13:11: **********Train Epoch 12: averaged Loss: 49.296230, tf_ratio: 1.000000\n",
      "2024-12-16 13:13: **********Val Epoch 12: average Loss: 52.974397\n",
      "2024-12-16 13:13: *********************************Current best model saved!\n",
      "2024-12-16 13:13: Train Epoch 13: 0/158 Loss: 44.754295\n",
      "2024-12-16 13:14: Train Epoch 13: 20/158 Loss: 44.303146\n",
      "2024-12-16 13:16: Train Epoch 13: 40/158 Loss: 49.797195\n",
      "2024-12-16 13:17: Train Epoch 13: 60/158 Loss: 40.594902\n",
      "2024-12-16 13:19: Train Epoch 13: 80/158 Loss: 45.033459\n",
      "2024-12-16 13:20: Train Epoch 13: 100/158 Loss: 45.149498\n",
      "2024-12-16 13:22: Train Epoch 13: 120/158 Loss: 47.521488\n",
      "2024-12-16 13:23: Train Epoch 13: 140/158 Loss: 51.181690\n",
      "2024-12-16 13:24: **********Train Epoch 13: averaged Loss: 45.314954, tf_ratio: 1.000000\n",
      "2024-12-16 13:26: **********Val Epoch 13: average Loss: 48.591657\n",
      "2024-12-16 13:26: *********************************Current best model saved!\n",
      "2024-12-16 13:26: Train Epoch 14: 0/158 Loss: 42.375214\n",
      "2024-12-16 13:27: Train Epoch 14: 20/158 Loss: 38.153549\n",
      "2024-12-16 13:29: Train Epoch 14: 40/158 Loss: 46.131245\n",
      "2024-12-16 13:30: Train Epoch 14: 60/158 Loss: 41.015480\n",
      "2024-12-16 13:32: Train Epoch 14: 80/158 Loss: 41.915188\n",
      "2024-12-16 13:33: Train Epoch 14: 100/158 Loss: 43.106068\n",
      "2024-12-16 13:34: Train Epoch 14: 120/158 Loss: 40.427879\n",
      "2024-12-16 13:36: Train Epoch 14: 140/158 Loss: 42.853577\n",
      "2024-12-16 13:37: **********Train Epoch 14: averaged Loss: 41.843962, tf_ratio: 1.000000\n",
      "2024-12-16 13:39: **********Val Epoch 14: average Loss: 45.058204\n",
      "2024-12-16 13:39: *********************************Current best model saved!\n",
      "2024-12-16 13:39: Train Epoch 15: 0/158 Loss: 38.717110\n",
      "2024-12-16 13:40: Train Epoch 15: 20/158 Loss: 48.235264\n",
      "2024-12-16 13:42: Train Epoch 15: 40/158 Loss: 34.318165\n",
      "2024-12-16 13:43: Train Epoch 15: 60/158 Loss: 38.279617\n",
      "2024-12-16 13:44: Train Epoch 15: 80/158 Loss: 39.279640\n",
      "2024-12-16 13:46: Train Epoch 15: 100/158 Loss: 37.473030\n",
      "2024-12-16 13:47: Train Epoch 15: 120/158 Loss: 35.853230\n",
      "2024-12-16 13:49: Train Epoch 15: 140/158 Loss: 40.958019\n",
      "2024-12-16 13:50: **********Train Epoch 15: averaged Loss: 38.910160, tf_ratio: 1.000000\n",
      "2024-12-16 13:52: **********Val Epoch 15: average Loss: 41.886524\n",
      "2024-12-16 13:52: *********************************Current best model saved!\n",
      "2024-12-16 13:52: Train Epoch 16: 0/158 Loss: 35.782566\n",
      "2024-12-16 13:53: Train Epoch 16: 20/158 Loss: 35.932556\n",
      "2024-12-16 13:55: Train Epoch 16: 40/158 Loss: 33.897449\n",
      "2024-12-16 13:57: Train Epoch 16: 60/158 Loss: 34.396561\n",
      "2024-12-16 13:58: Train Epoch 16: 80/158 Loss: 34.933487\n",
      "2024-12-16 14:00: Train Epoch 16: 100/158 Loss: 36.814308\n",
      "2024-12-16 14:01: Train Epoch 16: 120/158 Loss: 35.349144\n",
      "2024-12-16 14:02: Train Epoch 16: 140/158 Loss: 32.311932\n",
      "2024-12-16 14:04: **********Train Epoch 16: averaged Loss: 36.532929, tf_ratio: 1.000000\n",
      "2024-12-16 14:06: **********Val Epoch 16: average Loss: 39.272044\n",
      "2024-12-16 14:06: *********************************Current best model saved!\n",
      "2024-12-16 14:06: Train Epoch 17: 0/158 Loss: 38.011349\n",
      "2024-12-16 14:07: Train Epoch 17: 20/158 Loss: 32.872643\n",
      "2024-12-16 14:09: Train Epoch 17: 40/158 Loss: 34.187920\n",
      "2024-12-16 14:10: Train Epoch 17: 60/158 Loss: 32.667618\n",
      "2024-12-16 14:12: Train Epoch 17: 80/158 Loss: 32.532921\n",
      "2024-12-16 14:13: Train Epoch 17: 100/158 Loss: 36.063171\n",
      "2024-12-16 14:15: Train Epoch 17: 120/158 Loss: 36.379292\n",
      "2024-12-16 14:16: Train Epoch 17: 140/158 Loss: 33.486080\n",
      "2024-12-16 14:17: **********Train Epoch 17: averaged Loss: 34.456727, tf_ratio: 1.000000\n",
      "2024-12-16 14:19: **********Val Epoch 17: average Loss: 37.107309\n",
      "2024-12-16 14:19: *********************************Current best model saved!\n",
      "2024-12-16 14:19: Train Epoch 18: 0/158 Loss: 34.098766\n",
      "2024-12-16 14:20: Train Epoch 18: 20/158 Loss: 34.394279\n",
      "2024-12-16 14:22: Train Epoch 18: 40/158 Loss: 35.138756\n",
      "2024-12-16 14:23: Train Epoch 18: 60/158 Loss: 33.973606\n",
      "2024-12-16 14:25: Train Epoch 18: 80/158 Loss: 37.228107\n",
      "2024-12-16 14:26: Train Epoch 18: 100/158 Loss: 32.032116\n",
      "2024-12-16 14:28: Train Epoch 18: 120/158 Loss: 34.069176\n",
      "2024-12-16 14:29: Train Epoch 18: 140/158 Loss: 35.636951\n",
      "2024-12-16 14:30: **********Train Epoch 18: averaged Loss: 32.736374, tf_ratio: 1.000000\n",
      "2024-12-16 14:32: **********Val Epoch 18: average Loss: 35.280301\n",
      "2024-12-16 14:32: *********************************Current best model saved!\n",
      "2024-12-16 14:32: Train Epoch 19: 0/158 Loss: 30.120979\n",
      "2024-12-16 14:33: Train Epoch 19: 20/158 Loss: 31.861210\n",
      "2024-12-16 14:35: Train Epoch 19: 40/158 Loss: 33.070965\n",
      "2024-12-16 14:36: Train Epoch 19: 60/158 Loss: 30.209129\n",
      "2024-12-16 14:38: Train Epoch 19: 80/158 Loss: 28.699877\n",
      "2024-12-16 14:39: Train Epoch 19: 100/158 Loss: 30.422352\n",
      "2024-12-16 14:40: Train Epoch 19: 120/158 Loss: 32.162838\n",
      "2024-12-16 14:42: Train Epoch 19: 140/158 Loss: 32.008018\n",
      "2024-12-16 14:43: **********Train Epoch 19: averaged Loss: 31.327970, tf_ratio: 1.000000\n",
      "2024-12-16 14:45: **********Val Epoch 19: average Loss: 33.724893\n",
      "2024-12-16 14:45: *********************************Current best model saved!\n",
      "2024-12-16 14:45: Train Epoch 20: 0/158 Loss: 30.874868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "if args.mode.lower() == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    # Load the model on CPU\n",
    "    model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "    model.to(torch.device('cpu'))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5b065-64f1-4254-9811-a4e1e9be0ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed3534-f193-45f0-90c6-500ee84774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to JSON file at the end of training\n",
    "metrics_path = os.path.join(self.args.log_dir, 'training_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(self.metrics, f, indent=4)\n",
    "self.logger.info(f\"Training metrics saved to {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60729460-d5e5-4d87-8e0c-de20e63c7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('path/to/log_dir/training_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Access metrics for comparison\n",
    "train_loss_epoch_1 = metrics['train']['epoch_1']['loss']\n",
    "val_mae_epoch_1 = metrics['val']['epoch_1']['mae']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581707b-eeea-4360-a20c-b8bdfa801691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load metrics from JSON file\n",
    "metrics_path = 'path/to/log_dir/training_metrics.json'\n",
    "with open(metrics_path, 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Extract data for plotting\n",
    "epochs = range(1, len(metrics['train']) + 1)\n",
    "train_loss = [metrics['train'][f'epoch_{epoch}']['loss'] for epoch in epochs]\n",
    "val_loss = [metrics['val'][f'epoch_{epoch}']['loss'] for epoch in epochs]\n",
    "train_mae = [metrics['train'][f'epoch_{epoch}']['mae'] for epoch in epochs]\n",
    "val_mae = [metrics['val'][f'epoch_{epoch}']['mae'] for epoch in epochs]\n",
    "epoch_times = [metrics['train'][f'epoch_{epoch}']['time'] for epoch in epochs]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_mae, label='Train MAE', marker='o')\n",
    "plt.plot(epochs, val_mae, label='Validation MAE', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot epoch times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, epoch_times, label='Epoch Time', marker='o', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Time Per Epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e0d35-aeab-4324-8f7f-6bd10e9506a3",
   "metadata": {},
   "source": [
    "# E_init, SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70818b3d-6899-4f8d-87bf-395a12775d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 11:20: Experiment log path in: C:\\Users\\Hoda\\A - Uni\\AGCRN\\data\\experiments\\PEMSD4\\20241212112010\n",
      "2024-12-12 11:20: Train Epoch 1: 0/158 Loss: 184.452835\n",
      "2024-12-12 11:21: Train Epoch 1: 20/158 Loss: 201.930542\n",
      "2024-12-12 11:22: Train Epoch 1: 40/158 Loss: 199.674881\n",
      "2024-12-12 11:24: Train Epoch 1: 60/158 Loss: 197.639404\n",
      "2024-12-12 11:25: Train Epoch 1: 80/158 Loss: 156.176819\n",
      "2024-12-12 11:26: Train Epoch 1: 100/158 Loss: 199.525040\n",
      "2024-12-12 11:28: Train Epoch 1: 120/158 Loss: 176.501999\n",
      "2024-12-12 11:29: Train Epoch 1: 140/158 Loss: 149.526260\n",
      "2024-12-12 11:30: **********Train Epoch 1: averaged Loss: 191.300976, tf_ratio: 1.000000\n",
      "2024-12-12 11:31: **********Val Epoch 1: average Loss: 191.946438\n",
      "2024-12-12 11:31: *********************************Current best model saved!\n",
      "2024-12-12 11:31: Train Epoch 2: 0/158 Loss: 185.894775\n",
      "2024-12-12 11:33: Train Epoch 2: 20/158 Loss: 149.874542\n",
      "2024-12-12 11:34: Train Epoch 2: 40/158 Loss: 173.394089\n",
      "2024-12-12 11:36: Train Epoch 2: 60/158 Loss: 169.304886\n",
      "2024-12-12 11:37: Train Epoch 2: 80/158 Loss: 191.171387\n",
      "2024-12-12 11:38: Train Epoch 2: 100/158 Loss: 178.104797\n",
      "2024-12-12 11:40: Train Epoch 2: 120/158 Loss: 165.447250\n",
      "2024-12-12 11:41: Train Epoch 2: 140/158 Loss: 164.744751\n",
      "2024-12-12 11:42: **********Train Epoch 2: averaged Loss: 167.789054, tf_ratio: 1.000000\n",
      "2024-12-12 11:44: **********Val Epoch 2: average Loss: 172.920804\n",
      "2024-12-12 11:44: *********************************Current best model saved!\n",
      "2024-12-12 11:44: Train Epoch 3: 0/158 Loss: 165.789871\n",
      "2024-12-12 11:45: Train Epoch 3: 20/158 Loss: 145.898682\n",
      "2024-12-12 11:46: Train Epoch 3: 40/158 Loss: 168.035965\n",
      "2024-12-12 11:48: Train Epoch 3: 60/158 Loss: 162.226685\n",
      "2024-12-12 11:49: Train Epoch 3: 80/158 Loss: 135.531357\n",
      "2024-12-12 11:50: Train Epoch 3: 100/158 Loss: 139.044647\n",
      "2024-12-12 11:52: Train Epoch 3: 120/158 Loss: 174.384857\n",
      "2024-12-12 11:53: Train Epoch 3: 140/158 Loss: 166.901398\n",
      "2024-12-12 11:55: **********Train Epoch 3: averaged Loss: 152.619736, tf_ratio: 1.000000\n",
      "2024-12-12 11:56: **********Val Epoch 3: average Loss: 159.560705\n",
      "2024-12-12 11:56: *********************************Current best model saved!\n",
      "2024-12-12 11:56: Train Epoch 4: 0/158 Loss: 145.741562\n",
      "2024-12-12 11:57: Train Epoch 4: 20/158 Loss: 141.718277\n",
      "2024-12-12 11:59: Train Epoch 4: 40/158 Loss: 141.780319\n",
      "2024-12-12 12:00: Train Epoch 4: 60/158 Loss: 136.911957\n",
      "2024-12-12 12:01: Train Epoch 4: 80/158 Loss: 157.517853\n",
      "2024-12-12 12:03: Train Epoch 4: 100/158 Loss: 129.076233\n",
      "2024-12-12 12:04: Train Epoch 4: 120/158 Loss: 142.289993\n",
      "2024-12-12 12:05: Train Epoch 4: 140/158 Loss: 132.945694\n",
      "2024-12-12 12:07: **********Train Epoch 4: averaged Loss: 142.493547, tf_ratio: 1.000000\n",
      "2024-12-12 12:08: **********Val Epoch 4: average Loss: 150.270327\n",
      "2024-12-12 12:08: *********************************Current best model saved!\n",
      "2024-12-12 12:08: Train Epoch 5: 0/158 Loss: 138.265884\n",
      "2024-12-12 12:09: Train Epoch 5: 20/158 Loss: 134.012756\n",
      "2024-12-12 12:11: Train Epoch 5: 40/158 Loss: 128.183777\n",
      "2024-12-12 12:12: Train Epoch 5: 60/158 Loss: 130.285995\n",
      "2024-12-12 12:14: Train Epoch 5: 80/158 Loss: 120.682709\n",
      "2024-12-12 12:15: Train Epoch 5: 100/158 Loss: 139.554214\n",
      "2024-12-12 12:17: Train Epoch 5: 120/158 Loss: 131.029373\n",
      "2024-12-12 12:18: Train Epoch 5: 140/158 Loss: 98.221481\n",
      "2024-12-12 12:19: **********Train Epoch 5: averaged Loss: 129.887521, tf_ratio: 1.000000\n",
      "2024-12-12 12:21: **********Val Epoch 5: average Loss: 124.493850\n",
      "2024-12-12 12:21: *********************************Current best model saved!\n",
      "2024-12-12 12:21: Train Epoch 6: 0/158 Loss: 111.226318\n",
      "2024-12-12 12:22: Train Epoch 6: 20/158 Loss: 104.742409\n",
      "2024-12-12 12:24: Train Epoch 6: 40/158 Loss: 106.478165\n",
      "2024-12-12 12:25: Train Epoch 6: 60/158 Loss: 97.137115\n",
      "2024-12-12 12:27: Train Epoch 6: 80/158 Loss: 115.485138\n",
      "2024-12-12 12:28: Train Epoch 6: 100/158 Loss: 85.753265\n",
      "2024-12-12 12:29: Train Epoch 6: 120/158 Loss: 89.375229\n",
      "2024-12-12 12:31: Train Epoch 6: 140/158 Loss: 96.371231\n",
      "2024-12-12 12:32: **********Train Epoch 6: averaged Loss: 103.494773, tf_ratio: 1.000000\n",
      "2024-12-12 12:33: **********Val Epoch 6: average Loss: 106.693701\n",
      "2024-12-12 12:33: *********************************Current best model saved!\n",
      "2024-12-12 12:33: Train Epoch 7: 0/158 Loss: 91.947357\n",
      "2024-12-12 12:35: Train Epoch 7: 20/158 Loss: 101.653336\n",
      "2024-12-12 12:36: Train Epoch 7: 40/158 Loss: 101.555450\n",
      "2024-12-12 12:38: Train Epoch 7: 60/158 Loss: 99.062508\n",
      "2024-12-12 12:39: Train Epoch 7: 80/158 Loss: 93.455498\n",
      "2024-12-12 12:41: Train Epoch 7: 100/158 Loss: 88.632278\n",
      "2024-12-12 12:42: Train Epoch 7: 120/158 Loss: 76.499641\n",
      "2024-12-12 12:44: Train Epoch 7: 140/158 Loss: 82.886078\n",
      "2024-12-12 12:45: **********Train Epoch 7: averaged Loss: 89.272399, tf_ratio: 1.000000\n",
      "2024-12-12 12:46: **********Val Epoch 7: average Loss: 93.303583\n",
      "2024-12-12 12:46: *********************************Current best model saved!\n",
      "2024-12-12 12:46: Train Epoch 8: 0/158 Loss: 79.613731\n",
      "2024-12-12 12:48: Train Epoch 8: 20/158 Loss: 86.592758\n",
      "2024-12-12 12:49: Train Epoch 8: 40/158 Loss: 68.086792\n",
      "2024-12-12 12:51: Train Epoch 8: 60/158 Loss: 71.540894\n",
      "2024-12-12 12:53: Train Epoch 8: 80/158 Loss: 83.471931\n",
      "2024-12-12 12:54: Train Epoch 8: 100/158 Loss: 71.728340\n",
      "2024-12-12 12:56: Train Epoch 8: 120/158 Loss: 64.546295\n",
      "2024-12-12 12:58: Train Epoch 8: 140/158 Loss: 84.443665\n",
      "2024-12-12 12:59: **********Train Epoch 8: averaged Loss: 78.311383, tf_ratio: 1.000000\n",
      "2024-12-12 13:00: **********Val Epoch 8: average Loss: 82.720894\n",
      "2024-12-12 13:00: *********************************Current best model saved!\n",
      "2024-12-12 13:00: Train Epoch 9: 0/158 Loss: 78.397224\n",
      "2024-12-12 13:02: Train Epoch 9: 20/158 Loss: 57.369148\n",
      "2024-12-12 13:03: Train Epoch 9: 40/158 Loss: 79.059387\n",
      "2024-12-12 13:04: Train Epoch 9: 60/158 Loss: 76.084602\n",
      "2024-12-12 13:06: Train Epoch 9: 80/158 Loss: 65.672554\n",
      "2024-12-12 13:07: Train Epoch 9: 100/158 Loss: 69.856644\n",
      "2024-12-12 13:08: Train Epoch 9: 120/158 Loss: 66.331795\n",
      "2024-12-12 13:09: Train Epoch 9: 140/158 Loss: 67.065666\n",
      "2024-12-12 13:10: **********Train Epoch 9: averaged Loss: 69.677727, tf_ratio: 1.000000\n",
      "2024-12-12 13:12: **********Val Epoch 9: average Loss: 74.094494\n",
      "2024-12-12 13:12: *********************************Current best model saved!\n",
      "2024-12-12 13:12: Train Epoch 10: 0/158 Loss: 68.021156\n",
      "2024-12-12 13:13: Train Epoch 10: 20/158 Loss: 61.663792\n",
      "2024-12-12 13:15: Train Epoch 10: 40/158 Loss: 63.594994\n",
      "2024-12-12 13:17: Train Epoch 10: 60/158 Loss: 61.852955\n",
      "2024-12-12 13:18: Train Epoch 10: 80/158 Loss: 64.736778\n",
      "2024-12-12 13:20: Train Epoch 10: 100/158 Loss: 61.464619\n",
      "2024-12-12 13:21: Train Epoch 10: 120/158 Loss: 62.643578\n",
      "2024-12-12 13:23: Train Epoch 10: 140/158 Loss: 60.025341\n",
      "2024-12-12 13:24: **********Train Epoch 10: averaged Loss: 62.397351, tf_ratio: 1.000000\n",
      "2024-12-12 13:25: **********Val Epoch 10: average Loss: 66.584057\n",
      "2024-12-12 13:25: *********************************Current best model saved!\n",
      "2024-12-12 13:25: Train Epoch 11: 0/158 Loss: 63.815056\n",
      "2024-12-12 13:27: Train Epoch 11: 20/158 Loss: 64.937599\n",
      "2024-12-12 13:28: Train Epoch 11: 40/158 Loss: 57.805096\n",
      "2024-12-12 13:30: Train Epoch 11: 60/158 Loss: 59.971783\n",
      "2024-12-12 13:31: Train Epoch 11: 80/158 Loss: 52.544685\n",
      "2024-12-12 13:33: Train Epoch 11: 100/158 Loss: 62.658287\n",
      "2024-12-12 13:34: Train Epoch 11: 120/158 Loss: 56.328945\n",
      "2024-12-12 13:36: Train Epoch 11: 140/158 Loss: 55.018253\n",
      "2024-12-12 13:37: **********Train Epoch 11: averaged Loss: 56.276834, tf_ratio: 1.000000\n",
      "2024-12-12 13:39: **********Val Epoch 11: average Loss: 60.239075\n",
      "2024-12-12 13:39: *********************************Current best model saved!\n",
      "2024-12-12 13:39: Train Epoch 12: 0/158 Loss: 51.944592\n",
      "2024-12-12 13:40: Train Epoch 12: 20/158 Loss: 49.952293\n",
      "2024-12-12 13:42: Train Epoch 12: 40/158 Loss: 44.736408\n",
      "2024-12-12 13:43: Train Epoch 12: 60/158 Loss: 54.418827\n",
      "2024-12-12 13:45: Train Epoch 12: 80/158 Loss: 51.711079\n",
      "2024-12-12 13:46: Train Epoch 12: 100/158 Loss: 48.136459\n",
      "2024-12-12 13:48: Train Epoch 12: 120/158 Loss: 54.310799\n",
      "2024-12-12 13:49: Train Epoch 12: 140/158 Loss: 41.480274\n",
      "2024-12-12 13:51: **********Train Epoch 12: averaged Loss: 50.985913, tf_ratio: 1.000000\n",
      "2024-12-12 13:52: **********Val Epoch 12: average Loss: 54.691642\n",
      "2024-12-12 13:52: *********************************Current best model saved!\n",
      "2024-12-12 13:52: Train Epoch 13: 0/158 Loss: 46.173840\n",
      "2024-12-12 13:54: Train Epoch 13: 20/158 Loss: 45.410267\n",
      "2024-12-12 13:55: Train Epoch 13: 40/158 Loss: 51.054001\n",
      "2024-12-12 13:57: Train Epoch 13: 60/158 Loss: 41.587440\n",
      "2024-12-12 13:58: Train Epoch 13: 80/158 Loss: 46.260731\n",
      "2024-12-12 14:00: Train Epoch 13: 100/158 Loss: 46.316837\n",
      "2024-12-12 14:01: Train Epoch 13: 120/158 Loss: 48.703720\n",
      "2024-12-12 14:03: Train Epoch 13: 140/158 Loss: 52.883141\n",
      "2024-12-12 14:04: **********Train Epoch 13: averaged Loss: 46.540716, tf_ratio: 1.000000\n",
      "2024-12-12 14:05: **********Val Epoch 13: average Loss: 50.078393\n",
      "2024-12-12 14:05: *********************************Current best model saved!\n",
      "2024-12-12 14:05: Train Epoch 14: 0/158 Loss: 43.489887\n",
      "2024-12-12 14:07: Train Epoch 14: 20/158 Loss: 38.651840\n",
      "2024-12-12 14:09: Train Epoch 14: 40/158 Loss: 47.343372\n",
      "2024-12-12 14:10: Train Epoch 14: 60/158 Loss: 41.811794\n",
      "2024-12-12 14:12: Train Epoch 14: 80/158 Loss: 43.072945\n",
      "2024-12-12 14:13: Train Epoch 14: 100/158 Loss: 44.077267\n",
      "2024-12-12 14:14: Train Epoch 14: 120/158 Loss: 41.219730\n",
      "2024-12-12 14:15: Train Epoch 14: 140/158 Loss: 43.445286\n",
      "2024-12-12 14:16: **********Train Epoch 14: averaged Loss: 42.673868, tf_ratio: 1.000000\n",
      "2024-12-12 14:18: **********Val Epoch 14: average Loss: 45.927136\n",
      "2024-12-12 14:18: *********************************Current best model saved!\n",
      "2024-12-12 14:18: Train Epoch 15: 0/158 Loss: 39.302917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n\u001b[0;32m     35\u001b[0m                   args, lr_scheduler\u001b[38;5;241m=\u001b[39mlr_scheduler)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Load the model on CPU\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../pre-trained/.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mdataset), map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[1;32m~\\A - Uni\\thesis\\AGCRN_GSL-init\\model\\BasicTrainer.py:108\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m#epoch_time = time.time()\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     train_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch(epoch)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m#print(time.time()-epoch_time)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m#exit()\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\A - Uni\\thesis\\AGCRN_GSL-init\\model\\BasicTrainer.py:74\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     72\u001b[0m     teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#data and target shape: B, T, N, F; output shape: B, T, N, F\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data, target, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39mteacher_forcing_ratio)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreal_value:\n\u001b[0;32m     76\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(label)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\A - Uni\\thesis\\AGCRN_GSL-init\\model\\AGCRN.py:76\u001b[0m, in \u001b[0;36mAGCRN.forward\u001b[1;34m(self, source, targets, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, targets, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# source: B, T_1, N, D\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# target: B, T_2, N, D\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# supports = F.softmax(F.relu(torch.mm(self.nodevec1, self.nodevec1.transpose(0, 1))), dim=1)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     init_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39minit_hidden(source\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 76\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(source, init_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_embeddings)  \u001b[38;5;66;03m# B, T, N, hidden\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :, :]  \u001b[38;5;66;03m# B, 1, N, hidden\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# CNN-based predictor\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\A - Uni\\thesis\\AGCRN_GSL-init\\model\\AGCRN.py:28\u001b[0m, in \u001b[0;36mAVWDCRNN.forward\u001b[1;34m(self, x, init_state, node_embeddings)\u001b[0m\n\u001b[0;32m     26\u001b[0m inner_states \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_length):\n\u001b[1;32m---> 28\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdcrnn_cells[i](current_inputs[:, t, :, :], state, node_embeddings)\n\u001b[0;32m     29\u001b[0m     inner_states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     30\u001b[0m output_hidden\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "# with initializing embedding\n",
    "#init loss function, optimizer\n",
    "# train on PEMSD4\n",
    "# train on traffic flow\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "                             weight_decay=0, amsgrad=False)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "#config log path\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "args.log_dir = log_dir\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler=lr_scheduler)\n",
    "\n",
    "if args.mode.lower() == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    # Load the model on CPU\n",
    "    model.load_state_dict(torch.load('../pre-trained/.pth'.format(args.dataset), map_location=torch.device('cpu')))\n",
    "    model.to(torch.device('cpu'))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode specified: {}\".format(args.mode))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
