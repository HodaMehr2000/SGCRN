{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5745b0-cf0c-4690-bbba-fd751443772e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# #file_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "# print(file_dir)\n",
    "# sys.path.append(file_dir)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from model.AGCRN import AGCRN as Network\n",
    "from model.BasicTrainer import Trainer\n",
    "from lib.TrainInits import init_seed\n",
    "from lib.dataloader import get_dataloader\n",
    "from lib.TrainInits import print_model_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1540607e-e83a-4aa5-87ac-c5d658f4f8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bb2f18-df03-49d6-950e-254e8c8536f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.load('data/PEMS04/pems04.npz')['data'][:, :, 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e619428-1951-48fa-a882-bd2be22963dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed embeddings from est_adj_agcrn/E_init.npy with shape torch.Size([307, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Define the path to the adjacency matrix\n",
    "adj_file_name = \"est_adj_agcrn/E_init.npy\"\n",
    "\n",
    "# Check if the file exists and load it\n",
    "if os.path.exists(adj_file_name):\n",
    "    # Load the matrix from the file\n",
    "    precomputed_embeddings = torch.tensor(np.load(adj_file_name), dtype=torch.float32)\n",
    "    print(f\"Loaded precomputed embeddings from {adj_file_name} with shape {precomputed_embeddings.shape}\")\n",
    "else:\n",
    "    precomputed_embeddings = None\n",
    "    print(f\"Embedding file not found at {adj_file_name}; using random initialization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cb05ee-271b-4686-b5b8-b9d15c8e5a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Mode = 'Train'\n",
    "DEBUG = 'True'\n",
    "DATASET = 'PEMSD4'      #PEMSD4 or PEMSD8\n",
    "DEVICE = 'cuda:0'\n",
    "MODEL = 'AGCRN'\n",
    "\n",
    "#get configuration\n",
    "config_file = 'model/PEMSD4_AGCRN.conf'  # Assuming the file is in the same directory as your script\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "from lib.metrics import MAE_torch\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35edd13-5a85-4f54-95bc-721f6f01ae28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parser\n",
    "args = argparse.ArgumentParser(description='arguments')\n",
    "args.add_argument('--dataset', default=DATASET, type=str)\n",
    "args.add_argument('--mode', default=Mode, type=str)\n",
    "args.add_argument('--device', default=DEVICE, type=str, help='indices of GPUs')\n",
    "args.add_argument('--debug', default=DEBUG, type=eval)\n",
    "args.add_argument('--model', default=MODEL, type=str)\n",
    "args.add_argument('--cuda', default=True, type=bool)\n",
    "#data\n",
    "args.add_argument('--val_ratio', default=config['data']['val_ratio'], type=float)\n",
    "args.add_argument('--test_ratio', default=config['data']['test_ratio'], type=float)\n",
    "args.add_argument('--lag', default=config['data']['lag'], type=int)\n",
    "args.add_argument('--horizon', default=config['data']['horizon'], type=int)\n",
    "args.add_argument('--num_nodes', default=config['data']['num_nodes'], type=int)\n",
    "args.add_argument('--tod', default=config['data']['tod'], type=eval)\n",
    "args.add_argument('--normalizer', default=config['data']['normalizer'], type=str)\n",
    "args.add_argument('--column_wise', default=config['data']['column_wise'], type=eval)\n",
    "args.add_argument('--default_graph', default=config['data']['default_graph'], type=eval)\n",
    "#model\n",
    "args.add_argument('--input_dim', default=config['model']['input_dim'], type=int)\n",
    "args.add_argument('--output_dim', default=config['model']['output_dim'], type=int)\n",
    "args.add_argument('--embed_dim', default=config['model']['embed_dim'], type=int)\n",
    "args.add_argument('--rnn_units', default=config['model']['rnn_units'], type=int)\n",
    "args.add_argument('--num_layers', default=config['model']['num_layers'], type=int)\n",
    "args.add_argument('--cheb_k', default=config['model']['cheb_order'], type=int)\n",
    "#train\n",
    "args.add_argument('--loss_func', default=config['train']['loss_func'], type=str)\n",
    "args.add_argument('--seed', default=config['train']['seed'], type=int)\n",
    "args.add_argument('--batch_size', default=config['train']['batch_size'], type=int)\n",
    "args.add_argument('--epochs', default=config['train']['epochs'], type=int)\n",
    "args.add_argument('--lr_init', default=config['train']['lr_init'], type=float)\n",
    "args.add_argument('--lr_decay', default=config['train']['lr_decay'], type=eval)\n",
    "args.add_argument('--lr_decay_rate', default=config['train']['lr_decay_rate'], type=float)\n",
    "args.add_argument('--lr_decay_step', default=config['train']['lr_decay_step'], type=str)\n",
    "args.add_argument('--early_stop', default=config['train']['early_stop'], type=eval)\n",
    "args.add_argument('--early_stop_patience', default=config['train']['early_stop_patience'], type=int)\n",
    "args.add_argument('--grad_norm', default=config['train']['grad_norm'], type=eval)\n",
    "args.add_argument('--max_grad_norm', default=config['train']['max_grad_norm'], type=int)\n",
    "args.add_argument('--teacher_forcing', default=False, type=bool)\n",
    "#args.add_argument('--tf_decay_steps', default=2000, type=int, help='teacher forcing decay steps')\n",
    "args.add_argument('--real_value', default=config['train']['real_value'], type=eval, help = 'use real value for loss calculation')\n",
    "#test\n",
    "args.add_argument('--mae_thresh', default=config['test']['mae_thresh'], type=eval)\n",
    "args.add_argument('--mape_thresh', default=config['test']['mape_thresh'], type=float)\n",
    "#log\n",
    "args.add_argument('--log_dir', default='./', type=str)\n",
    "args.add_argument('--log_step', default=config['log']['log_step'], type=int)\n",
    "args.add_argument('--plot', default=config['log']['plot'], type=eval)\n",
    "args = args.parse_args()\n",
    "init_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(int(args.device[5]))\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f76c9d-f421-464e-b4a5-076044dab27a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "node_embeddings torch.Size([307, 10]) True\n",
      "encoder.dcrnn_cells.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True\n",
      "encoder.dcrnn_cells.0.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder.dcrnn_cells.0.update.weights_pool torch.Size([10, 2, 65, 64]) True\n",
      "encoder.dcrnn_cells.0.update.bias_pool torch.Size([10, 64]) True\n",
      "encoder.dcrnn_cells.1.gate.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "encoder.dcrnn_cells.1.gate.bias_pool torch.Size([10, 128]) True\n",
      "encoder.dcrnn_cells.1.update.weights_pool torch.Size([10, 2, 128, 64]) True\n",
      "encoder.dcrnn_cells.1.update.bias_pool torch.Size([10, 64]) True\n",
      "end_conv.weight torch.Size([12, 1, 1, 64]) True\n",
      "end_conv.bias torch.Size([12]) True\n",
      "Total params num: 748810\n",
      "*****************Finish Parameter****************\n",
      "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
      "Normalize the dataset by Standard Normalization\n",
      "Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)\n",
      "Val:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
      "Test:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/lib/dataloader.py:77: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  X, Y = TensorFloat(X), TensorFloat(Y)\n"
     ]
    }
   ],
   "source": [
    "#init model\n",
    "# Initialize the model with precomputed embeddings\n",
    "model = Network(args, precomputed_embeddings=precomputed_embeddings)\n",
    "model = model.to(args.device)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "print_model_parameters(model, only_num=False)\n",
    "\n",
    "#load dataset\n",
    "train_loader, val_loader, test_loader, scaler = get_dataloader(args,\n",
    "                                                               normalizer=args.normalizer,\n",
    "                                                               tod=args.tod, dow=False,\n",
    "                                                               weather=False, single=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061eaba8-dc33-4468-9d09-bfc8d6587f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust embeddings to match expected node count\n",
    "expected_nodes = args.num_nodes  # 307\n",
    "current_nodes, embed_dim = precomputed_embeddings.shape  # 307, embed_dim\n",
    "if current_nodes < expected_nodes:\n",
    "    padding = torch.zeros(expected_nodes - current_nodes, embed_dim)  # Zero embeddings for missing nodes\n",
    "    precomputed_embeddings = torch.cat((precomputed_embeddings, padding), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6299e4f-629d-4986-b5e2-128af0658bad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "__file__ = 'data/PEMS04/pems04.npz'\n",
    "file_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "print(file_dir)\n",
    "sys.path.append(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d0635d-4943-46c2-9080-bc66bab5791f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DEVICE = 'cpu'  # Update the device to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62738d5b-ca5f-45a7-954d-815dae536dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     args.device = 'cuda:0'\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Switching to CPU.\")\n",
    "#     args.device = 'cpu'\n",
    "\n",
    "# model = model.to(args.device)\n",
    "\n",
    "# # Update the trainer call to ensure data and model are on the correct device\n",
    "# trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler, args, lr_scheduler=lr_scheduler)\n",
    "# if args.mode.lower() == 'train':\n",
    "#     trainer.train()\n",
    "# elif args.mode.lower() == 'test':\n",
    "#     model.load_state_dict(torch.load('../pre-trained/{}.pth'.format(args.dataset), map_location=args.device))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError(f\"Invalid mode: {args.mode}. Expected 'train' or 'test'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20bff20-8456-4acc-a71e-685eafe08d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args.device = 'cuda:0'\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    args.device = 'cpu'\n",
    "\n",
    "# Ensure the model and data are moved to the correct device\n",
    "model = model.to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d7e630a-2580-4e10-83af-368b8c1e9164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b6746b-1c78-4aea-83e9-98c21e711f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:59: Train Epoch 1: 0/158 Loss: 207.048584\n",
      "2024-12-06 17:59: Train Epoch 1: 20/158 Loss: 180.675720\n",
      "2024-12-06 18:00: Train Epoch 1: 40/158 Loss: 195.901291\n",
      "2024-12-06 18:00: Train Epoch 1: 60/158 Loss: 185.352875\n",
      "2024-12-06 18:00: Train Epoch 1: 80/158 Loss: 183.701355\n",
      "2024-12-06 18:00: Train Epoch 1: 100/158 Loss: 205.981171\n",
      "2024-12-06 18:00: Train Epoch 1: 120/158 Loss: 194.591919\n",
      "2024-12-06 18:00: Train Epoch 1: 140/158 Loss: 175.404388\n",
      "2024-12-06 18:00: **********Train Epoch 1: averaged Loss: 191.398754, tf_ratio: 1.000000\n",
      "2024-12-06 18:00: **********Val Epoch 1: average Loss: 192.045513\n",
      "2024-12-06 18:00: *********************************Current best model saved!\n",
      "2024-12-06 18:00: Train Epoch 2: 0/158 Loss: 183.595688\n",
      "2024-12-06 18:00: Train Epoch 2: 20/158 Loss: 187.893127\n",
      "2024-12-06 18:00: Train Epoch 2: 40/158 Loss: 165.947006\n",
      "2024-12-06 18:00: Train Epoch 2: 60/158 Loss: 171.949448\n",
      "2024-12-06 18:00: Train Epoch 2: 80/158 Loss: 151.457260\n",
      "2024-12-06 18:00: Train Epoch 2: 100/158 Loss: 152.754517\n",
      "2024-12-06 18:00: Train Epoch 2: 120/158 Loss: 167.598434\n",
      "2024-12-06 18:00: Train Epoch 2: 140/158 Loss: 187.306839\n",
      "2024-12-06 18:00: **********Train Epoch 2: averaged Loss: 167.791945, tf_ratio: 1.000000\n",
      "2024-12-06 18:00: **********Val Epoch 2: average Loss: 172.965203\n",
      "2024-12-06 18:00: *********************************Current best model saved!\n",
      "2024-12-06 18:00: Train Epoch 3: 0/158 Loss: 173.975037\n",
      "2024-12-06 18:00: Train Epoch 3: 20/158 Loss: 163.325912\n",
      "2024-12-06 18:00: Train Epoch 3: 40/158 Loss: 134.163727\n",
      "2024-12-06 18:01: Train Epoch 3: 60/158 Loss: 164.764160\n",
      "2024-12-06 18:01: Train Epoch 3: 80/158 Loss: 161.920044\n",
      "2024-12-06 18:01: Train Epoch 3: 100/158 Loss: 135.275452\n",
      "2024-12-06 18:01: Train Epoch 3: 120/158 Loss: 140.786423\n",
      "2024-12-06 18:01: Train Epoch 3: 140/158 Loss: 144.837234\n",
      "2024-12-06 18:01: **********Train Epoch 3: averaged Loss: 152.821636, tf_ratio: 1.000000\n",
      "2024-12-06 18:01: **********Val Epoch 3: average Loss: 159.573178\n",
      "2024-12-06 18:01: *********************************Current best model saved!\n",
      "2024-12-06 18:01: Train Epoch 4: 0/158 Loss: 137.089142\n",
      "2024-12-06 18:01: Train Epoch 4: 20/158 Loss: 130.285141\n",
      "2024-12-06 18:01: Train Epoch 4: 40/158 Loss: 134.645691\n",
      "2024-12-06 18:01: Train Epoch 4: 60/158 Loss: 128.882874\n",
      "2024-12-06 18:01: Train Epoch 4: 80/158 Loss: 145.380859\n",
      "2024-12-06 18:01: Train Epoch 4: 100/158 Loss: 142.512299\n",
      "2024-12-06 18:01: Train Epoch 4: 120/158 Loss: 140.961090\n",
      "2024-12-06 18:01: Train Epoch 4: 140/158 Loss: 133.653641\n",
      "2024-12-06 18:01: **********Train Epoch 4: averaged Loss: 142.455419, tf_ratio: 1.000000\n",
      "2024-12-06 18:01: **********Val Epoch 4: average Loss: 150.203009\n",
      "2024-12-06 18:01: *********************************Current best model saved!\n",
      "2024-12-06 18:01: Train Epoch 5: 0/158 Loss: 149.470932\n",
      "2024-12-06 18:01: Train Epoch 5: 20/158 Loss: 131.394318\n",
      "2024-12-06 18:01: Train Epoch 5: 40/158 Loss: 147.647125\n",
      "2024-12-06 18:01: Train Epoch 5: 60/158 Loss: 131.623138\n",
      "2024-12-06 18:01: Train Epoch 5: 80/158 Loss: 129.228455\n",
      "2024-12-06 18:02: Train Epoch 5: 100/158 Loss: 135.549591\n",
      "2024-12-06 18:02: Train Epoch 5: 120/158 Loss: 146.964355\n",
      "2024-12-06 18:02: Train Epoch 5: 140/158 Loss: 106.331078\n",
      "2024-12-06 18:02: **********Train Epoch 5: averaged Loss: 135.859747, tf_ratio: 1.000000\n",
      "2024-12-06 18:02: **********Val Epoch 5: average Loss: 144.024560\n",
      "2024-12-06 18:02: *********************************Current best model saved!\n",
      "2024-12-06 18:02: Train Epoch 6: 0/158 Loss: 126.323692\n",
      "2024-12-06 18:02: Train Epoch 6: 20/158 Loss: 133.434052\n",
      "2024-12-06 18:02: Train Epoch 6: 40/158 Loss: 122.562485\n",
      "2024-12-06 18:02: Train Epoch 6: 60/158 Loss: 120.851913\n",
      "2024-12-06 18:02: Train Epoch 6: 80/158 Loss: 130.833191\n",
      "2024-12-06 18:02: Train Epoch 6: 100/158 Loss: 120.087654\n",
      "2024-12-06 18:02: Train Epoch 6: 120/158 Loss: 134.605728\n",
      "2024-12-06 18:02: Train Epoch 6: 140/158 Loss: 133.660019\n",
      "2024-12-06 18:02: **********Train Epoch 6: averaged Loss: 131.733317, tf_ratio: 1.000000\n",
      "2024-12-06 18:02: **********Val Epoch 6: average Loss: 139.893156\n",
      "2024-12-06 18:02: *********************************Current best model saved!\n",
      "2024-12-06 18:02: Train Epoch 7: 0/158 Loss: 135.830994\n",
      "2024-12-06 18:02: Train Epoch 7: 20/158 Loss: 131.734772\n",
      "2024-12-06 18:02: Train Epoch 7: 40/158 Loss: 138.355209\n",
      "2024-12-06 18:02: Train Epoch 7: 60/158 Loss: 130.583054\n",
      "2024-12-06 18:02: Train Epoch 7: 80/158 Loss: 121.433220\n",
      "2024-12-06 18:02: Train Epoch 7: 100/158 Loss: 118.939728\n",
      "2024-12-06 18:02: Train Epoch 7: 120/158 Loss: 98.797760\n",
      "2024-12-06 18:03: Train Epoch 7: 140/158 Loss: 107.735565\n",
      "2024-12-06 18:03: **********Train Epoch 7: averaged Loss: 121.019028, tf_ratio: 1.000000\n",
      "2024-12-06 18:03: **********Val Epoch 7: average Loss: 116.568319\n",
      "2024-12-06 18:03: *********************************Current best model saved!\n",
      "2024-12-06 18:03: Train Epoch 8: 0/158 Loss: 102.435410\n",
      "2024-12-06 18:03: Train Epoch 8: 20/158 Loss: 116.961929\n",
      "2024-12-06 18:03: Train Epoch 8: 40/158 Loss: 95.965530\n",
      "2024-12-06 18:03: Train Epoch 8: 60/158 Loss: 98.630257\n",
      "2024-12-06 18:03: Train Epoch 8: 80/158 Loss: 91.067345\n",
      "2024-12-06 18:03: Train Epoch 8: 100/158 Loss: 96.366539\n",
      "2024-12-06 18:03: Train Epoch 8: 120/158 Loss: 100.896614\n",
      "2024-12-06 18:03: Train Epoch 8: 140/158 Loss: 89.381866\n",
      "2024-12-06 18:03: **********Train Epoch 8: averaged Loss: 97.960637, tf_ratio: 1.000000\n",
      "2024-12-06 18:03: **********Val Epoch 8: average Loss: 97.505493\n",
      "2024-12-06 18:03: *********************************Current best model saved!\n",
      "2024-12-06 18:03: Train Epoch 9: 0/158 Loss: 95.318748\n",
      "2024-12-06 18:03: Train Epoch 9: 20/158 Loss: 82.721352\n",
      "2024-12-06 18:03: Train Epoch 9: 40/158 Loss: 82.010712\n",
      "2024-12-06 18:03: Train Epoch 9: 60/158 Loss: 79.855133\n",
      "2024-12-06 18:03: Train Epoch 9: 80/158 Loss: 75.628296\n",
      "2024-12-06 18:03: Train Epoch 9: 100/158 Loss: 74.164131\n",
      "2024-12-06 18:03: Train Epoch 9: 120/158 Loss: 69.806870\n",
      "2024-12-06 18:03: Train Epoch 9: 140/158 Loss: 68.958115\n",
      "2024-12-06 18:04: **********Train Epoch 9: averaged Loss: 80.163233, tf_ratio: 1.000000\n",
      "2024-12-06 18:04: **********Val Epoch 9: average Loss: 82.566845\n",
      "2024-12-06 18:04: *********************************Current best model saved!\n",
      "2024-12-06 18:04: Train Epoch 10: 0/158 Loss: 73.961006\n",
      "2024-12-06 18:04: Train Epoch 10: 20/158 Loss: 69.080612\n",
      "2024-12-06 18:04: Train Epoch 10: 40/158 Loss: 64.341881\n",
      "2024-12-06 18:04: Train Epoch 10: 60/158 Loss: 75.135353\n",
      "2024-12-06 18:04: Train Epoch 10: 80/158 Loss: 75.002930\n",
      "2024-12-06 18:04: Train Epoch 10: 100/158 Loss: 69.268051\n",
      "2024-12-06 18:04: Train Epoch 10: 120/158 Loss: 67.942810\n",
      "2024-12-06 18:04: Train Epoch 10: 140/158 Loss: 60.096313\n",
      "2024-12-06 18:04: **********Train Epoch 10: averaged Loss: 68.853176, tf_ratio: 1.000000\n",
      "2024-12-06 18:04: **********Val Epoch 10: average Loss: 72.636409\n",
      "2024-12-06 18:04: *********************************Current best model saved!\n",
      "2024-12-06 18:04: Train Epoch 11: 0/158 Loss: 71.210815\n",
      "2024-12-06 18:04: Train Epoch 11: 20/158 Loss: 62.518566\n",
      "2024-12-06 18:04: Train Epoch 11: 40/158 Loss: 83.762871\n",
      "2024-12-06 18:04: Train Epoch 11: 60/158 Loss: 62.725529\n",
      "2024-12-06 18:04: Train Epoch 11: 80/158 Loss: 58.833469\n",
      "2024-12-06 18:04: Train Epoch 11: 100/158 Loss: 70.844727\n",
      "2024-12-06 18:04: Train Epoch 11: 120/158 Loss: 58.207584\n",
      "2024-12-06 18:04: Train Epoch 11: 140/158 Loss: 51.838657\n",
      "2024-12-06 18:04: **********Train Epoch 11: averaged Loss: 60.960935, tf_ratio: 1.000000\n",
      "2024-12-06 18:04: **********Val Epoch 11: average Loss: 64.552551\n",
      "2024-12-06 18:04: *********************************Current best model saved!\n",
      "2024-12-06 18:04: Train Epoch 12: 0/158 Loss: 63.769115\n",
      "2024-12-06 18:05: Train Epoch 12: 20/158 Loss: 56.276218\n",
      "2024-12-06 18:05: Train Epoch 12: 40/158 Loss: 53.137779\n",
      "2024-12-06 18:05: Train Epoch 12: 60/158 Loss: 49.477737\n",
      "2024-12-06 18:05: Train Epoch 12: 80/158 Loss: 43.950920\n",
      "2024-12-06 18:05: Train Epoch 12: 100/158 Loss: 55.597672\n",
      "2024-12-06 18:05: Train Epoch 12: 120/158 Loss: 45.166599\n",
      "2024-12-06 18:05: Train Epoch 12: 140/158 Loss: 52.034237\n",
      "2024-12-06 18:05: **********Train Epoch 12: averaged Loss: 54.358453, tf_ratio: 1.000000\n",
      "2024-12-06 18:05: **********Val Epoch 12: average Loss: 58.057923\n",
      "2024-12-06 18:05: *********************************Current best model saved!\n",
      "2024-12-06 18:05: Train Epoch 13: 0/158 Loss: 47.909214\n",
      "2024-12-06 18:05: Train Epoch 13: 20/158 Loss: 50.604126\n",
      "2024-12-06 18:05: Train Epoch 13: 40/158 Loss: 50.181030\n",
      "2024-12-06 18:05: Train Epoch 13: 60/158 Loss: 46.734978\n",
      "2024-12-06 18:05: Train Epoch 13: 80/158 Loss: 46.714260\n",
      "2024-12-06 18:05: Train Epoch 13: 100/158 Loss: 50.205666\n",
      "2024-12-06 18:05: Train Epoch 13: 120/158 Loss: 51.027538\n",
      "2024-12-06 18:05: Train Epoch 13: 140/158 Loss: 47.845695\n",
      "2024-12-06 18:05: **********Train Epoch 13: averaged Loss: 49.102599, tf_ratio: 1.000000\n",
      "2024-12-06 18:05: **********Val Epoch 13: average Loss: 52.338098\n",
      "2024-12-06 18:05: *********************************Current best model saved!\n",
      "2024-12-06 18:05: Train Epoch 14: 0/158 Loss: 51.550686\n",
      "2024-12-06 18:05: Train Epoch 14: 20/158 Loss: 47.790684\n",
      "2024-12-06 18:05: Train Epoch 14: 40/158 Loss: 42.299484\n",
      "2024-12-06 18:06: Train Epoch 14: 60/158 Loss: 49.300144\n",
      "2024-12-06 18:06: Train Epoch 14: 80/158 Loss: 46.774647\n",
      "2024-12-06 18:06: Train Epoch 14: 100/158 Loss: 42.233486\n",
      "2024-12-06 18:06: Train Epoch 14: 120/158 Loss: 37.512337\n",
      "2024-12-06 18:06: Train Epoch 14: 140/158 Loss: 46.945492\n",
      "2024-12-06 18:06: **********Train Epoch 14: averaged Loss: 44.664731, tf_ratio: 1.000000\n",
      "2024-12-06 18:06: **********Val Epoch 14: average Loss: 47.901223\n",
      "2024-12-06 18:06: *********************************Current best model saved!\n",
      "2024-12-06 18:06: Train Epoch 15: 0/158 Loss: 45.071548\n",
      "2024-12-06 18:06: Train Epoch 15: 20/158 Loss: 43.671097\n",
      "2024-12-06 18:06: Train Epoch 15: 40/158 Loss: 45.494843\n",
      "2024-12-06 18:06: Train Epoch 15: 60/158 Loss: 39.980412\n",
      "2024-12-06 18:06: Train Epoch 15: 80/158 Loss: 40.353989\n",
      "2024-12-06 18:06: Train Epoch 15: 100/158 Loss: 34.918652\n",
      "2024-12-06 18:06: Train Epoch 15: 120/158 Loss: 36.031944\n",
      "2024-12-06 18:06: Train Epoch 15: 140/158 Loss: 35.562462\n",
      "2024-12-06 18:06: **********Train Epoch 15: averaged Loss: 41.057888, tf_ratio: 1.000000\n",
      "2024-12-06 18:06: **********Val Epoch 15: average Loss: 43.928484\n",
      "2024-12-06 18:06: *********************************Current best model saved!\n",
      "2024-12-06 18:06: Train Epoch 16: 0/158 Loss: 36.668983\n",
      "2024-12-06 18:06: Train Epoch 16: 20/158 Loss: 42.357502\n",
      "2024-12-06 18:06: Train Epoch 16: 40/158 Loss: 35.614170\n",
      "2024-12-06 18:06: Train Epoch 16: 60/158 Loss: 37.990925\n",
      "2024-12-06 18:07: Train Epoch 16: 80/158 Loss: 38.243122\n",
      "2024-12-06 18:07: Train Epoch 16: 100/158 Loss: 40.714867\n",
      "2024-12-06 18:07: Train Epoch 16: 120/158 Loss: 33.276531\n",
      "2024-12-06 18:07: Train Epoch 16: 140/158 Loss: 35.682911\n",
      "2024-12-06 18:07: **********Train Epoch 16: averaged Loss: 37.975340, tf_ratio: 1.000000\n",
      "2024-12-06 18:07: **********Val Epoch 16: average Loss: 40.671489\n",
      "2024-12-06 18:07: *********************************Current best model saved!\n",
      "2024-12-06 18:07: Train Epoch 17: 0/158 Loss: 38.921665\n",
      "2024-12-06 18:07: Train Epoch 17: 20/158 Loss: 35.448616\n",
      "2024-12-06 18:07: Train Epoch 17: 40/158 Loss: 38.836819\n",
      "2024-12-06 18:07: Train Epoch 17: 60/158 Loss: 30.904152\n",
      "2024-12-06 18:07: Train Epoch 17: 80/158 Loss: 33.067005\n",
      "2024-12-06 18:07: Train Epoch 17: 100/158 Loss: 29.925829\n",
      "2024-12-06 18:07: Train Epoch 17: 120/158 Loss: 33.756618\n",
      "2024-12-06 18:07: Train Epoch 17: 140/158 Loss: 37.280300\n",
      "2024-12-06 18:07: **********Train Epoch 17: averaged Loss: 35.384168, tf_ratio: 1.000000\n",
      "2024-12-06 18:07: **********Val Epoch 17: average Loss: 38.048667\n",
      "2024-12-06 18:07: *********************************Current best model saved!\n",
      "2024-12-06 18:07: Train Epoch 18: 0/158 Loss: 34.270302\n",
      "2024-12-06 18:07: Train Epoch 18: 20/158 Loss: 33.511173\n",
      "2024-12-06 18:07: Train Epoch 18: 40/158 Loss: 30.010431\n",
      "2024-12-06 18:07: Train Epoch 18: 60/158 Loss: 34.322689\n",
      "2024-12-06 18:07: Train Epoch 18: 80/158 Loss: 35.746021\n",
      "2024-12-06 18:07: Train Epoch 18: 100/158 Loss: 29.725214\n",
      "2024-12-06 18:07: Train Epoch 18: 120/158 Loss: 28.569853\n",
      "2024-12-06 18:08: Train Epoch 18: 140/158 Loss: 33.190338\n",
      "2024-12-06 18:08: **********Train Epoch 18: averaged Loss: 33.154987, tf_ratio: 1.000000\n",
      "2024-12-06 18:08: **********Val Epoch 18: average Loss: 35.637462\n",
      "2024-12-06 18:08: *********************************Current best model saved!\n",
      "2024-12-06 18:08: Train Epoch 19: 0/158 Loss: 32.744640\n",
      "2024-12-06 18:08: Train Epoch 19: 20/158 Loss: 31.068277\n",
      "2024-12-06 18:08: Train Epoch 19: 40/158 Loss: 33.081753\n",
      "2024-12-06 18:08: Train Epoch 19: 60/158 Loss: 30.052605\n",
      "2024-12-06 18:08: Train Epoch 19: 80/158 Loss: 32.089931\n",
      "2024-12-06 18:08: Train Epoch 19: 100/158 Loss: 30.506207\n",
      "2024-12-06 18:08: Train Epoch 19: 120/158 Loss: 29.532019\n",
      "2024-12-06 18:08: Train Epoch 19: 140/158 Loss: 29.997593\n",
      "2024-12-06 18:08: **********Train Epoch 19: averaged Loss: 31.227753, tf_ratio: 1.000000\n",
      "2024-12-06 18:08: **********Val Epoch 19: average Loss: 33.543677\n",
      "2024-12-06 18:08: *********************************Current best model saved!\n",
      "2024-12-06 18:08: Train Epoch 20: 0/158 Loss: 28.695841\n",
      "2024-12-06 18:08: Train Epoch 20: 20/158 Loss: 27.869017\n",
      "2024-12-06 18:08: Train Epoch 20: 40/158 Loss: 27.461323\n",
      "2024-12-06 18:08: Train Epoch 20: 60/158 Loss: 28.013077\n",
      "2024-12-06 18:08: Train Epoch 20: 80/158 Loss: 25.455292\n",
      "2024-12-06 18:08: Train Epoch 20: 100/158 Loss: 28.223345\n",
      "2024-12-06 18:08: Train Epoch 20: 120/158 Loss: 29.928223\n",
      "2024-12-06 18:08: Train Epoch 20: 140/158 Loss: 30.793924\n",
      "2024-12-06 18:08: **********Train Epoch 20: averaged Loss: 29.580164, tf_ratio: 1.000000\n",
      "2024-12-06 18:08: **********Val Epoch 20: average Loss: 31.978972\n",
      "2024-12-06 18:08: *********************************Current best model saved!\n",
      "2024-12-06 18:08: Train Epoch 21: 0/158 Loss: 29.256367\n",
      "2024-12-06 18:08: Train Epoch 21: 20/158 Loss: 30.003475\n",
      "2024-12-06 18:08: Train Epoch 21: 40/158 Loss: 26.408609\n",
      "2024-12-06 18:09: Train Epoch 21: 60/158 Loss: 28.694347\n",
      "2024-12-06 18:09: Train Epoch 21: 80/158 Loss: 26.259272\n",
      "2024-12-06 18:09: Train Epoch 21: 100/158 Loss: 24.886988\n",
      "2024-12-06 18:09: Train Epoch 21: 120/158 Loss: 27.769262\n",
      "2024-12-06 18:09: Train Epoch 21: 140/158 Loss: 30.024429\n",
      "2024-12-06 18:09: **********Train Epoch 21: averaged Loss: 28.165941, tf_ratio: 1.000000\n",
      "2024-12-06 18:09: **********Val Epoch 21: average Loss: 30.400623\n",
      "2024-12-06 18:09: *********************************Current best model saved!\n",
      "2024-12-06 18:09: Train Epoch 22: 0/158 Loss: 25.088562\n",
      "2024-12-06 18:09: Train Epoch 22: 20/158 Loss: 28.357748\n",
      "2024-12-06 18:09: Train Epoch 22: 40/158 Loss: 28.969616\n",
      "2024-12-06 18:09: Train Epoch 22: 60/158 Loss: 26.931721\n",
      "2024-12-06 18:09: Train Epoch 22: 80/158 Loss: 25.288942\n",
      "2024-12-06 18:09: Train Epoch 22: 100/158 Loss: 25.711044\n",
      "2024-12-06 18:09: Train Epoch 22: 120/158 Loss: 27.455086\n",
      "2024-12-06 18:09: Train Epoch 22: 140/158 Loss: 32.232552\n",
      "2024-12-06 18:09: **********Train Epoch 22: averaged Loss: 26.980410, tf_ratio: 1.000000\n",
      "2024-12-06 18:09: **********Val Epoch 22: average Loss: 29.155985\n",
      "2024-12-06 18:09: *********************************Current best model saved!\n",
      "2024-12-06 18:09: Train Epoch 23: 0/158 Loss: 26.394184\n",
      "2024-12-06 18:09: Train Epoch 23: 20/158 Loss: 27.240042\n",
      "2024-12-06 18:09: Train Epoch 23: 40/158 Loss: 26.921207\n",
      "2024-12-06 18:09: Train Epoch 23: 60/158 Loss: 23.862497\n",
      "2024-12-06 18:09: Train Epoch 23: 80/158 Loss: 25.698654\n",
      "2024-12-06 18:09: Train Epoch 23: 100/158 Loss: 26.495762\n",
      "2024-12-06 18:10: Train Epoch 23: 120/158 Loss: 26.427950\n",
      "2024-12-06 18:10: Train Epoch 23: 140/158 Loss: 26.704165\n",
      "2024-12-06 18:10: **********Train Epoch 23: averaged Loss: 25.915626, tf_ratio: 1.000000\n",
      "2024-12-06 18:10: **********Val Epoch 23: average Loss: 27.954131\n",
      "2024-12-06 18:10: *********************************Current best model saved!\n",
      "2024-12-06 18:10: Train Epoch 24: 0/158 Loss: 27.961287\n",
      "2024-12-06 18:10: Train Epoch 24: 20/158 Loss: 26.254297\n",
      "2024-12-06 18:10: Train Epoch 24: 40/158 Loss: 28.043810\n",
      "2024-12-06 18:10: Train Epoch 24: 60/158 Loss: 24.276619\n",
      "2024-12-06 18:10: Train Epoch 24: 80/158 Loss: 25.237576\n",
      "2024-12-06 18:10: Train Epoch 24: 100/158 Loss: 22.319483\n",
      "2024-12-06 18:10: Train Epoch 24: 120/158 Loss: 24.003458\n",
      "2024-12-06 18:10: Train Epoch 24: 140/158 Loss: 24.525919\n",
      "2024-12-06 18:10: **********Train Epoch 24: averaged Loss: 25.039565, tf_ratio: 1.000000\n",
      "2024-12-06 18:10: **********Val Epoch 24: average Loss: 27.114504\n",
      "2024-12-06 18:10: *********************************Current best model saved!\n",
      "2024-12-06 18:10: Train Epoch 25: 0/158 Loss: 23.575159\n",
      "2024-12-06 18:10: Train Epoch 25: 20/158 Loss: 22.855291\n",
      "2024-12-06 18:10: Train Epoch 25: 40/158 Loss: 25.959475\n",
      "2024-12-06 18:10: Train Epoch 25: 60/158 Loss: 24.020483\n",
      "2024-12-06 18:10: Train Epoch 25: 80/158 Loss: 23.032452\n",
      "2024-12-06 18:10: Train Epoch 25: 100/158 Loss: 23.873669\n",
      "2024-12-06 18:10: Train Epoch 25: 120/158 Loss: 26.189672\n",
      "2024-12-06 18:10: Train Epoch 25: 140/158 Loss: 24.954802\n",
      "2024-12-06 18:10: **********Train Epoch 25: averaged Loss: 24.367019, tf_ratio: 1.000000\n",
      "2024-12-06 18:10: **********Val Epoch 25: average Loss: 26.282186\n",
      "2024-12-06 18:10: *********************************Current best model saved!\n",
      "2024-12-06 18:10: Train Epoch 26: 0/158 Loss: 25.846573\n",
      "2024-12-06 18:11: Train Epoch 26: 20/158 Loss: 25.766022\n",
      "2024-12-06 18:11: Train Epoch 26: 40/158 Loss: 26.048542\n",
      "2024-12-06 18:11: Train Epoch 26: 60/158 Loss: 22.260612\n",
      "2024-12-06 18:11: Train Epoch 26: 80/158 Loss: 22.515333\n",
      "2024-12-06 18:11: Train Epoch 26: 100/158 Loss: 24.143480\n",
      "2024-12-06 18:11: Train Epoch 26: 120/158 Loss: 21.427370\n",
      "2024-12-06 18:11: Train Epoch 26: 140/158 Loss: 24.048256\n",
      "2024-12-06 18:11: **********Train Epoch 26: averaged Loss: 23.665456, tf_ratio: 1.000000\n",
      "2024-12-06 18:11: **********Val Epoch 26: average Loss: 25.617833\n",
      "2024-12-06 18:11: *********************************Current best model saved!\n",
      "2024-12-06 18:11: Train Epoch 27: 0/158 Loss: 25.708403\n",
      "2024-12-06 18:11: Train Epoch 27: 20/158 Loss: 25.311251\n",
      "2024-12-06 18:11: Train Epoch 27: 40/158 Loss: 25.427568\n",
      "2024-12-06 18:11: Train Epoch 27: 60/158 Loss: 25.395851\n",
      "2024-12-06 18:11: Train Epoch 27: 80/158 Loss: 22.655226\n",
      "2024-12-06 18:11: Train Epoch 27: 100/158 Loss: 23.023361\n",
      "2024-12-06 18:11: Train Epoch 27: 120/158 Loss: 25.250511\n",
      "2024-12-06 18:11: Train Epoch 27: 140/158 Loss: 24.461500\n",
      "2024-12-06 18:11: **********Train Epoch 27: averaged Loss: 23.087380, tf_ratio: 1.000000\n",
      "2024-12-06 18:11: **********Val Epoch 27: average Loss: 24.879200\n",
      "2024-12-06 18:11: *********************************Current best model saved!\n",
      "2024-12-06 18:11: Train Epoch 28: 0/158 Loss: 22.128555\n",
      "2024-12-06 18:11: Train Epoch 28: 20/158 Loss: 22.239582\n",
      "2024-12-06 18:11: Train Epoch 28: 40/158 Loss: 23.975883\n",
      "2024-12-06 18:11: Train Epoch 28: 60/158 Loss: 20.818729\n",
      "2024-12-06 18:11: Train Epoch 28: 80/158 Loss: 22.347486\n",
      "2024-12-06 18:12: Train Epoch 28: 100/158 Loss: 20.455738\n",
      "2024-12-06 18:12: Train Epoch 28: 120/158 Loss: 24.326563\n",
      "2024-12-06 18:12: Train Epoch 28: 140/158 Loss: 20.822340\n",
      "2024-12-06 18:12: **********Train Epoch 28: averaged Loss: 22.479372, tf_ratio: 1.000000\n",
      "2024-12-06 18:12: **********Val Epoch 28: average Loss: 24.321945\n",
      "2024-12-06 18:12: *********************************Current best model saved!\n",
      "2024-12-06 18:12: Train Epoch 29: 0/158 Loss: 22.827999\n",
      "2024-12-06 18:12: Train Epoch 29: 20/158 Loss: 22.940317\n",
      "2024-12-06 18:12: Train Epoch 29: 40/158 Loss: 20.350101\n",
      "2024-12-06 18:12: Train Epoch 29: 60/158 Loss: 24.265402\n",
      "2024-12-06 18:12: Train Epoch 29: 80/158 Loss: 19.978600\n",
      "2024-12-06 18:12: Train Epoch 29: 100/158 Loss: 22.219545\n",
      "2024-12-06 18:12: Train Epoch 29: 120/158 Loss: 22.876339\n",
      "2024-12-06 18:12: Train Epoch 29: 140/158 Loss: 21.627113\n",
      "2024-12-06 18:12: **********Train Epoch 29: averaged Loss: 22.111784, tf_ratio: 1.000000\n",
      "2024-12-06 18:12: **********Val Epoch 29: average Loss: 24.158984\n",
      "2024-12-06 18:12: *********************************Current best model saved!\n",
      "2024-12-06 18:12: Train Epoch 30: 0/158 Loss: 21.747791\n",
      "2024-12-06 18:12: Train Epoch 30: 20/158 Loss: 23.228292\n",
      "2024-12-06 18:12: Train Epoch 30: 40/158 Loss: 19.605995\n",
      "2024-12-06 18:12: Train Epoch 30: 60/158 Loss: 20.964663\n",
      "2024-12-06 18:12: Train Epoch 30: 80/158 Loss: 21.131224\n",
      "2024-12-06 18:12: Train Epoch 30: 100/158 Loss: 22.629921\n",
      "2024-12-06 18:12: Train Epoch 30: 120/158 Loss: 22.176525\n",
      "2024-12-06 18:12: Train Epoch 30: 140/158 Loss: 21.561152\n",
      "2024-12-06 18:12: **********Train Epoch 30: averaged Loss: 21.694873, tf_ratio: 1.000000\n",
      "2024-12-06 18:13: **********Val Epoch 30: average Loss: 23.439056\n",
      "2024-12-06 18:13: *********************************Current best model saved!\n",
      "2024-12-06 18:13: Train Epoch 31: 0/158 Loss: 21.471186\n",
      "2024-12-06 18:13: Train Epoch 31: 20/158 Loss: 20.789415\n",
      "2024-12-06 18:13: Train Epoch 31: 40/158 Loss: 21.626427\n",
      "2024-12-06 18:13: Train Epoch 31: 60/158 Loss: 22.025940\n",
      "2024-12-06 18:13: Train Epoch 31: 80/158 Loss: 24.713562\n",
      "2024-12-06 18:13: Train Epoch 31: 100/158 Loss: 22.638840\n",
      "2024-12-06 18:13: Train Epoch 31: 120/158 Loss: 22.723228\n",
      "2024-12-06 18:13: Train Epoch 31: 140/158 Loss: 22.485340\n",
      "2024-12-06 18:13: **********Train Epoch 31: averaged Loss: 21.390488, tf_ratio: 1.000000\n",
      "2024-12-06 18:13: **********Val Epoch 31: average Loss: 23.162257\n",
      "2024-12-06 18:13: *********************************Current best model saved!\n",
      "2024-12-06 18:13: Train Epoch 32: 0/158 Loss: 22.424044\n",
      "2024-12-06 18:13: Train Epoch 32: 20/158 Loss: 20.465479\n",
      "2024-12-06 18:13: Train Epoch 32: 40/158 Loss: 18.977995\n",
      "2024-12-06 18:13: Train Epoch 32: 60/158 Loss: 20.988817\n",
      "2024-12-06 18:13: Train Epoch 32: 80/158 Loss: 23.050779\n",
      "2024-12-06 18:13: Train Epoch 32: 100/158 Loss: 23.617842\n",
      "2024-12-06 18:13: Train Epoch 32: 120/158 Loss: 20.276625\n",
      "2024-12-06 18:13: Train Epoch 32: 140/158 Loss: 22.253525\n",
      "2024-12-06 18:13: **********Train Epoch 32: averaged Loss: 21.042098, tf_ratio: 1.000000\n",
      "2024-12-06 18:13: **********Val Epoch 32: average Loss: 22.770871\n",
      "2024-12-06 18:13: *********************************Current best model saved!\n",
      "2024-12-06 18:13: Train Epoch 33: 0/158 Loss: 21.597857\n",
      "2024-12-06 18:13: Train Epoch 33: 20/158 Loss: 20.262045\n",
      "2024-12-06 18:13: Train Epoch 33: 40/158 Loss: 20.493078\n",
      "2024-12-06 18:14: Train Epoch 33: 60/158 Loss: 19.856125\n",
      "2024-12-06 18:14: Train Epoch 33: 80/158 Loss: 19.485445\n",
      "2024-12-06 18:14: Train Epoch 33: 100/158 Loss: 19.487268\n",
      "2024-12-06 18:14: Train Epoch 33: 120/158 Loss: 19.722996\n",
      "2024-12-06 18:14: Train Epoch 33: 140/158 Loss: 20.068544\n",
      "2024-12-06 18:14: **********Train Epoch 33: averaged Loss: 20.690704, tf_ratio: 1.000000\n",
      "2024-12-06 18:14: **********Val Epoch 33: average Loss: 22.519797\n",
      "2024-12-06 18:14: *********************************Current best model saved!\n",
      "2024-12-06 18:14: Train Epoch 34: 0/158 Loss: 21.266600\n",
      "2024-12-06 18:14: Train Epoch 34: 20/158 Loss: 21.457397\n",
      "2024-12-06 18:14: Train Epoch 34: 40/158 Loss: 20.384375\n",
      "2024-12-06 18:14: Train Epoch 34: 60/158 Loss: 21.661379\n",
      "2024-12-06 18:14: Train Epoch 34: 80/158 Loss: 21.113205\n",
      "2024-12-06 18:14: Train Epoch 34: 100/158 Loss: 21.641441\n",
      "2024-12-06 18:14: Train Epoch 34: 120/158 Loss: 21.194490\n",
      "2024-12-06 18:14: Train Epoch 34: 140/158 Loss: 19.797796\n",
      "2024-12-06 18:14: **********Train Epoch 34: averaged Loss: 20.377113, tf_ratio: 1.000000\n",
      "2024-12-06 18:14: **********Val Epoch 34: average Loss: 22.176723\n",
      "2024-12-06 18:14: *********************************Current best model saved!\n",
      "2024-12-06 18:14: Train Epoch 35: 0/158 Loss: 18.636553\n",
      "2024-12-06 18:14: Train Epoch 35: 20/158 Loss: 20.234533\n",
      "2024-12-06 18:14: Train Epoch 35: 40/158 Loss: 21.895344\n",
      "2024-12-06 18:14: Train Epoch 35: 60/158 Loss: 19.642830\n",
      "2024-12-06 18:14: Train Epoch 35: 80/158 Loss: 19.506058\n",
      "2024-12-06 18:14: Train Epoch 35: 100/158 Loss: 19.250259\n",
      "2024-12-06 18:14: Train Epoch 35: 120/158 Loss: 20.859755\n",
      "2024-12-06 18:15: Train Epoch 35: 140/158 Loss: 19.052864\n",
      "2024-12-06 18:15: **********Train Epoch 35: averaged Loss: 20.161027, tf_ratio: 1.000000\n",
      "2024-12-06 18:15: **********Val Epoch 35: average Loss: 22.001274\n",
      "2024-12-06 18:15: *********************************Current best model saved!\n",
      "2024-12-06 18:15: Train Epoch 36: 0/158 Loss: 23.040751\n",
      "2024-12-06 18:15: Train Epoch 36: 20/158 Loss: 19.905445\n",
      "2024-12-06 18:15: Train Epoch 36: 40/158 Loss: 20.341251\n",
      "2024-12-06 18:15: Train Epoch 36: 60/158 Loss: 20.477858\n",
      "2024-12-06 18:15: Train Epoch 36: 80/158 Loss: 18.867733\n",
      "2024-12-06 18:15: Train Epoch 36: 100/158 Loss: 23.558825\n",
      "2024-12-06 18:15: Train Epoch 36: 120/158 Loss: 19.680370\n",
      "2024-12-06 18:15: Train Epoch 36: 140/158 Loss: 18.970362\n",
      "2024-12-06 18:15: **********Train Epoch 36: averaged Loss: 20.012757, tf_ratio: 1.000000\n",
      "2024-12-06 18:15: **********Val Epoch 36: average Loss: 21.719141\n",
      "2024-12-06 18:15: *********************************Current best model saved!\n",
      "2024-12-06 18:15: Train Epoch 37: 0/158 Loss: 18.212795\n",
      "2024-12-06 18:15: Train Epoch 37: 20/158 Loss: 22.035616\n",
      "2024-12-06 18:15: Train Epoch 37: 40/158 Loss: 20.621737\n",
      "2024-12-06 18:15: Train Epoch 37: 60/158 Loss: 20.482374\n",
      "2024-12-06 18:15: Train Epoch 37: 80/158 Loss: 19.477060\n",
      "2024-12-06 18:15: Train Epoch 37: 100/158 Loss: 18.478924\n",
      "2024-12-06 18:15: Train Epoch 37: 120/158 Loss: 19.307394\n",
      "2024-12-06 18:15: Train Epoch 37: 140/158 Loss: 19.184341\n",
      "2024-12-06 18:15: **********Train Epoch 37: averaged Loss: 19.775854, tf_ratio: 1.000000\n",
      "2024-12-06 18:15: **********Val Epoch 37: average Loss: 21.540898\n",
      "2024-12-06 18:15: *********************************Current best model saved!\n",
      "2024-12-06 18:15: Train Epoch 38: 0/158 Loss: 20.942799\n",
      "2024-12-06 18:15: Train Epoch 38: 20/158 Loss: 20.091621\n",
      "2024-12-06 18:15: Train Epoch 38: 40/158 Loss: 17.972048\n",
      "2024-12-06 18:16: Train Epoch 38: 60/158 Loss: 19.586096\n",
      "2024-12-06 18:16: Train Epoch 38: 80/158 Loss: 19.273996\n",
      "2024-12-06 18:16: Train Epoch 38: 100/158 Loss: 20.013603\n",
      "2024-12-06 18:16: Train Epoch 38: 120/158 Loss: 19.676546\n",
      "2024-12-06 18:16: Train Epoch 38: 140/158 Loss: 20.538198\n",
      "2024-12-06 18:16: **********Train Epoch 38: averaged Loss: 19.594333, tf_ratio: 1.000000\n",
      "2024-12-06 18:16: **********Val Epoch 38: average Loss: 21.444365\n",
      "2024-12-06 18:16: *********************************Current best model saved!\n",
      "2024-12-06 18:16: Train Epoch 39: 0/158 Loss: 18.471272\n",
      "2024-12-06 18:16: Train Epoch 39: 20/158 Loss: 20.492252\n",
      "2024-12-06 18:16: Train Epoch 39: 40/158 Loss: 19.896357\n",
      "2024-12-06 18:16: Train Epoch 39: 60/158 Loss: 19.889160\n",
      "2024-12-06 18:16: Train Epoch 39: 80/158 Loss: 20.089651\n",
      "2024-12-06 18:16: Train Epoch 39: 100/158 Loss: 18.574028\n",
      "2024-12-06 18:16: Train Epoch 39: 120/158 Loss: 19.587456\n",
      "2024-12-06 18:16: Train Epoch 39: 140/158 Loss: 20.632004\n",
      "2024-12-06 18:16: **********Train Epoch 39: averaged Loss: 19.412531, tf_ratio: 1.000000\n",
      "2024-12-06 18:16: **********Val Epoch 39: average Loss: 21.195163\n",
      "2024-12-06 18:16: *********************************Current best model saved!\n",
      "2024-12-06 18:16: Train Epoch 40: 0/158 Loss: 19.386984\n",
      "2024-12-06 18:16: Train Epoch 40: 20/158 Loss: 19.609461\n",
      "2024-12-06 18:16: Train Epoch 40: 40/158 Loss: 20.532387\n",
      "2024-12-06 18:16: Train Epoch 40: 60/158 Loss: 17.842356\n",
      "2024-12-06 18:16: Train Epoch 40: 80/158 Loss: 19.465460\n",
      "2024-12-06 18:16: Train Epoch 40: 100/158 Loss: 18.138483\n",
      "2024-12-06 18:16: Train Epoch 40: 120/158 Loss: 21.602005\n",
      "2024-12-06 18:17: Train Epoch 40: 140/158 Loss: 18.937168\n",
      "2024-12-06 18:17: **********Train Epoch 40: averaged Loss: 19.281525, tf_ratio: 1.000000\n",
      "2024-12-06 18:17: **********Val Epoch 40: average Loss: 21.037039\n",
      "2024-12-06 18:17: *********************************Current best model saved!\n",
      "2024-12-06 18:17: Train Epoch 41: 0/158 Loss: 19.133204\n",
      "2024-12-06 18:17: Train Epoch 41: 20/158 Loss: 18.842453\n",
      "2024-12-06 18:17: Train Epoch 41: 40/158 Loss: 18.315203\n",
      "2024-12-06 18:17: Train Epoch 41: 60/158 Loss: 18.647211\n",
      "2024-12-06 18:17: Train Epoch 41: 80/158 Loss: 19.123386\n",
      "2024-12-06 18:17: Train Epoch 41: 100/158 Loss: 19.133551\n",
      "2024-12-06 18:17: Train Epoch 41: 120/158 Loss: 19.596291\n",
      "2024-12-06 18:17: Train Epoch 41: 140/158 Loss: 18.269140\n",
      "2024-12-06 18:17: **********Train Epoch 41: averaged Loss: 19.136091, tf_ratio: 1.000000\n",
      "2024-12-06 18:17: **********Val Epoch 41: average Loss: 20.949480\n",
      "2024-12-06 18:17: *********************************Current best model saved!\n",
      "2024-12-06 18:17: Train Epoch 42: 0/158 Loss: 20.550444\n",
      "2024-12-06 18:17: Train Epoch 42: 20/158 Loss: 19.034246\n",
      "2024-12-06 18:17: Train Epoch 42: 40/158 Loss: 18.956697\n",
      "2024-12-06 18:17: Train Epoch 42: 60/158 Loss: 17.806143\n",
      "2024-12-06 18:17: Train Epoch 42: 80/158 Loss: 19.343594\n",
      "2024-12-06 18:17: Train Epoch 42: 100/158 Loss: 16.993996\n",
      "2024-12-06 18:17: Train Epoch 42: 120/158 Loss: 18.292429\n",
      "2024-12-06 18:17: Train Epoch 42: 140/158 Loss: 20.445890\n",
      "2024-12-06 18:17: **********Train Epoch 42: averaged Loss: 18.960219, tf_ratio: 1.000000\n",
      "2024-12-06 18:17: **********Val Epoch 42: average Loss: 20.731828\n",
      "2024-12-06 18:17: *********************************Current best model saved!\n",
      "2024-12-06 18:17: Train Epoch 43: 0/158 Loss: 18.172489\n",
      "2024-12-06 18:17: Train Epoch 43: 20/158 Loss: 19.856104\n",
      "2024-12-06 18:18: Train Epoch 43: 40/158 Loss: 19.670458\n",
      "2024-12-06 18:18: Train Epoch 43: 60/158 Loss: 20.698298\n",
      "2024-12-06 18:18: Train Epoch 43: 80/158 Loss: 19.184189\n",
      "2024-12-06 18:18: Train Epoch 43: 100/158 Loss: 19.155699\n",
      "2024-12-06 18:18: Train Epoch 43: 120/158 Loss: 18.291317\n",
      "2024-12-06 18:18: Train Epoch 43: 140/158 Loss: 19.191275\n",
      "2024-12-06 18:18: **********Train Epoch 43: averaged Loss: 18.881664, tf_ratio: 1.000000\n",
      "2024-12-06 18:18: **********Val Epoch 43: average Loss: 20.647825\n",
      "2024-12-06 18:18: *********************************Current best model saved!\n",
      "2024-12-06 18:18: Train Epoch 44: 0/158 Loss: 19.705263\n",
      "2024-12-06 18:18: Train Epoch 44: 20/158 Loss: 18.645061\n",
      "2024-12-06 18:18: Train Epoch 44: 40/158 Loss: 18.273340\n",
      "2024-12-06 18:18: Train Epoch 44: 60/158 Loss: 20.947910\n",
      "2024-12-06 18:18: Train Epoch 44: 80/158 Loss: 17.172754\n",
      "2024-12-06 18:18: Train Epoch 44: 100/158 Loss: 17.252230\n",
      "2024-12-06 18:18: Train Epoch 44: 120/158 Loss: 19.909277\n",
      "2024-12-06 18:18: Train Epoch 44: 140/158 Loss: 19.051609\n",
      "2024-12-06 18:18: **********Train Epoch 44: averaged Loss: 18.761392, tf_ratio: 1.000000\n",
      "2024-12-06 18:18: **********Val Epoch 44: average Loss: 20.602847\n",
      "2024-12-06 18:18: *********************************Current best model saved!\n",
      "2024-12-06 18:18: Train Epoch 45: 0/158 Loss: 19.209953\n",
      "2024-12-06 18:18: Train Epoch 45: 20/158 Loss: 18.466255\n",
      "2024-12-06 18:18: Train Epoch 45: 40/158 Loss: 18.504963\n",
      "2024-12-06 18:18: Train Epoch 45: 60/158 Loss: 19.266123\n",
      "2024-12-06 18:18: Train Epoch 45: 80/158 Loss: 18.863506\n",
      "2024-12-06 18:18: Train Epoch 45: 100/158 Loss: 18.599579\n",
      "2024-12-06 18:19: Train Epoch 45: 120/158 Loss: 17.099585\n",
      "2024-12-06 18:19: Train Epoch 45: 140/158 Loss: 17.222128\n",
      "2024-12-06 18:19: **********Train Epoch 45: averaged Loss: 18.693703, tf_ratio: 1.000000\n",
      "2024-12-06 18:19: **********Val Epoch 45: average Loss: 20.455060\n",
      "2024-12-06 18:19: *********************************Current best model saved!\n",
      "2024-12-06 18:19: Train Epoch 46: 0/158 Loss: 18.453606\n",
      "2024-12-06 18:19: Train Epoch 46: 20/158 Loss: 18.994890\n",
      "2024-12-06 18:19: Train Epoch 46: 40/158 Loss: 17.426706\n",
      "2024-12-06 18:19: Train Epoch 46: 60/158 Loss: 18.862082\n",
      "2024-12-06 18:19: Train Epoch 46: 80/158 Loss: 20.575161\n",
      "2024-12-06 18:19: Train Epoch 46: 100/158 Loss: 17.743866\n",
      "2024-12-06 18:19: Train Epoch 46: 120/158 Loss: 17.111765\n",
      "2024-12-06 18:19: Train Epoch 46: 140/158 Loss: 18.934069\n",
      "2024-12-06 18:19: **********Train Epoch 46: averaged Loss: 18.555879, tf_ratio: 1.000000\n",
      "2024-12-06 18:19: **********Val Epoch 46: average Loss: 20.407005\n",
      "2024-12-06 18:19: *********************************Current best model saved!\n",
      "2024-12-06 18:19: Train Epoch 47: 0/158 Loss: 18.785534\n",
      "2024-12-06 18:19: Train Epoch 47: 20/158 Loss: 18.844505\n",
      "2024-12-06 18:19: Train Epoch 47: 40/158 Loss: 17.734560\n",
      "2024-12-06 18:19: Train Epoch 47: 60/158 Loss: 19.822887\n",
      "2024-12-06 18:19: Train Epoch 47: 80/158 Loss: 19.564642\n",
      "2024-12-06 18:19: Train Epoch 47: 100/158 Loss: 16.791737\n",
      "2024-12-06 18:19: Train Epoch 47: 120/158 Loss: 17.033077\n",
      "2024-12-06 18:19: Train Epoch 47: 140/158 Loss: 17.353821\n",
      "2024-12-06 18:19: **********Train Epoch 47: averaged Loss: 18.529844, tf_ratio: 1.000000\n",
      "2024-12-06 18:19: **********Val Epoch 47: average Loss: 20.427505\n",
      "2024-12-06 18:19: Train Epoch 48: 0/158 Loss: 18.559082\n",
      "2024-12-06 18:20: Train Epoch 48: 20/158 Loss: 18.675503\n",
      "2024-12-06 18:20: Train Epoch 48: 40/158 Loss: 18.619987\n",
      "2024-12-06 18:20: Train Epoch 48: 60/158 Loss: 19.668644\n",
      "2024-12-06 18:20: Train Epoch 48: 80/158 Loss: 19.725433\n",
      "2024-12-06 18:20: Train Epoch 48: 100/158 Loss: 18.953953\n",
      "2024-12-06 18:20: Train Epoch 48: 120/158 Loss: 18.981030\n",
      "2024-12-06 18:20: Train Epoch 48: 140/158 Loss: 16.455021\n",
      "2024-12-06 18:20: **********Train Epoch 48: averaged Loss: 18.406333, tf_ratio: 1.000000\n",
      "2024-12-06 18:20: **********Val Epoch 48: average Loss: 20.284492\n",
      "2024-12-06 18:20: *********************************Current best model saved!\n",
      "2024-12-06 18:20: Train Epoch 49: 0/158 Loss: 20.063412\n",
      "2024-12-06 18:20: Train Epoch 49: 20/158 Loss: 17.761940\n",
      "2024-12-06 18:20: Train Epoch 49: 40/158 Loss: 19.864304\n",
      "2024-12-06 18:20: Train Epoch 49: 60/158 Loss: 16.745783\n",
      "2024-12-06 18:20: Train Epoch 49: 80/158 Loss: 17.504185\n",
      "2024-12-06 18:20: Train Epoch 49: 100/158 Loss: 19.033075\n",
      "2024-12-06 18:20: Train Epoch 49: 120/158 Loss: 16.117378\n",
      "2024-12-06 18:20: Train Epoch 49: 140/158 Loss: 17.996820\n",
      "2024-12-06 18:20: **********Train Epoch 49: averaged Loss: 18.286595, tf_ratio: 1.000000\n",
      "2024-12-06 18:20: **********Val Epoch 49: average Loss: 20.260723\n",
      "2024-12-06 18:20: *********************************Current best model saved!\n",
      "2024-12-06 18:20: Train Epoch 50: 0/158 Loss: 18.135172\n",
      "2024-12-06 18:20: Train Epoch 50: 20/158 Loss: 19.265024\n",
      "2024-12-06 18:20: Train Epoch 50: 40/158 Loss: 19.459436\n",
      "2024-12-06 18:20: Train Epoch 50: 60/158 Loss: 19.198172\n",
      "2024-12-06 18:20: Train Epoch 50: 80/158 Loss: 18.743559\n",
      "2024-12-06 18:21: Train Epoch 50: 100/158 Loss: 16.819412\n",
      "2024-12-06 18:21: Train Epoch 50: 120/158 Loss: 18.102695\n",
      "2024-12-06 18:21: Train Epoch 50: 140/158 Loss: 17.316610\n",
      "2024-12-06 18:21: **********Train Epoch 50: averaged Loss: 18.226974, tf_ratio: 1.000000\n",
      "2024-12-06 18:21: **********Val Epoch 50: average Loss: 20.161805\n",
      "2024-12-06 18:21: *********************************Current best model saved!\n",
      "2024-12-06 18:21: Train Epoch 51: 0/158 Loss: 18.313820\n",
      "2024-12-06 18:21: Train Epoch 51: 20/158 Loss: 18.018877\n",
      "2024-12-06 18:21: Train Epoch 51: 40/158 Loss: 18.653862\n",
      "2024-12-06 18:21: Train Epoch 51: 60/158 Loss: 18.556049\n",
      "2024-12-06 18:21: Train Epoch 51: 80/158 Loss: 18.191698\n",
      "2024-12-06 18:21: Train Epoch 51: 100/158 Loss: 18.209616\n",
      "2024-12-06 18:21: Train Epoch 51: 120/158 Loss: 18.227217\n",
      "2024-12-06 18:21: Train Epoch 51: 140/158 Loss: 18.056736\n",
      "2024-12-06 18:21: **********Train Epoch 51: averaged Loss: 18.186484, tf_ratio: 1.000000\n",
      "2024-12-06 18:21: **********Val Epoch 51: average Loss: 20.235851\n",
      "2024-12-06 18:21: Train Epoch 52: 0/158 Loss: 16.793100\n",
      "2024-12-06 18:21: Train Epoch 52: 20/158 Loss: 17.458481\n",
      "2024-12-06 18:21: Train Epoch 52: 40/158 Loss: 17.754770\n",
      "2024-12-06 18:21: Train Epoch 52: 60/158 Loss: 18.280880\n",
      "2024-12-06 18:21: Train Epoch 52: 80/158 Loss: 16.327818\n",
      "2024-12-06 18:21: Train Epoch 52: 100/158 Loss: 18.741310\n",
      "2024-12-06 18:21: Train Epoch 52: 120/158 Loss: 18.696672\n",
      "2024-12-06 18:21: Train Epoch 52: 140/158 Loss: 19.424049\n",
      "2024-12-06 18:21: **********Train Epoch 52: averaged Loss: 18.110879, tf_ratio: 1.000000\n",
      "2024-12-06 18:22: **********Val Epoch 52: average Loss: 20.087365\n",
      "2024-12-06 18:22: *********************************Current best model saved!\n",
      "2024-12-06 18:22: Train Epoch 53: 0/158 Loss: 18.203335\n",
      "2024-12-06 18:22: Train Epoch 53: 20/158 Loss: 17.681639\n",
      "2024-12-06 18:22: Train Epoch 53: 40/158 Loss: 18.040739\n",
      "2024-12-06 18:22: Train Epoch 53: 60/158 Loss: 18.312613\n",
      "2024-12-06 18:22: Train Epoch 53: 80/158 Loss: 19.232212\n",
      "2024-12-06 18:22: Train Epoch 53: 100/158 Loss: 17.499088\n",
      "2024-12-06 18:22: Train Epoch 53: 120/158 Loss: 18.187752\n",
      "2024-12-06 18:22: Train Epoch 53: 140/158 Loss: 17.243059\n",
      "2024-12-06 18:22: **********Train Epoch 53: averaged Loss: 18.034555, tf_ratio: 1.000000\n",
      "2024-12-06 18:22: **********Val Epoch 53: average Loss: 20.015580\n",
      "2024-12-06 18:22: *********************************Current best model saved!\n",
      "2024-12-06 18:22: Train Epoch 54: 0/158 Loss: 17.806934\n",
      "2024-12-06 18:22: Train Epoch 54: 20/158 Loss: 17.959145\n",
      "2024-12-06 18:22: Train Epoch 54: 40/158 Loss: 18.331436\n",
      "2024-12-06 18:22: Train Epoch 54: 60/158 Loss: 17.950262\n",
      "2024-12-06 18:22: Train Epoch 54: 80/158 Loss: 17.645861\n",
      "2024-12-06 18:22: Train Epoch 54: 100/158 Loss: 17.954712\n",
      "2024-12-06 18:22: Train Epoch 54: 120/158 Loss: 16.505424\n",
      "2024-12-06 18:22: Train Epoch 54: 140/158 Loss: 17.917250\n",
      "2024-12-06 18:22: **********Train Epoch 54: averaged Loss: 18.007219, tf_ratio: 1.000000\n",
      "2024-12-06 18:22: **********Val Epoch 54: average Loss: 20.043483\n",
      "2024-12-06 18:22: Train Epoch 55: 0/158 Loss: 18.071323\n",
      "2024-12-06 18:22: Train Epoch 55: 20/158 Loss: 16.465870\n",
      "2024-12-06 18:22: Train Epoch 55: 40/158 Loss: 18.706377\n",
      "2024-12-06 18:22: Train Epoch 55: 60/158 Loss: 16.966030\n",
      "2024-12-06 18:23: Train Epoch 55: 80/158 Loss: 18.085768\n",
      "2024-12-06 18:23: Train Epoch 55: 100/158 Loss: 18.553280\n",
      "2024-12-06 18:23: Train Epoch 55: 120/158 Loss: 18.325148\n",
      "2024-12-06 18:23: Train Epoch 55: 140/158 Loss: 18.483665\n",
      "2024-12-06 18:23: **********Train Epoch 55: averaged Loss: 17.928936, tf_ratio: 1.000000\n",
      "2024-12-06 18:23: **********Val Epoch 55: average Loss: 19.952112\n",
      "2024-12-06 18:23: *********************************Current best model saved!\n",
      "2024-12-06 18:23: Train Epoch 56: 0/158 Loss: 18.262171\n",
      "2024-12-06 18:23: Train Epoch 56: 20/158 Loss: 18.101871\n",
      "2024-12-06 18:23: Train Epoch 56: 40/158 Loss: 17.409937\n",
      "2024-12-06 18:23: Train Epoch 56: 60/158 Loss: 17.997061\n",
      "2024-12-06 18:23: Train Epoch 56: 80/158 Loss: 18.224884\n",
      "2024-12-06 18:23: Train Epoch 56: 100/158 Loss: 15.904301\n",
      "2024-12-06 18:23: Train Epoch 56: 120/158 Loss: 19.764086\n",
      "2024-12-06 18:23: Train Epoch 56: 140/158 Loss: 16.982340\n",
      "2024-12-06 18:23: **********Train Epoch 56: averaged Loss: 17.866501, tf_ratio: 1.000000\n",
      "2024-12-06 18:23: **********Val Epoch 56: average Loss: 19.889207\n",
      "2024-12-06 18:23: *********************************Current best model saved!\n",
      "2024-12-06 18:23: Train Epoch 57: 0/158 Loss: 17.975557\n",
      "2024-12-06 18:23: Train Epoch 57: 20/158 Loss: 16.530977\n",
      "2024-12-06 18:23: Train Epoch 57: 40/158 Loss: 18.346935\n",
      "2024-12-06 18:23: Train Epoch 57: 60/158 Loss: 18.245461\n",
      "2024-12-06 18:23: Train Epoch 57: 80/158 Loss: 18.231405\n",
      "2024-12-06 18:23: Train Epoch 57: 100/158 Loss: 19.217707\n",
      "2024-12-06 18:23: Train Epoch 57: 120/158 Loss: 18.054121\n",
      "2024-12-06 18:23: Train Epoch 57: 140/158 Loss: 18.538286\n",
      "2024-12-06 18:24: **********Train Epoch 57: averaged Loss: 17.886090, tf_ratio: 1.000000\n",
      "2024-12-06 18:24: **********Val Epoch 57: average Loss: 19.831498\n",
      "2024-12-06 18:24: *********************************Current best model saved!\n",
      "2024-12-06 18:24: Train Epoch 58: 0/158 Loss: 17.720432\n",
      "2024-12-06 18:24: Train Epoch 58: 20/158 Loss: 18.458168\n",
      "2024-12-06 18:24: Train Epoch 58: 40/158 Loss: 18.480793\n",
      "2024-12-06 18:24: Train Epoch 58: 60/158 Loss: 17.529331\n",
      "2024-12-06 18:24: Train Epoch 58: 80/158 Loss: 16.973591\n",
      "2024-12-06 18:24: Train Epoch 58: 100/158 Loss: 17.544069\n",
      "2024-12-06 18:24: Train Epoch 58: 120/158 Loss: 17.843870\n",
      "2024-12-06 18:24: Train Epoch 58: 140/158 Loss: 18.563620\n",
      "2024-12-06 18:24: **********Train Epoch 58: averaged Loss: 17.795711, tf_ratio: 1.000000\n",
      "2024-12-06 18:24: **********Val Epoch 58: average Loss: 19.825164\n",
      "2024-12-06 18:24: *********************************Current best model saved!\n",
      "2024-12-06 18:24: Train Epoch 59: 0/158 Loss: 18.248913\n",
      "2024-12-06 18:24: Train Epoch 59: 20/158 Loss: 18.162958\n",
      "2024-12-06 18:24: Train Epoch 59: 40/158 Loss: 18.547634\n",
      "2024-12-06 18:24: Train Epoch 59: 60/158 Loss: 18.533155\n",
      "2024-12-06 18:24: Train Epoch 59: 80/158 Loss: 17.202293\n",
      "2024-12-06 18:24: Train Epoch 59: 100/158 Loss: 17.841837\n",
      "2024-12-06 18:24: Train Epoch 59: 120/158 Loss: 18.655289\n",
      "2024-12-06 18:24: Train Epoch 59: 140/158 Loss: 17.787933\n",
      "2024-12-06 18:24: **********Train Epoch 59: averaged Loss: 17.742375, tf_ratio: 1.000000\n",
      "2024-12-06 18:24: **********Val Epoch 59: average Loss: 19.823867\n",
      "2024-12-06 18:24: *********************************Current best model saved!\n",
      "2024-12-06 18:24: Train Epoch 60: 0/158 Loss: 17.789131\n",
      "2024-12-06 18:24: Train Epoch 60: 20/158 Loss: 18.039291\n",
      "2024-12-06 18:24: Train Epoch 60: 40/158 Loss: 18.727724\n",
      "2024-12-06 18:25: Train Epoch 60: 60/158 Loss: 16.439890\n",
      "2024-12-06 18:25: Train Epoch 60: 80/158 Loss: 17.296137\n",
      "2024-12-06 18:25: Train Epoch 60: 100/158 Loss: 17.342667\n",
      "2024-12-06 18:25: Train Epoch 60: 120/158 Loss: 16.299419\n",
      "2024-12-06 18:25: Train Epoch 60: 140/158 Loss: 18.168556\n",
      "2024-12-06 18:25: **********Train Epoch 60: averaged Loss: 17.713480, tf_ratio: 1.000000\n",
      "2024-12-06 18:25: **********Val Epoch 60: average Loss: 19.792109\n",
      "2024-12-06 18:25: *********************************Current best model saved!\n",
      "2024-12-06 18:25: Train Epoch 61: 0/158 Loss: 16.478270\n",
      "2024-12-06 18:25: Train Epoch 61: 20/158 Loss: 16.903532\n",
      "2024-12-06 18:25: Train Epoch 61: 40/158 Loss: 19.307386\n",
      "2024-12-06 18:25: Train Epoch 61: 60/158 Loss: 17.522394\n",
      "2024-12-06 18:25: Train Epoch 61: 80/158 Loss: 18.022408\n",
      "2024-12-06 18:25: Train Epoch 61: 100/158 Loss: 17.884157\n",
      "2024-12-06 18:25: Train Epoch 61: 120/158 Loss: 17.159927\n",
      "2024-12-06 18:25: Train Epoch 61: 140/158 Loss: 15.981811\n",
      "2024-12-06 18:25: **********Train Epoch 61: averaged Loss: 17.659531, tf_ratio: 1.000000\n",
      "2024-12-06 18:25: **********Val Epoch 61: average Loss: 19.740001\n",
      "2024-12-06 18:25: *********************************Current best model saved!\n",
      "2024-12-06 18:25: Train Epoch 62: 0/158 Loss: 16.783062\n",
      "2024-12-06 18:25: Train Epoch 62: 20/158 Loss: 17.293306\n",
      "2024-12-06 18:25: Train Epoch 62: 40/158 Loss: 17.001316\n",
      "2024-12-06 18:25: Train Epoch 62: 60/158 Loss: 17.656147\n",
      "2024-12-06 18:25: Train Epoch 62: 80/158 Loss: 15.779560\n",
      "2024-12-06 18:25: Train Epoch 62: 100/158 Loss: 18.347729\n",
      "2024-12-06 18:25: Train Epoch 62: 120/158 Loss: 17.089777\n",
      "2024-12-06 18:26: Train Epoch 62: 140/158 Loss: 18.651690\n",
      "2024-12-06 18:26: **********Train Epoch 62: averaged Loss: 17.630358, tf_ratio: 1.000000\n",
      "2024-12-06 18:26: **********Val Epoch 62: average Loss: 19.753734\n",
      "2024-12-06 18:26: Train Epoch 63: 0/158 Loss: 17.667322\n",
      "2024-12-06 18:26: Train Epoch 63: 20/158 Loss: 17.303671\n",
      "2024-12-06 18:26: Train Epoch 63: 40/158 Loss: 17.884859\n",
      "2024-12-06 18:26: Train Epoch 63: 60/158 Loss: 16.876711\n",
      "2024-12-06 18:26: Train Epoch 63: 80/158 Loss: 19.706125\n",
      "2024-12-06 18:26: Train Epoch 63: 100/158 Loss: 18.233900\n",
      "2024-12-06 18:26: Train Epoch 63: 120/158 Loss: 17.672428\n",
      "2024-12-06 18:26: Train Epoch 63: 140/158 Loss: 17.799978\n",
      "2024-12-06 18:26: **********Train Epoch 63: averaged Loss: 17.609444, tf_ratio: 1.000000\n",
      "2024-12-06 18:26: **********Val Epoch 63: average Loss: 19.742484\n",
      "2024-12-06 18:26: Train Epoch 64: 0/158 Loss: 18.880255\n",
      "2024-12-06 18:26: Train Epoch 64: 20/158 Loss: 16.714258\n",
      "2024-12-06 18:26: Train Epoch 64: 40/158 Loss: 17.659185\n",
      "2024-12-06 18:26: Train Epoch 64: 60/158 Loss: 17.835896\n",
      "2024-12-06 18:26: Train Epoch 64: 80/158 Loss: 17.653393\n",
      "2024-12-06 18:26: Train Epoch 64: 100/158 Loss: 17.905273\n",
      "2024-12-06 18:26: Train Epoch 64: 120/158 Loss: 17.167561\n",
      "2024-12-06 18:26: Train Epoch 64: 140/158 Loss: 16.991314\n",
      "2024-12-06 18:26: **********Train Epoch 64: averaged Loss: 17.542277, tf_ratio: 1.000000\n",
      "2024-12-06 18:26: **********Val Epoch 64: average Loss: 19.669359\n",
      "2024-12-06 18:26: *********************************Current best model saved!\n",
      "2024-12-06 18:26: Train Epoch 65: 0/158 Loss: 18.129179\n",
      "2024-12-06 18:26: Train Epoch 65: 20/158 Loss: 17.014343\n",
      "2024-12-06 18:27: Train Epoch 65: 40/158 Loss: 17.497828\n",
      "2024-12-06 18:27: Train Epoch 65: 60/158 Loss: 17.489019\n",
      "2024-12-06 18:27: Train Epoch 65: 80/158 Loss: 17.463459\n",
      "2024-12-06 18:27: Train Epoch 65: 100/158 Loss: 20.184423\n",
      "2024-12-06 18:27: Train Epoch 65: 120/158 Loss: 16.427923\n",
      "2024-12-06 18:27: Train Epoch 65: 140/158 Loss: 17.815657\n",
      "2024-12-06 18:27: **********Train Epoch 65: averaged Loss: 17.516090, tf_ratio: 1.000000\n",
      "2024-12-06 18:27: **********Val Epoch 65: average Loss: 19.656399\n",
      "2024-12-06 18:27: *********************************Current best model saved!\n",
      "2024-12-06 18:27: Train Epoch 66: 0/158 Loss: 16.037584\n",
      "2024-12-06 18:27: Train Epoch 66: 20/158 Loss: 18.254108\n",
      "2024-12-06 18:27: Train Epoch 66: 40/158 Loss: 17.275677\n",
      "2024-12-06 18:27: Train Epoch 66: 60/158 Loss: 18.276484\n",
      "2024-12-06 18:27: Train Epoch 66: 80/158 Loss: 18.173307\n",
      "2024-12-06 18:27: Train Epoch 66: 100/158 Loss: 17.739681\n",
      "2024-12-06 18:27: Train Epoch 66: 120/158 Loss: 17.927744\n",
      "2024-12-06 18:27: Train Epoch 66: 140/158 Loss: 18.691628\n",
      "2024-12-06 18:27: **********Train Epoch 66: averaged Loss: 17.514976, tf_ratio: 1.000000\n",
      "2024-12-06 18:27: **********Val Epoch 66: average Loss: 19.839629\n",
      "2024-12-06 18:27: Train Epoch 67: 0/158 Loss: 16.987398\n",
      "2024-12-06 18:27: Train Epoch 67: 20/158 Loss: 15.941523\n",
      "2024-12-06 18:27: Train Epoch 67: 40/158 Loss: 18.049435\n",
      "2024-12-06 18:27: Train Epoch 67: 60/158 Loss: 16.756069\n",
      "2024-12-06 18:27: Train Epoch 67: 80/158 Loss: 17.366396\n",
      "2024-12-06 18:27: Train Epoch 67: 100/158 Loss: 19.661747\n",
      "2024-12-06 18:28: Train Epoch 67: 120/158 Loss: 18.312853\n",
      "2024-12-06 18:28: Train Epoch 67: 140/158 Loss: 16.523468\n",
      "2024-12-06 18:28: **********Train Epoch 67: averaged Loss: 17.470740, tf_ratio: 1.000000\n",
      "2024-12-06 18:28: **********Val Epoch 67: average Loss: 19.661816\n",
      "2024-12-06 18:28: Train Epoch 68: 0/158 Loss: 18.842924\n",
      "2024-12-06 18:28: Train Epoch 68: 20/158 Loss: 17.790106\n",
      "2024-12-06 18:28: Train Epoch 68: 40/158 Loss: 16.500299\n",
      "2024-12-06 18:28: Train Epoch 68: 60/158 Loss: 16.694273\n",
      "2024-12-06 18:28: Train Epoch 68: 80/158 Loss: 18.024864\n",
      "2024-12-06 18:28: Train Epoch 68: 100/158 Loss: 17.509909\n",
      "2024-12-06 18:28: Train Epoch 68: 120/158 Loss: 16.873844\n",
      "2024-12-06 18:28: Train Epoch 68: 140/158 Loss: 18.985001\n",
      "2024-12-06 18:28: **********Train Epoch 68: averaged Loss: 17.418376, tf_ratio: 1.000000\n",
      "2024-12-06 18:28: **********Val Epoch 68: average Loss: 19.686972\n",
      "2024-12-06 18:28: Train Epoch 69: 0/158 Loss: 17.525808\n",
      "2024-12-06 18:28: Train Epoch 69: 20/158 Loss: 16.815277\n",
      "2024-12-06 18:28: Train Epoch 69: 40/158 Loss: 17.700718\n",
      "2024-12-06 18:28: Train Epoch 69: 60/158 Loss: 17.583454\n",
      "2024-12-06 18:28: Train Epoch 69: 80/158 Loss: 17.857109\n",
      "2024-12-06 18:28: Train Epoch 69: 100/158 Loss: 17.491983\n",
      "2024-12-06 18:28: Train Epoch 69: 120/158 Loss: 16.995071\n",
      "2024-12-06 18:28: Train Epoch 69: 140/158 Loss: 16.447102\n",
      "2024-12-06 18:28: **********Train Epoch 69: averaged Loss: 17.385555, tf_ratio: 1.000000\n",
      "2024-12-06 18:28: **********Val Epoch 69: average Loss: 19.745214\n",
      "2024-12-06 18:28: Train Epoch 70: 0/158 Loss: 17.799616\n",
      "2024-12-06 18:28: Train Epoch 70: 20/158 Loss: 16.514978\n",
      "2024-12-06 18:29: Train Epoch 70: 40/158 Loss: 17.384619\n",
      "2024-12-06 18:29: Train Epoch 70: 60/158 Loss: 16.608561\n",
      "2024-12-06 18:29: Train Epoch 70: 80/158 Loss: 17.003855\n",
      "2024-12-06 18:29: Train Epoch 70: 100/158 Loss: 17.494453\n",
      "2024-12-06 18:29: Train Epoch 70: 120/158 Loss: 16.021046\n",
      "2024-12-06 18:29: Train Epoch 70: 140/158 Loss: 18.127314\n",
      "2024-12-06 18:29: **********Train Epoch 70: averaged Loss: 17.357283, tf_ratio: 1.000000\n",
      "2024-12-06 18:29: **********Val Epoch 70: average Loss: 19.655950\n",
      "2024-12-06 18:29: *********************************Current best model saved!\n",
      "2024-12-06 18:29: Train Epoch 71: 0/158 Loss: 17.784409\n",
      "2024-12-06 18:29: Train Epoch 71: 20/158 Loss: 17.216717\n",
      "2024-12-06 18:29: Train Epoch 71: 40/158 Loss: 17.191708\n",
      "2024-12-06 18:29: Train Epoch 71: 60/158 Loss: 17.192793\n",
      "2024-12-06 18:29: Train Epoch 71: 80/158 Loss: 15.961543\n",
      "2024-12-06 18:29: Train Epoch 71: 100/158 Loss: 18.885450\n",
      "2024-12-06 18:29: Train Epoch 71: 120/158 Loss: 18.752060\n",
      "2024-12-06 18:29: Train Epoch 71: 140/158 Loss: 17.123611\n",
      "2024-12-06 18:29: **********Train Epoch 71: averaged Loss: 17.332370, tf_ratio: 1.000000\n",
      "2024-12-06 18:29: **********Val Epoch 71: average Loss: 19.669990\n",
      "2024-12-06 18:29: Train Epoch 72: 0/158 Loss: 16.390978\n",
      "2024-12-06 18:29: Train Epoch 72: 20/158 Loss: 17.928688\n",
      "2024-12-06 18:29: Train Epoch 72: 40/158 Loss: 16.815426\n",
      "2024-12-06 18:29: Train Epoch 72: 60/158 Loss: 17.456842\n",
      "2024-12-06 18:29: Train Epoch 72: 80/158 Loss: 17.199055\n",
      "2024-12-06 18:29: Train Epoch 72: 100/158 Loss: 17.358236\n",
      "2024-12-06 18:30: Train Epoch 72: 120/158 Loss: 17.792870\n",
      "2024-12-06 18:30: Train Epoch 72: 140/158 Loss: 16.158060\n",
      "2024-12-06 18:30: **********Train Epoch 72: averaged Loss: 17.291609, tf_ratio: 1.000000\n",
      "2024-12-06 18:30: **********Val Epoch 72: average Loss: 19.663312\n",
      "2024-12-06 18:30: Train Epoch 73: 0/158 Loss: 16.467951\n",
      "2024-12-06 18:30: Train Epoch 73: 20/158 Loss: 17.519064\n",
      "2024-12-06 18:30: Train Epoch 73: 40/158 Loss: 17.731432\n",
      "2024-12-06 18:30: Train Epoch 73: 60/158 Loss: 17.398550\n",
      "2024-12-06 18:30: Train Epoch 73: 80/158 Loss: 16.891903\n",
      "2024-12-06 18:30: Train Epoch 73: 100/158 Loss: 17.529648\n",
      "2024-12-06 18:30: Train Epoch 73: 120/158 Loss: 17.782967\n",
      "2024-12-06 18:30: Train Epoch 73: 140/158 Loss: 16.045036\n",
      "2024-12-06 18:30: **********Train Epoch 73: averaged Loss: 17.318274, tf_ratio: 1.000000\n",
      "2024-12-06 18:30: **********Val Epoch 73: average Loss: 19.603176\n",
      "2024-12-06 18:30: *********************************Current best model saved!\n",
      "2024-12-06 18:30: Train Epoch 74: 0/158 Loss: 16.705469\n",
      "2024-12-06 18:30: Train Epoch 74: 20/158 Loss: 16.916359\n",
      "2024-12-06 18:30: Train Epoch 74: 40/158 Loss: 17.944145\n",
      "2024-12-06 18:30: Train Epoch 74: 60/158 Loss: 17.701157\n",
      "2024-12-06 18:30: Train Epoch 74: 80/158 Loss: 16.284466\n",
      "2024-12-06 18:30: Train Epoch 74: 100/158 Loss: 16.646132\n",
      "2024-12-06 18:30: Train Epoch 74: 120/158 Loss: 16.908134\n",
      "2024-12-06 18:30: Train Epoch 74: 140/158 Loss: 18.441929\n",
      "2024-12-06 18:30: **********Train Epoch 74: averaged Loss: 17.250059, tf_ratio: 1.000000\n",
      "2024-12-06 18:30: **********Val Epoch 74: average Loss: 19.604324\n",
      "2024-12-06 18:30: Train Epoch 75: 0/158 Loss: 16.415150\n",
      "2024-12-06 18:31: Train Epoch 75: 20/158 Loss: 17.372730\n",
      "2024-12-06 18:31: Train Epoch 75: 40/158 Loss: 16.396517\n",
      "2024-12-06 18:31: Train Epoch 75: 60/158 Loss: 17.797094\n",
      "2024-12-06 18:31: Train Epoch 75: 80/158 Loss: 17.004284\n",
      "2024-12-06 18:31: Train Epoch 75: 100/158 Loss: 17.493820\n",
      "2024-12-06 18:31: Train Epoch 75: 120/158 Loss: 15.784223\n",
      "2024-12-06 18:31: Train Epoch 75: 140/158 Loss: 16.000523\n",
      "2024-12-06 18:31: **********Train Epoch 75: averaged Loss: 17.225034, tf_ratio: 1.000000\n",
      "2024-12-06 18:31: **********Val Epoch 75: average Loss: 19.578380\n",
      "2024-12-06 18:31: *********************************Current best model saved!\n",
      "2024-12-06 18:31: Train Epoch 76: 0/158 Loss: 16.545923\n",
      "2024-12-06 18:31: Train Epoch 76: 20/158 Loss: 17.746809\n",
      "2024-12-06 18:31: Train Epoch 76: 40/158 Loss: 16.566475\n",
      "2024-12-06 18:31: Train Epoch 76: 60/158 Loss: 18.273075\n",
      "2024-12-06 18:31: Train Epoch 76: 80/158 Loss: 17.332245\n",
      "2024-12-06 18:31: Train Epoch 76: 100/158 Loss: 18.360279\n",
      "2024-12-06 18:31: Train Epoch 76: 120/158 Loss: 16.701368\n",
      "2024-12-06 18:31: Train Epoch 76: 140/158 Loss: 16.507277\n",
      "2024-12-06 18:31: **********Train Epoch 76: averaged Loss: 17.202687, tf_ratio: 1.000000\n",
      "2024-12-06 18:31: **********Val Epoch 76: average Loss: 19.661407\n",
      "2024-12-06 18:31: Train Epoch 77: 0/158 Loss: 16.528215\n",
      "2024-12-06 18:31: Train Epoch 77: 20/158 Loss: 16.858139\n",
      "2024-12-06 18:31: Train Epoch 77: 40/158 Loss: 19.062904\n",
      "2024-12-06 18:31: Train Epoch 77: 60/158 Loss: 15.693095\n",
      "2024-12-06 18:31: Train Epoch 77: 80/158 Loss: 17.554224\n",
      "2024-12-06 18:32: Train Epoch 77: 100/158 Loss: 16.354755\n",
      "2024-12-06 18:32: Train Epoch 77: 120/158 Loss: 18.467812\n",
      "2024-12-06 18:32: Train Epoch 77: 140/158 Loss: 18.402029\n",
      "2024-12-06 18:32: **********Train Epoch 77: averaged Loss: 17.184237, tf_ratio: 1.000000\n",
      "2024-12-06 18:32: **********Val Epoch 77: average Loss: 19.625702\n",
      "2024-12-06 18:32: Train Epoch 78: 0/158 Loss: 16.010237\n",
      "2024-12-06 18:32: Train Epoch 78: 20/158 Loss: 17.018650\n",
      "2024-12-06 18:32: Train Epoch 78: 40/158 Loss: 18.765175\n",
      "2024-12-06 18:32: Train Epoch 78: 60/158 Loss: 16.711500\n",
      "2024-12-06 18:32: Train Epoch 78: 80/158 Loss: 17.093689\n",
      "2024-12-06 18:32: Train Epoch 78: 100/158 Loss: 19.204882\n",
      "2024-12-06 18:32: Train Epoch 78: 120/158 Loss: 16.247494\n",
      "2024-12-06 18:32: Train Epoch 78: 140/158 Loss: 16.919081\n",
      "2024-12-06 18:32: **********Train Epoch 78: averaged Loss: 17.174304, tf_ratio: 1.000000\n",
      "2024-12-06 18:32: **********Val Epoch 78: average Loss: 19.525447\n",
      "2024-12-06 18:32: *********************************Current best model saved!\n",
      "2024-12-06 18:32: Train Epoch 79: 0/158 Loss: 16.886562\n",
      "2024-12-06 18:32: Train Epoch 79: 20/158 Loss: 15.758048\n",
      "2024-12-06 18:32: Train Epoch 79: 40/158 Loss: 18.167274\n",
      "2024-12-06 18:32: Train Epoch 79: 60/158 Loss: 15.750805\n",
      "2024-12-06 18:32: Train Epoch 79: 80/158 Loss: 16.741474\n",
      "2024-12-06 18:32: Train Epoch 79: 100/158 Loss: 17.725267\n",
      "2024-12-06 18:32: Train Epoch 79: 120/158 Loss: 17.737680\n",
      "2024-12-06 18:32: Train Epoch 79: 140/158 Loss: 16.757727\n",
      "2024-12-06 18:32: **********Train Epoch 79: averaged Loss: 17.127795, tf_ratio: 1.000000\n",
      "2024-12-06 18:32: **********Val Epoch 79: average Loss: 19.557995\n",
      "2024-12-06 18:32: Train Epoch 80: 0/158 Loss: 17.815662\n",
      "2024-12-06 18:33: Train Epoch 80: 20/158 Loss: 17.308050\n",
      "2024-12-06 18:33: Train Epoch 80: 40/158 Loss: 17.763737\n",
      "2024-12-06 18:33: Train Epoch 80: 60/158 Loss: 16.131845\n",
      "2024-12-06 18:33: Train Epoch 80: 80/158 Loss: 15.748866\n",
      "2024-12-06 18:33: Train Epoch 80: 100/158 Loss: 16.576746\n",
      "2024-12-06 18:33: Train Epoch 80: 120/158 Loss: 17.886986\n",
      "2024-12-06 18:33: Train Epoch 80: 140/158 Loss: 17.030409\n",
      "2024-12-06 18:33: **********Train Epoch 80: averaged Loss: 17.132274, tf_ratio: 1.000000\n",
      "2024-12-06 18:33: **********Val Epoch 80: average Loss: 19.589850\n",
      "2024-12-06 18:33: Train Epoch 81: 0/158 Loss: 16.059784\n",
      "2024-12-06 18:33: Train Epoch 81: 20/158 Loss: 18.520737\n",
      "2024-12-06 18:33: Train Epoch 81: 40/158 Loss: 16.996174\n",
      "2024-12-06 18:33: Train Epoch 81: 60/158 Loss: 17.460035\n",
      "2024-12-06 18:33: Train Epoch 81: 80/158 Loss: 16.658110\n",
      "2024-12-06 18:33: Train Epoch 81: 100/158 Loss: 17.701015\n",
      "2024-12-06 18:33: Train Epoch 81: 120/158 Loss: 16.644997\n",
      "2024-12-06 18:33: Train Epoch 81: 140/158 Loss: 17.878269\n",
      "2024-12-06 18:33: **********Train Epoch 81: averaged Loss: 17.098271, tf_ratio: 1.000000\n",
      "2024-12-06 18:33: **********Val Epoch 81: average Loss: 19.625837\n",
      "2024-12-06 18:33: Train Epoch 82: 0/158 Loss: 16.778585\n",
      "2024-12-06 18:33: Train Epoch 82: 20/158 Loss: 17.558735\n",
      "2024-12-06 18:33: Train Epoch 82: 40/158 Loss: 18.369436\n",
      "2024-12-06 18:33: Train Epoch 82: 60/158 Loss: 17.626553\n",
      "2024-12-06 18:34: Train Epoch 82: 80/158 Loss: 16.987610\n",
      "2024-12-06 18:34: Train Epoch 82: 100/158 Loss: 17.443085\n",
      "2024-12-06 18:34: Train Epoch 82: 120/158 Loss: 18.569342\n",
      "2024-12-06 18:34: Train Epoch 82: 140/158 Loss: 16.512169\n",
      "2024-12-06 18:34: **********Train Epoch 82: averaged Loss: 17.128293, tf_ratio: 1.000000\n",
      "2024-12-06 18:34: **********Val Epoch 82: average Loss: 19.512488\n",
      "2024-12-06 18:34: *********************************Current best model saved!\n",
      "2024-12-06 18:34: Train Epoch 83: 0/158 Loss: 17.370535\n",
      "2024-12-06 18:34: Train Epoch 83: 20/158 Loss: 16.970781\n",
      "2024-12-06 18:34: Train Epoch 83: 40/158 Loss: 17.993246\n",
      "2024-12-06 18:34: Train Epoch 83: 60/158 Loss: 17.625158\n",
      "2024-12-06 18:34: Train Epoch 83: 80/158 Loss: 16.876942\n",
      "2024-12-06 18:34: Train Epoch 83: 100/158 Loss: 16.225288\n",
      "2024-12-06 18:34: Train Epoch 83: 120/158 Loss: 16.557339\n",
      "2024-12-06 18:34: Train Epoch 83: 140/158 Loss: 17.532244\n",
      "2024-12-06 18:34: **********Train Epoch 83: averaged Loss: 17.070789, tf_ratio: 1.000000\n",
      "2024-12-06 18:34: **********Val Epoch 83: average Loss: 19.568425\n",
      "2024-12-06 18:34: Train Epoch 84: 0/158 Loss: 16.753872\n",
      "2024-12-06 18:34: Train Epoch 84: 20/158 Loss: 17.255821\n",
      "2024-12-06 18:34: Train Epoch 84: 40/158 Loss: 18.223839\n",
      "2024-12-06 18:34: Train Epoch 84: 60/158 Loss: 18.131121\n",
      "2024-12-06 18:34: Train Epoch 84: 80/158 Loss: 16.725853\n",
      "2024-12-06 18:34: Train Epoch 84: 100/158 Loss: 16.921486\n",
      "2024-12-06 18:34: Train Epoch 84: 120/158 Loss: 16.448378\n",
      "2024-12-06 18:34: Train Epoch 84: 140/158 Loss: 17.378185\n",
      "2024-12-06 18:35: **********Train Epoch 84: averaged Loss: 17.055548, tf_ratio: 1.000000\n",
      "2024-12-06 18:35: **********Val Epoch 84: average Loss: 19.514727\n",
      "2024-12-06 18:35: Train Epoch 85: 0/158 Loss: 16.497881\n",
      "2024-12-06 18:35: Train Epoch 85: 20/158 Loss: 16.400055\n",
      "2024-12-06 18:35: Train Epoch 85: 40/158 Loss: 16.928366\n",
      "2024-12-06 18:35: Train Epoch 85: 60/158 Loss: 17.545321\n",
      "2024-12-06 18:35: Train Epoch 85: 80/158 Loss: 15.911493\n",
      "2024-12-06 18:35: Train Epoch 85: 100/158 Loss: 17.672119\n",
      "2024-12-06 18:35: Train Epoch 85: 120/158 Loss: 16.452339\n",
      "2024-12-06 18:35: Train Epoch 85: 140/158 Loss: 18.308714\n",
      "2024-12-06 18:35: **********Train Epoch 85: averaged Loss: 16.997707, tf_ratio: 1.000000\n",
      "2024-12-06 18:35: **********Val Epoch 85: average Loss: 19.541137\n",
      "2024-12-06 18:35: Train Epoch 86: 0/158 Loss: 15.227959\n",
      "2024-12-06 18:35: Train Epoch 86: 20/158 Loss: 16.752945\n",
      "2024-12-06 18:35: Train Epoch 86: 40/158 Loss: 16.153736\n",
      "2024-12-06 18:35: Train Epoch 86: 60/158 Loss: 17.331802\n",
      "2024-12-06 18:35: Train Epoch 86: 80/158 Loss: 17.101015\n",
      "2024-12-06 18:35: Train Epoch 86: 100/158 Loss: 16.601770\n",
      "2024-12-06 18:35: Train Epoch 86: 120/158 Loss: 16.999304\n",
      "2024-12-06 18:35: Train Epoch 86: 140/158 Loss: 17.211784\n",
      "2024-12-06 18:35: **********Train Epoch 86: averaged Loss: 16.999602, tf_ratio: 1.000000\n",
      "2024-12-06 18:35: **********Val Epoch 86: average Loss: 19.518694\n",
      "2024-12-06 18:35: Train Epoch 87: 0/158 Loss: 18.360085\n",
      "2024-12-06 18:35: Train Epoch 87: 20/158 Loss: 17.133865\n",
      "2024-12-06 18:35: Train Epoch 87: 40/158 Loss: 16.383591\n",
      "2024-12-06 18:35: Train Epoch 87: 60/158 Loss: 16.939579\n",
      "2024-12-06 18:36: Train Epoch 87: 80/158 Loss: 17.048208\n",
      "2024-12-06 18:36: Train Epoch 87: 100/158 Loss: 16.839336\n",
      "2024-12-06 18:36: Train Epoch 87: 120/158 Loss: 16.354122\n",
      "2024-12-06 18:36: Train Epoch 87: 140/158 Loss: 18.988071\n",
      "2024-12-06 18:36: **********Train Epoch 87: averaged Loss: 16.985402, tf_ratio: 1.000000\n",
      "2024-12-06 18:36: **********Val Epoch 87: average Loss: 19.523035\n",
      "2024-12-06 18:36: Train Epoch 88: 0/158 Loss: 16.516905\n",
      "2024-12-06 18:36: Train Epoch 88: 20/158 Loss: 15.626220\n",
      "2024-12-06 18:36: Train Epoch 88: 40/158 Loss: 17.075609\n",
      "2024-12-06 18:36: Train Epoch 88: 60/158 Loss: 16.098707\n",
      "2024-12-06 18:36: Train Epoch 88: 80/158 Loss: 17.629711\n",
      "2024-12-06 18:36: Train Epoch 88: 100/158 Loss: 16.718069\n",
      "2024-12-06 18:36: Train Epoch 88: 120/158 Loss: 17.060825\n",
      "2024-12-06 18:36: Train Epoch 88: 140/158 Loss: 16.175619\n",
      "2024-12-06 18:36: **********Train Epoch 88: averaged Loss: 16.946819, tf_ratio: 1.000000\n",
      "2024-12-06 18:36: **********Val Epoch 88: average Loss: 19.581292\n",
      "2024-12-06 18:36: Train Epoch 89: 0/158 Loss: 16.158272\n",
      "2024-12-06 18:36: Train Epoch 89: 20/158 Loss: 18.472910\n",
      "2024-12-06 18:36: Train Epoch 89: 40/158 Loss: 16.885738\n",
      "2024-12-06 18:36: Train Epoch 89: 60/158 Loss: 18.175301\n",
      "2024-12-06 18:36: Train Epoch 89: 80/158 Loss: 17.842585\n",
      "2024-12-06 18:36: Train Epoch 89: 100/158 Loss: 17.568771\n",
      "2024-12-06 18:36: Train Epoch 89: 120/158 Loss: 15.643711\n",
      "2024-12-06 18:36: Train Epoch 89: 140/158 Loss: 16.572147\n",
      "2024-12-06 18:37: **********Train Epoch 89: averaged Loss: 16.950062, tf_ratio: 1.000000\n",
      "2024-12-06 18:37: **********Val Epoch 89: average Loss: 19.517789\n",
      "2024-12-06 18:37: Train Epoch 90: 0/158 Loss: 17.619644\n",
      "2024-12-06 18:37: Train Epoch 90: 20/158 Loss: 15.488289\n",
      "2024-12-06 18:37: Train Epoch 90: 40/158 Loss: 16.436741\n",
      "2024-12-06 18:37: Train Epoch 90: 60/158 Loss: 16.930180\n",
      "2024-12-06 18:37: Train Epoch 90: 80/158 Loss: 17.653753\n",
      "2024-12-06 18:37: Train Epoch 90: 100/158 Loss: 16.956575\n",
      "2024-12-06 18:37: Train Epoch 90: 120/158 Loss: 18.665751\n",
      "2024-12-06 18:37: Train Epoch 90: 140/158 Loss: 16.519569\n",
      "2024-12-06 18:37: **********Train Epoch 90: averaged Loss: 16.982847, tf_ratio: 1.000000\n",
      "2024-12-06 18:37: **********Val Epoch 90: average Loss: 19.550990\n",
      "2024-12-06 18:37: Train Epoch 91: 0/158 Loss: 16.431124\n",
      "2024-12-06 18:37: Train Epoch 91: 20/158 Loss: 16.967867\n",
      "2024-12-06 18:37: Train Epoch 91: 40/158 Loss: 16.687408\n",
      "2024-12-06 18:37: Train Epoch 91: 60/158 Loss: 16.204079\n",
      "2024-12-06 18:37: Train Epoch 91: 80/158 Loss: 15.498323\n",
      "2024-12-06 18:37: Train Epoch 91: 100/158 Loss: 14.765727\n",
      "2024-12-06 18:37: Train Epoch 91: 120/158 Loss: 17.380350\n",
      "2024-12-06 18:37: Train Epoch 91: 140/158 Loss: 17.721153\n",
      "2024-12-06 18:37: **********Train Epoch 91: averaged Loss: 16.898860, tf_ratio: 1.000000\n",
      "2024-12-06 18:37: **********Val Epoch 91: average Loss: 19.562638\n",
      "2024-12-06 18:37: Train Epoch 92: 0/158 Loss: 16.849411\n",
      "2024-12-06 18:37: Train Epoch 92: 20/158 Loss: 18.144770\n",
      "2024-12-06 18:37: Train Epoch 92: 40/158 Loss: 15.679193\n",
      "2024-12-06 18:38: Train Epoch 92: 60/158 Loss: 16.901731\n",
      "2024-12-06 18:38: Train Epoch 92: 80/158 Loss: 16.755365\n",
      "2024-12-06 18:38: Train Epoch 92: 100/158 Loss: 17.092512\n",
      "2024-12-06 18:38: Train Epoch 92: 120/158 Loss: 17.002388\n",
      "2024-12-06 18:38: Train Epoch 92: 140/158 Loss: 17.780796\n",
      "2024-12-06 18:38: **********Train Epoch 92: averaged Loss: 16.922155, tf_ratio: 1.000000\n",
      "2024-12-06 18:38: **********Val Epoch 92: average Loss: 19.523646\n",
      "2024-12-06 18:38: Train Epoch 93: 0/158 Loss: 15.577678\n",
      "2024-12-06 18:38: Train Epoch 93: 20/158 Loss: 17.739414\n",
      "2024-12-06 18:38: Train Epoch 93: 40/158 Loss: 16.267096\n",
      "2024-12-06 18:38: Train Epoch 93: 60/158 Loss: 18.277592\n",
      "2024-12-06 18:38: Train Epoch 93: 80/158 Loss: 16.086926\n",
      "2024-12-06 18:38: Train Epoch 93: 100/158 Loss: 16.871822\n",
      "2024-12-06 18:38: Train Epoch 93: 120/158 Loss: 16.323767\n",
      "2024-12-06 18:38: Train Epoch 93: 140/158 Loss: 16.185293\n",
      "2024-12-06 18:38: **********Train Epoch 93: averaged Loss: 16.903164, tf_ratio: 1.000000\n",
      "2024-12-06 18:38: **********Val Epoch 93: average Loss: 19.476844\n",
      "2024-12-06 18:38: *********************************Current best model saved!\n",
      "2024-12-06 18:38: Train Epoch 94: 0/158 Loss: 17.336853\n",
      "2024-12-06 18:38: Train Epoch 94: 20/158 Loss: 17.114248\n",
      "2024-12-06 18:38: Train Epoch 94: 40/158 Loss: 16.246231\n",
      "2024-12-06 18:38: Train Epoch 94: 60/158 Loss: 18.533091\n",
      "2024-12-06 18:38: Train Epoch 94: 80/158 Loss: 16.468044\n",
      "2024-12-06 18:38: Train Epoch 94: 100/158 Loss: 17.111849\n",
      "2024-12-06 18:38: Train Epoch 94: 120/158 Loss: 16.882263\n",
      "2024-12-06 18:39: Train Epoch 94: 140/158 Loss: 17.367392\n",
      "2024-12-06 18:39: **********Train Epoch 94: averaged Loss: 16.866201, tf_ratio: 1.000000\n",
      "2024-12-06 18:39: **********Val Epoch 94: average Loss: 19.458689\n",
      "2024-12-06 18:39: *********************************Current best model saved!\n",
      "2024-12-06 18:39: Train Epoch 95: 0/158 Loss: 16.139174\n",
      "2024-12-06 18:39: Train Epoch 95: 20/158 Loss: 16.461344\n",
      "2024-12-06 18:39: Train Epoch 95: 40/158 Loss: 17.639065\n",
      "2024-12-06 18:39: Train Epoch 95: 60/158 Loss: 16.956951\n",
      "2024-12-06 18:39: Train Epoch 95: 80/158 Loss: 18.285679\n",
      "2024-12-06 18:39: Train Epoch 95: 100/158 Loss: 15.915596\n",
      "2024-12-06 18:39: Train Epoch 95: 120/158 Loss: 17.591663\n",
      "2024-12-06 18:39: Train Epoch 95: 140/158 Loss: 14.714630\n",
      "2024-12-06 18:39: **********Train Epoch 95: averaged Loss: 16.904458, tf_ratio: 1.000000\n",
      "2024-12-06 18:39: **********Val Epoch 95: average Loss: 19.574336\n",
      "2024-12-06 18:39: Train Epoch 96: 0/158 Loss: 15.195674\n",
      "2024-12-06 18:39: Train Epoch 96: 20/158 Loss: 17.779640\n",
      "2024-12-06 18:39: Train Epoch 96: 40/158 Loss: 17.276625\n",
      "2024-12-06 18:39: Train Epoch 96: 60/158 Loss: 16.907007\n",
      "2024-12-06 18:39: Train Epoch 96: 80/158 Loss: 15.904383\n",
      "2024-12-06 18:39: Train Epoch 96: 100/158 Loss: 17.232054\n",
      "2024-12-06 18:39: Train Epoch 96: 120/158 Loss: 15.580585\n",
      "2024-12-06 18:39: Train Epoch 96: 140/158 Loss: 16.833399\n",
      "2024-12-06 18:39: **********Train Epoch 96: averaged Loss: 16.844491, tf_ratio: 1.000000\n",
      "2024-12-06 18:39: **********Val Epoch 96: average Loss: 19.490029\n",
      "2024-12-06 18:39: Train Epoch 97: 0/158 Loss: 15.105927\n",
      "2024-12-06 18:39: Train Epoch 97: 20/158 Loss: 15.977933\n",
      "2024-12-06 18:40: Train Epoch 97: 40/158 Loss: 16.231834\n",
      "2024-12-06 18:40: Train Epoch 97: 60/158 Loss: 16.582619\n",
      "2024-12-06 18:40: Train Epoch 97: 80/158 Loss: 16.995363\n",
      "2024-12-06 18:40: Train Epoch 97: 100/158 Loss: 16.315878\n",
      "2024-12-06 18:40: Train Epoch 97: 120/158 Loss: 16.700613\n",
      "2024-12-06 18:40: Train Epoch 97: 140/158 Loss: 16.476515\n",
      "2024-12-06 18:40: **********Train Epoch 97: averaged Loss: 16.831968, tf_ratio: 1.000000\n",
      "2024-12-06 18:40: **********Val Epoch 97: average Loss: 19.524902\n",
      "2024-12-06 18:40: Train Epoch 98: 0/158 Loss: 17.594942\n",
      "2024-12-06 18:40: Train Epoch 98: 20/158 Loss: 15.757738\n",
      "2024-12-06 18:40: Train Epoch 98: 40/158 Loss: 16.329578\n",
      "2024-12-06 18:40: Train Epoch 98: 60/158 Loss: 16.454769\n",
      "2024-12-06 18:40: Train Epoch 98: 80/158 Loss: 17.701183\n",
      "2024-12-06 18:40: Train Epoch 98: 100/158 Loss: 16.057623\n",
      "2024-12-06 18:40: Train Epoch 98: 120/158 Loss: 17.244123\n",
      "2024-12-06 18:40: Train Epoch 98: 140/158 Loss: 17.733702\n",
      "2024-12-06 18:40: **********Train Epoch 98: averaged Loss: 16.799068, tf_ratio: 1.000000\n",
      "2024-12-06 18:40: **********Val Epoch 98: average Loss: 19.504851\n",
      "2024-12-06 18:40: Train Epoch 99: 0/158 Loss: 17.732447\n",
      "2024-12-06 18:40: Train Epoch 99: 20/158 Loss: 17.301123\n",
      "2024-12-06 18:40: Train Epoch 99: 40/158 Loss: 18.033457\n",
      "2024-12-06 18:40: Train Epoch 99: 60/158 Loss: 16.547594\n",
      "2024-12-06 18:40: Train Epoch 99: 80/158 Loss: 16.751371\n",
      "2024-12-06 18:40: Train Epoch 99: 100/158 Loss: 16.735975\n",
      "2024-12-06 18:41: Train Epoch 99: 120/158 Loss: 16.480448\n",
      "2024-12-06 18:41: Train Epoch 99: 140/158 Loss: 17.515335\n",
      "2024-12-06 18:41: **********Train Epoch 99: averaged Loss: 16.803086, tf_ratio: 1.000000\n",
      "2024-12-06 18:41: **********Val Epoch 99: average Loss: 19.528617\n",
      "2024-12-06 18:41: Train Epoch 100: 0/158 Loss: 16.796280\n",
      "2024-12-06 18:41: Train Epoch 100: 20/158 Loss: 16.153259\n",
      "2024-12-06 18:41: Train Epoch 100: 40/158 Loss: 17.754759\n",
      "2024-12-06 18:41: Train Epoch 100: 60/158 Loss: 15.823922\n",
      "2024-12-06 18:41: Train Epoch 100: 80/158 Loss: 17.068436\n",
      "2024-12-06 18:41: Train Epoch 100: 100/158 Loss: 17.412642\n",
      "2024-12-06 18:41: Train Epoch 100: 120/158 Loss: 16.373419\n",
      "2024-12-06 18:41: Train Epoch 100: 140/158 Loss: 16.590279\n",
      "2024-12-06 18:41: **********Train Epoch 100: averaged Loss: 16.813715, tf_ratio: 1.000000\n",
      "2024-12-06 18:41: **********Val Epoch 100: average Loss: 19.488389\n",
      "2024-12-06 18:41: Total training time: 41.6615min, best loss: 19.458689\n",
      "2024-12-06 18:41: Horizon 01, MAE: 18.68, RMSE: 30.26, MAPE: 12.3836%\n",
      "2024-12-06 18:41: Horizon 02, MAE: 18.73, RMSE: 30.59, MAPE: 12.3600%\n",
      "2024-12-06 18:41: Horizon 03, MAE: 18.94, RMSE: 31.03, MAPE: 12.4657%\n",
      "2024-12-06 18:41: Horizon 04, MAE: 19.20, RMSE: 31.50, MAPE: 12.6077%\n",
      "2024-12-06 18:41: Horizon 05, MAE: 19.49, RMSE: 31.99, MAPE: 12.7701%\n",
      "2024-12-06 18:41: Horizon 06, MAE: 19.76, RMSE: 32.46, MAPE: 12.9211%\n",
      "2024-12-06 18:41: Horizon 07, MAE: 19.99, RMSE: 32.89, MAPE: 13.0665%\n",
      "2024-12-06 18:41: Horizon 08, MAE: 20.17, RMSE: 33.27, MAPE: 13.1602%\n",
      "2024-12-06 18:41: Horizon 09, MAE: 20.34, RMSE: 33.63, MAPE: 13.2325%\n",
      "2024-12-06 18:41: Horizon 10, MAE: 20.55, RMSE: 34.02, MAPE: 13.3472%\n",
      "2024-12-06 18:41: Horizon 11, MAE: 20.92, RMSE: 34.56, MAPE: 13.5412%\n",
      "2024-12-06 18:41: Horizon 12, MAE: 21.49, RMSE: 35.29, MAPE: 13.9036%\n",
      "2024-12-06 18:41: Average Horizon, MAE: 19.85, RMSE: 32.66, MAPE: 12.9800%\n"
     ]
    }
   ],
   "source": [
    "Mode = 'train'  # Use lowercase 'train' instead of 'Train'\n",
    "\n",
    "if args.mode.lower() == 'train':  # Normalize to lowercase for comparison\n",
    "    trainer.train()\n",
    "elif args.mode.lower() == 'test':\n",
    "    model.load_state_dict(torch.load('../pre-trained/{}.pth'.format(args.dataset)))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "else:\n",
    "    raise ValueError(f\"Invalid mode: {args.mode}. Expected 'train' or 'test'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160c95e-bf8b-47ca-bfb7-4902bd34fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using loss function: {args.loss_func}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12cdaca2-84f5-4103-9ac2-a914548e9dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:59: Experiment log path in: /home/jovyan/work/data/PEMS04/experiments/PEMSD4/20241206175947\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtest(model, trainer\u001b[38;5;241m.\u001b[39margs, test_loader, scaler, trainer\u001b[38;5;241m.\u001b[39mlogger)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #init loss function, optimizer\n",
    "# if args.loss_func == 'mask_mae':\n",
    "#     loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "# elif args.loss_func == 'mae':\n",
    "#     loss = torch.nn.L1Loss().to(args.device)\n",
    "# elif args.loss_func == 'mse':\n",
    "#     loss = torch.nn.MSELoss().to(args.device)\n",
    "# else:\n",
    "#     raise ValueError\n",
    "\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init, eps=1.0e-8,\n",
    "#                              weight_decay=0, amsgrad=False)\n",
    "# #learning rate decay\n",
    "# lr_scheduler = None\n",
    "# if args.lr_decay:\n",
    "#     print('Applying learning rate decay.')\n",
    "#     lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "#                                                         milestones=lr_decay_steps,\n",
    "#                                                         gamma=args.lr_decay_rate)\n",
    "#     #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "# #config log path\n",
    "# current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "# current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "# log_dir = os.path.join(current_dir,'experiments', args.dataset, current_time)\n",
    "# args.log_dir = log_dir\n",
    "\n",
    "# #start training\n",
    "# trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "#                   args, lr_scheduler=lr_scheduler)\n",
    "# if args.mode == 'train':\n",
    "#     trainer.train()\n",
    "# elif args.mode == 'test':\n",
    "#     model.load_state_dict(torch.load('../pre-trained/{}.pth'.format(args.dataset)))\n",
    "#     print(\"Load saved model\")\n",
    "#     trainer.test(model, trainer.args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110789f2-5205-49f6-b6c1-a6128d584c0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m precomputed_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(args\u001b[38;5;241m.\u001b[39mnum_nodes, args\u001b[38;5;241m.\u001b[39membed_dim)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecomputed_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecomputed_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/work/model/AGCRN.py:47\u001b[0m, in \u001b[0;36mAGCRN.__init__\u001b[0;34m(self, args, precomputed_embeddings)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28msuper\u001b[39m(AGCRN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_node \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mnum_nodes\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrnn_units\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39moutput_dim\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'input_dim'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from model.AGCRN import AGCRN as Network\n",
    "# from model.BasicTrainer import Trainer\n",
    "# from lib.dataloader import get_dataloader\n",
    "# from lib.TrainInits import print_model_parameters, init_seed\n",
    "# from lib.metrics import MAE_torch\n",
    "\n",
    "# # Setup configuration and argument parser\n",
    "# args = argparse.Namespace()\n",
    "\n",
    "# # Configuration Parameters\n",
    "# args.dataset = 'PEMSD4'\n",
    "# args.mode = 'train'  # Use 'train' or 'test'\n",
    "# args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# args.loss_func = 'mae'\n",
    "# args.lr_init = 0.003\n",
    "# args.lr_decay = True\n",
    "# args.lr_decay_rate = 0.3\n",
    "# args.lr_decay_step = '5,20,40,70'\n",
    "# args.epochs = 100\n",
    "# args.batch_size = 64\n",
    "# args.weight_decay = 0.0001\n",
    "# args.log_step = 20\n",
    "# args.embed_dim = 10  # Example embedding dimension\n",
    "# args.num_nodes = 307  # Example node count\n",
    "\n",
    "# # Initialize seed\n",
    "# init_seed(42)\n",
    "\n",
    "# # Initialize the model\n",
    "# precomputed_embeddings = np.random.rand(args.num_nodes, args.embed_dim).astype(np.float32)\n",
    "# model = Network(args, precomputed_embeddings=torch.tensor(precomputed_embeddings))\n",
    "# model = model.to(args.device)\n",
    "\n",
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         torch.nn.init.xavier_uniform_(p)\n",
    "#     else:\n",
    "#         torch.nn.init.uniform_(p)\n",
    "# print_model_parameters(model, only_num=False)\n",
    "\n",
    "# # Load dataset\n",
    "# train_loader, val_loader, test_loader, scaler = get_dataloader(\n",
    "#     args, normalizer='std', tod=False, dow=False, weather=False, single=False\n",
    "# )\n",
    "\n",
    "# # Initialize loss function\n",
    "# if args.loss_func == 'mask_mae':\n",
    "#     def masked_mae_loss(scaler, mask_value):\n",
    "#         def loss(preds, labels):\n",
    "#             preds = scaler.inverse_transform(preds)\n",
    "#             labels = scaler.inverse_transform(labels)\n",
    "#             return MAE_torch(preds, labels, mask_value)\n",
    "#         return loss\n",
    "\n",
    "#     loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "# elif args.loss_func == 'mae':\n",
    "#     loss = torch.nn.L1Loss().to(args.device)\n",
    "# elif args.loss_func == 'mse':\n",
    "#     loss = torch.nn.MSELoss().to(args.device)\n",
    "# else:\n",
    "#     raise ValueError(f\"Unknown loss function: {args.loss_func}\")\n",
    "\n",
    "# # Initialize optimizer\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     params=model.parameters(), lr=args.lr_init, eps=1.0e-8, weight_decay=args.weight_decay, amsgrad=False\n",
    "# )\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# lr_scheduler = None\n",
    "# if args.lr_decay:\n",
    "#     print(\"Applying learning rate decay.\")\n",
    "#     lr_decay_steps = [int(i) for i in args.lr_decay_step.split(',')]\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#         optimizer=optimizer, milestones=lr_decay_steps, gamma=args.lr_decay_rate\n",
    "#     )\n",
    "\n",
    "# # Configure log path\n",
    "# current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "# current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "# log_dir = os.path.join(current_dir, 'experiments', args.dataset, current_time)\n",
    "# args.log_dir = log_dir\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# print(f\"Experiment log path in: {args.log_dir}\")\n",
    "\n",
    "# # Initialize trainer\n",
    "# trainer = Trainer(model, loss, optimizer, train_loader, val_loader, test_loader, scaler, args, lr_scheduler)\n",
    "\n",
    "# # Train or Test\n",
    "# if args.mode.lower() == 'train':\n",
    "#     trainer.train()\n",
    "# elif args.mode.lower() == 'test':\n",
    "#     model.load_state_dict(torch.load(f'../pre-trained/{args.dataset}.pth'))\n",
    "#     print(\"Loaded saved model.\")\n",
    "#     trainer.test(model, args, test_loader, scaler, trainer.logger)\n",
    "# else:\n",
    "#     raise ValueError(f\"Invalid mode: {args.mode}. Expected 'train' or 'test'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf335564-179b-4faf-a411-b11a299f439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
